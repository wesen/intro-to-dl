{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting deeper with Keras\n",
    "* Tensorflow is a powerful and flexible tool, but coding large neural architectures with it is tedious.\n",
    "* There are plenty of deep learning toolkits that work on top of it like Slim, TFLearn, Sonnet, Keras.\n",
    "* Choice is matter of taste and particular task\n",
    "* We'll be using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n",
      "'ln' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ln' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# use preloaded keras datasets and models\n",
    "! mkdir -p ~/.keras/datasets\n",
    "! mkdir -p ~/.keras/models\n",
    "! ln -s $(realpath ../readonly/keras/datasets/*) ~/.keras/datasets/\n",
    "! ln -s $(realpath ../readonly/keras/models/*) ~/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from preprocessed_mnist import load_dataset\n",
    "import keras\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "y_train,y_val,y_test = map(keras.utils.np_utils.to_categorical,[y_train,y_val,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26a1fdd2128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pretty keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "import keras.layers as ll\n",
    "\n",
    "model = Sequential(name=\"mlp\")\n",
    "\n",
    "model.add(ll.InputLayer([28, 28, 1]))\n",
    "model.add(ll.Reshape((28,28)))\n",
    "\n",
    "model.add(ll.Flatten())\n",
    "\n",
    "# network body\n",
    "model.add(ll.Dense(50))\n",
    "model.add(ll.Activation('relu'))\n",
    "\n",
    "model.add(ll.Dense(30))\n",
    "model.add(ll.Activation('sigmoid'))\n",
    "\n",
    "# output layer: 10 neurons for each class with softmax\n",
    "model.add(ll.Dense(10, activation='softmax'))\n",
    "\n",
    "# categorical_crossentropy is your good old crossentropy\n",
    "# but applied for one-hot-encoded vectors\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model.compile(keras.optimizers.SGD(lr=1.0, momentum=0.1, decay=0.0001), \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 41,090\n",
      "Trainable params: 41,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interface\n",
    "\n",
    "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAD8CAYAAADOg5fGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26a0088ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "shift_range = 0.0\n",
    "datagen = ImageDataGenerator(rotation_range=10, \n",
    "#                              featurewise_center=True, \n",
    "#                              featurewise_std_normalization=True,\n",
    "                             width_shift_range=shift_range,\n",
    "                             height_shift_range=shift_range,\n",
    "#                             zca_whitening=True\n",
    "                            )\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "y_train,y_val,y_test = map(keras.utils.np_utils.to_categorical,[y_train,y_val,y_test])\n",
    "\n",
    "test_train = X_train.copy()\n",
    "test_train = test_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "\n",
    "test_val = X_val.copy()\n",
    "test_val = test_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "test_test = X_test.copy()\n",
    "test_test = test_test.reshape(test_val.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "\n",
    "datagen.fit(test_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(test_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break\n",
    "\n",
    "# model.fit_generator(datagen.flow(test_train, y_train, batch_size=32), steps_per_epoch=len(test_train)/32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9763\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0780 - acc: 0.9762 - val_loss: 0.0793 - val_acc: 0.9744\n",
      "Epoch 2/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9767\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0762 - acc: 0.9767 - val_loss: 0.0774 - val_acc: 0.9760\n",
      "Epoch 3/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9775\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0736 - acc: 0.9776 - val_loss: 0.0767 - val_acc: 0.9759\n",
      "Epoch 4/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9783\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0717 - acc: 0.9784 - val_loss: 0.0753 - val_acc: 0.9761\n",
      "Epoch 5/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9791\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0699 - acc: 0.9791 - val_loss: 0.0754 - val_acc: 0.9766\n",
      "Epoch 6/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9794\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0678 - acc: 0.9795 - val_loss: 0.0742 - val_acc: 0.9778\n",
      "Epoch 7/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9801\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0660 - acc: 0.9800 - val_loss: 0.0742 - val_acc: 0.9767\n",
      "Epoch 8/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9804\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0645 - acc: 0.9803 - val_loss: 0.0726 - val_acc: 0.9769\n",
      "Epoch 9/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9807\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0633 - acc: 0.9807 - val_loss: 0.0732 - val_acc: 0.9773\n",
      "Epoch 10/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9817\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0610 - acc: 0.9817 - val_loss: 0.0732 - val_acc: 0.9765\n",
      "Epoch 11/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9818\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0600 - acc: 0.9818 - val_loss: 0.0719 - val_acc: 0.9777\n",
      "Epoch 12/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9823\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0581 - acc: 0.9823 - val_loss: 0.0718 - val_acc: 0.9775\n",
      "Epoch 13/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9831\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0569 - acc: 0.9831 - val_loss: 0.0704 - val_acc: 0.9776\n",
      "Epoch 14/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9830\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0566 - acc: 0.9830 - val_loss: 0.0709 - val_acc: 0.9786\n",
      "Epoch 15/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9834\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0557 - acc: 0.9833 - val_loss: 0.0718 - val_acc: 0.9785\n",
      "Epoch 16/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9838\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0541 - acc: 0.9837 - val_loss: 0.0707 - val_acc: 0.9781\n",
      "Epoch 17/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9839\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0532 - acc: 0.9839 - val_loss: 0.0704 - val_acc: 0.9778\n",
      "Epoch 18/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9836\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0532 - acc: 0.9836 - val_loss: 0.0714 - val_acc: 0.9778\n",
      "Epoch 19/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9846\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 13s - loss: 0.0508 - acc: 0.9846 - val_loss: 0.0713 - val_acc: 0.9781\n",
      "Epoch 20/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9853\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0494 - acc: 0.9852 - val_loss: 0.0701 - val_acc: 0.9792\n",
      "Epoch 21/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9860\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0476 - acc: 0.9859 - val_loss: 0.0702 - val_acc: 0.9787\n",
      "Epoch 22/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9863\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0465 - acc: 0.9862 - val_loss: 0.0709 - val_acc: 0.9788\n",
      "Epoch 23/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9854\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0474 - acc: 0.9854 - val_loss: 0.0701 - val_acc: 0.9784\n",
      "Epoch 24/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9864\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0453 - acc: 0.9864 - val_loss: 0.0720 - val_acc: 0.9783\n",
      "Epoch 25/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9869\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0445 - acc: 0.9869 - val_loss: 0.0718 - val_acc: 0.9779\n",
      "Epoch 26/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9864\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0444 - acc: 0.9865 - val_loss: 0.0693 - val_acc: 0.9794\n",
      "Epoch 27/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9873\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0428 - acc: 0.9872 - val_loss: 0.0706 - val_acc: 0.9783\n",
      "Epoch 28/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9872\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0419 - acc: 0.9872 - val_loss: 0.0715 - val_acc: 0.9778\n",
      "Epoch 29/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9876\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0413 - acc: 0.9876 - val_loss: 0.0721 - val_acc: 0.9789\n",
      "Epoch 30/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9880\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0406 - acc: 0.9879 - val_loss: 0.0697 - val_acc: 0.9797\n",
      "Epoch 31/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9876\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0405 - acc: 0.9876 - val_loss: 0.0715 - val_acc: 0.9789\n",
      "Epoch 32/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9882\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0397 - acc: 0.9882 - val_loss: 0.0711 - val_acc: 0.9798\n",
      "Epoch 33/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9885\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0388 - acc: 0.9885 - val_loss: 0.0694 - val_acc: 0.9795\n",
      "Epoch 34/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9885\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0385 - acc: 0.9885 - val_loss: 0.0704 - val_acc: 0.9786\n",
      "Epoch 35/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9888\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0375 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9794\n",
      "Epoch 36/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9896\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0358 - acc: 0.9896 - val_loss: 0.0705 - val_acc: 0.9800\n",
      "Epoch 37/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9896\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0350 - acc: 0.9896 - val_loss: 0.0712 - val_acc: 0.9798\n",
      "Epoch 38/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9895\n",
      "LR: 0.001000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 12s - loss: 0.0354 - acc: 0.9895 - val_loss: 0.0720 - val_acc: 0.9787\n",
      "Epoch 39/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9898\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0343 - acc: 0.9898 - val_loss: 0.0712 - val_acc: 0.9791\n",
      "Epoch 40/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9898\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0348 - acc: 0.9898 - val_loss: 0.0724 - val_acc: 0.9784\n",
      "Epoch 41/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9902\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0333 - acc: 0.9901 - val_loss: 0.0721 - val_acc: 0.9794\n",
      "Epoch 42/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9899\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0722 - val_acc: 0.9789\n",
      "Epoch 43/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9906\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0325 - acc: 0.9905 - val_loss: 0.0707 - val_acc: 0.9794\n",
      "Epoch 44/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9905\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0323 - acc: 0.9905 - val_loss: 0.0742 - val_acc: 0.9787\n",
      "Epoch 45/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9907\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0314 - acc: 0.9907 - val_loss: 0.0720 - val_acc: 0.9792\n",
      "Epoch 46/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9904\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0317 - acc: 0.9905 - val_loss: 0.0736 - val_acc: 0.9791\n",
      "Epoch 47/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9911\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0307 - acc: 0.9911 - val_loss: 0.0717 - val_acc: 0.9794\n",
      "Epoch 48/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9912\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0303 - acc: 0.9912 - val_loss: 0.0720 - val_acc: 0.9788\n",
      "Epoch 49/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9914\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0290 - acc: 0.9915 - val_loss: 0.0726 - val_acc: 0.9792\n",
      "Epoch 50/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9912\n",
      "LR: 0.001000\n",
      "\n",
      "100/100 [==============================] - 12s - loss: 0.0300 - acc: 0.9912 - val_loss: 0.0736 - val_acc: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a16a9b8d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "model.fit_generator(datagen.flow(test_train, y_train, batch_size=batch_size), steps_per_epoch=2 * len(test_train)/batch_size, epochs=50,\n",
    "                    validation_data=(test_val, y_val),\n",
    "                   callbacks=[SGDLearningRateTracker()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 8s - loss: 0.2016 - acc: 0.9407 - val_loss: 0.1490 - val_acc: 0.9565\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 8s - loss: 0.1326 - acc: 0.9607 - val_loss: 0.1180 - val_acc: 0.9660\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 8s - loss: 0.1057 - acc: 0.9687 - val_loss: 0.1069 - val_acc: 0.9694\n",
      "Epoch 4/10\n",
      " 4160/50000 [=>............................] - ETA: 7s - loss: 0.0840 - acc: 0.9769"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-3c74e63e81aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#          Highly customizable under the hood.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model.fit(test_train, y_train,\n\u001b[1;32m----> 4\u001b[1;33m           validation_data=(test_val, y_val), epochs=10);\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit(X,y) ships with a neat automatic logging.\n",
    "#          Highly customizable under the hood.\n",
    "model.fit(test_train, y_train,\n",
    "          validation_data=(test_val, y_val), epochs=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  7.30745375e-09,   1.08722061e-05,   2.61888144e-05,\n",
       "          9.99904990e-01,   5.35814837e-10,   1.82550539e-05,\n",
       "          1.06895326e-10,   1.02607885e-06,   3.80318925e-05,\n",
       "          5.67525774e-07],\n",
       "       [  1.37968343e-07,   1.26270515e-06,   2.74728990e-07,\n",
       "          2.49382083e-05,   1.95730934e-08,   3.32583804e-05,\n",
       "          9.00859220e-07,   1.25495390e-07,   9.99915361e-01,\n",
       "          2.36731375e-05]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate probabilities P(y|x)\n",
    "model.predict_proba(test_val[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save trained weights\n",
    "model.save(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9824/10000 [============================>.] - ETA: 0s\n",
      "Loss, Accuracy =  [0.061010356263536958, 0.98040000000000005]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoss, Accuracy = \", model.evaluate(test_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whoops!\n",
    "So far our model is staggeringly inefficient. There is something wring with it. Guess, what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9568/10000 [===========================>..] - ETA: 0s\n",
      "Test accuracy: 98.04 %\n",
      "Great job!\n"
     ]
    }
   ],
   "source": [
    "# Test score...\n",
    "test_predictions = model.predict_proba(test_test).argmax(axis=-1)\n",
    "test_answers = y_test.argmax(axis=-1)\n",
    "\n",
    "test_accuracy = np.mean(test_predictions==test_answers)\n",
    "\n",
    "print(\"\\nTest accuracy: {} %\".format(test_accuracy*100))\n",
    "\n",
    "assert test_accuracy>=0.92,\"Logistic regression can do better!\"\n",
    "assert test_accuracy>=0.975,\"Your network can do better!\"\n",
    "print(\"Great job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer_submitter = grading.Grader(\"0ybD9ZxxEeea8A6GzH-6CA\")\n",
    "answer_submitter.set_answer(\"N56DR\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-0639a365657e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-0639a365657e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    answer_submitter.submit(<your-email>, <your-assignment-token>)\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "answer_submitter.submit('wesen@ruinwesen.com', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras + tensorboard\n",
    "\n",
    "Remember the interactive graphs from Tensorboard one notebook ago? \n",
    "\n",
    "Thing is, Keras can use tensorboard to show you a lot of useful information about the learning progress. Just take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm -r /tmp/tboard/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "          epochs=10,\n",
    "          callbacks=[TensorBoard(\"/tmp/tboard\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips & tricks\n",
    "\n",
    "Here are some tips on what you could do. Don't worry, to reach the passing threshold you don't need to try all the ideas listed here, feel free to stop once you reach the 0.975 accuracy mark.\n",
    "\n",
    " * __Network size__\n",
    "   * More neurons, \n",
    "   * More layers, ([docs](https://keras.io/))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "\n",
    " * __Early Stopping__\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "     \n",
    "\n",
    " * __Faster optimization__\n",
    "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "\n",
    "\n",
    " * __Regularize__ to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "     * Can be done manually or via - https://keras.io/regularizers/\n",
    "   \n",
    "   \n",
    " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
    "   * https://keras.io/preprocessing/image/\n",
    "   * Zoom-in+slice = move\n",
    "   * Rotate+zoom(to remove black stripes)\n",
    "   * any other perturbations\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
