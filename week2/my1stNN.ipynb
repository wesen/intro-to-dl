{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110868d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Relaunch tensorboard, and import useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching processes belonging to you were found\r\n"
     ]
    }
   ],
   "source": [
    "! killall tensorboard\n",
    "import os\n",
    "os.system(\"tensorboard --logdir=/tmp/tboard --port=7007 &\");\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection and preparation\n",
    "\n",
    "We first take a look at the data, and prepare a version having just two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a flattened version of the data, because tensorflow has difficulty computing gradients across a reshape node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"1nn-reshaper\"):\n",
    "    # input shape\n",
    "    x = tf.placeholder(tf.float32, shape=(None, rows, cols), name=\"input_X\")\n",
    "    y = tf.placeholder(tf.uint8, shape=(None,), name=\"input_Y\")\n",
    "\n",
    "    one_hot_y = tf.one_hot(y, 10)\n",
    "    flat_x = tf.reshape(x, [-1, rows * cols])\n",
    "\n",
    "X_train_flat, y_train_one_hot = s.run([flat_x, one_hot_y], feed_dict={\n",
    "    x: X_train,\n",
    "    y: y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer neural network\n",
    "\n",
    "We first create a simple single layer neural network. We use a one_hot encode layer and a reshape layer to create a flat X. This will prove to be troublesome in the future, but it works for the first version. We use a softmax function to compute the predicted classes.\n",
    "\n",
    "Also, for debugging, we create the `correct_prediction` and `accuracy` nodes, to avoid having to compute these metrics by hand after running the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = 28\n",
    "cols = 28\n",
    "classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "with tf.variable_scope(\"1nn\"):\n",
    "    weights = tf.get_variable(\"weights\", \n",
    "                              [rows * cols, classes],\n",
    "                              initializer=tf.random_normal_initializer(mean=0, stddev=1e-3),\n",
    "                              dtype=tf.float32)\n",
    "    b = tf.get_variable(\"bias\", [classes],\n",
    "                    initializer=tf.random_uniform_initializer(minval=0, maxval=1e-3),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "    input_X = tf.placeholder(tf.float32, shape=(None, rows, cols), name=\"input_X\")\n",
    "    input_y = tf.placeholder(tf.uint8, shape=(None,), name=\"input_Y\")\n",
    "\n",
    "    one_hot_y = tf.one_hot(input_y, classes)\n",
    "    flat_X = tf.reshape(input_X, [-1, rows * cols])\n",
    "    \n",
    "    predicted_y = tf.nn.softmax(tf.matmul(flat_X, weights) + b)\n",
    "    class_loss = -tf.log(predicted_y) * one_hot_y - tf.log(1 - predicted_y) * (1 - one_hot_y)\n",
    "    loss = tf.reduce_mean(class_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.cast(input_y, tf.int64), tf.argmax(predicted_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that the weights get correctly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  5.79033105e-04,   5.30614925e-04,   3.36251920e-04, ...,\n",
      "          2.76694028e-03,   1.15641451e-03,  -1.63831748e-03],\n",
      "       [ -1.58736168e-03,   1.21381214e-04,  -2.19016010e-03, ...,\n",
      "          1.32015243e-03,   4.94840438e-04,   1.25256577e-03],\n",
      "       [ -3.99170749e-05,   3.11565556e-04,   6.22138497e-04, ...,\n",
      "         -6.53253344e-04,   4.62306161e-05,  -8.24748189e-04],\n",
      "       ..., \n",
      "       [  1.43660349e-04,  -8.87794231e-05,   2.98190513e-04, ...,\n",
      "          2.59238703e-04,   4.58338851e-04,  -1.08343644e-04],\n",
      "       [  8.14772618e-04,   6.56542543e-04,   1.24888425e-03, ...,\n",
      "         -9.27994843e-04,  -7.12979992e-04,   1.58063893e-03],\n",
      "       [  5.21749025e-04,  -1.53792300e-03,   1.33906677e-03, ...,\n",
      "         -1.45935116e-03,  -2.51419330e-03,   1.67449866e-03]], dtype=float32), array([  2.86260271e-04,   9.36999510e-04,   8.29001729e-05,\n",
      "         5.16016968e-04,   8.68820585e-04,   6.75416377e-04,\n",
      "         8.76370585e-04,   2.40242618e-04,   2.61650217e-04,\n",
      "         6.24309832e-05], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "print(s.run([weights, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that the network computes different values correctly, or at least that they seem to be roughly correct. I struggled a bit with different data shapes and wrong initialization. I decided to keep this step to show what pains I went through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  5.79033105e-04,   5.30614925e-04,   3.36251920e-04, ...,\n",
       "           2.76694028e-03,   1.15641451e-03,  -1.63831748e-03],\n",
       "        [ -1.58736168e-03,   1.21381214e-04,  -2.19016010e-03, ...,\n",
       "           1.32015243e-03,   4.94840438e-04,   1.25256577e-03],\n",
       "        [ -3.99170749e-05,   3.11565556e-04,   6.22138497e-04, ...,\n",
       "          -6.53253344e-04,   4.62306161e-05,  -8.24748189e-04],\n",
       "        ..., \n",
       "        [  1.43660349e-04,  -8.87794231e-05,   2.98190513e-04, ...,\n",
       "           2.59238703e-04,   4.58338851e-04,  -1.08343644e-04],\n",
       "        [  8.14772618e-04,   6.56542543e-04,   1.24888425e-03, ...,\n",
       "          -9.27994843e-04,  -7.12979992e-04,   1.58063893e-03],\n",
       "        [  5.21749025e-04,  -1.53792300e-03,   1.33906677e-03, ...,\n",
       "          -1.45935116e-03,  -2.51419330e-03,   1.67449866e-03]], dtype=float32),\n",
       " array([  2.86260271e-04,   9.36999510e-04,   8.29001729e-05,\n",
       "          5.16016968e-04,   8.68820585e-04,   6.75416377e-04,\n",
       "          8.76370585e-04,   2.40242618e-04,   2.61650217e-04,\n",
       "          6.24309832e-05], dtype=float32),\n",
       " array([[ 0.10137516,  0.10030492,  0.09839335,  0.10016569,  0.10039964,\n",
       "          0.10035313,  0.09931353,  0.09967161,  0.09963091,  0.10039211],\n",
       "        [ 0.10024301,  0.10021225,  0.10119337,  0.09891429,  0.10105252,\n",
       "          0.09899658,  0.10070042,  0.09855074,  0.10112355,  0.0990133 ],\n",
       "        [ 0.09860776,  0.0992314 ,  0.09991227,  0.1002026 ,  0.10112312,\n",
       "          0.10064189,  0.09889916,  0.09989031,  0.10192948,  0.099562  ]], dtype=float32),\n",
       " array([[ 0.10688964,  0.10569935,  0.10357691,  0.10554467,  0.10580463,\n",
       "          2.29906011,  0.10459802,  0.1049957 ,  0.10495048,  0.10579628],\n",
       "        [ 2.30015802,  0.10559641,  0.10668736,  0.10415487,  0.10653067,\n",
       "          0.10424622,  0.10613909,  0.10375152,  0.10660971,  0.10426481],\n",
       "        [ 0.1038148 ,  0.10450691,  0.10526306,  0.10558567,  2.29141641,\n",
       "          0.106074  ,  0.10413814,  0.10523862,  0.10750669,  0.10487396]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       " 0.32444909,\n",
       " array([False, False, False], dtype=bool),\n",
       " 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.run([weights, b, predicted_y, class_loss, one_hot_y, loss, correct_prediction, accuracy],\n",
    "     {\n",
    "         input_X: X_train[:3],\n",
    "         input_y: y_train[:3]\n",
    "     })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute accuracy metrics when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "losses = []\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "for i in range(30):\n",
    "    s.run(optimizer, {input_X: X_train, input_y: y_train})\n",
    "    loss_i = s.run(loss,  {input_X: X_train, input_y: y_train})\n",
    "    losses += [loss_i]\n",
    "    train_accuracies += [s.run(accuracy, {input_X:X_train, input_y: y_train})]\n",
    "    test_accuracies += [s.run(accuracy, {input_X:X_test, input_y: y_test})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11019aeb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw10ppRLQ/w/MzGip5KSjnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c27d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses);\n",
    "plt.plot(train_accuracies)\n",
    "plt.plot(test_accuracies);\n",
    "plt.legend(['loss', 'training accuracy', 'test accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer neural network\n",
    "\n",
    "In this neural network, we use a second 50 neuron layer. The first layer has a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n",
      "sigma 1.0\n"
     ]
    }
   ],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "rows = 28\n",
    "cols = 28\n",
    "classes = 10\n",
    "\n",
    "hidden_size = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "with tf.variable_scope(\"1nn-2layers\"):\n",
    "    # input shape\n",
    "    input_x = tf.placeholder(tf.float32, shape=(None, rows, cols), name=\"input_X\")\n",
    "    label_y = tf.placeholder(tf.uint8, shape=(None,), name=\"input_Y\")\n",
    "\n",
    "    y_ = tf.one_hot(label_y, classes, dtype=tf.float32)\n",
    "    x = tf.reshape(input_x, [-1, rows * cols])\n",
    "    \n",
    "    sigma = np.sqrt(2. / 10.)\n",
    "    sigma = 1.\n",
    "    print(\"sigma\", sigma)\n",
    "  \n",
    "    # input layer\n",
    "    W1 = tf.get_variable(\"weights\", \n",
    "                              [rows * cols, hidden_size],\n",
    "                              initializer=tf.random_normal_initializer(mean=0, stddev=sigma),\n",
    "                              dtype=tf.float32)\n",
    "    b1 = tf.get_variable(\"bias\", [hidden_size],\n",
    "                    initializer=tf.random_normal_initializer(mean=0, stddev=sigma),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "    y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "    # hidden layer\n",
    "    W2 = tf.get_variable(\"hidden_weights\", \n",
    "                              [hidden_size, classes],\n",
    "                              initializer=tf.random_normal_initializer(mean=0, stddev=sigma),\n",
    "                              dtype=tf.float32)\n",
    "    b2 = tf.get_variable(\"hidden_bias\", [classes],\n",
    "                    initializer=tf.random_normal_initializer(mean=0, stddev=sigma),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "    y2 = tf.nn.softmax(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "    class_loss = -tf.log(y2) * y_ - tf.log(1 - y2) * (1 - y_)\n",
    "    loss = tf.reduce_mean(class_loss)\n",
    "\n",
    "    # output\n",
    "    y = y2\n",
    "\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(tf.cast(y_, tf.float32) * tf.log(y), \n",
    "    reduction_indices=[1]))\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    s.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a generator that will provide us with randomized minibatches, to do incremental training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MiniBatchGenerator:\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        self.batch_size = batch_size\n",
    "        self.current_i = 0\n",
    "        self.total_size = len(self.X)\n",
    "        \n",
    "        self.reshuffle()\n",
    "        \n",
    "    def reshuffle(self):        \n",
    "        idxs = list(range(self.total_size))\n",
    "        random.shuffle(idxs)\n",
    "        self.X_shuffled = self.X[idxs]\n",
    "        self.y_shuffled = self.y[idxs]        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        res_X = []\n",
    "        res_y = []\n",
    "        \n",
    "        end_i = self.current_i + self.batch_size\n",
    "        \n",
    "        if end_i > self.total_size:\n",
    "            rem = end_i - self.total_size\n",
    "            res_X = self.X_shuffled[self.current_i:]\n",
    "            res_y = self.y_shuffled[self.current_i:]\n",
    "            self.reshuffle()\n",
    "            res_X = np.append(res_X, self.X_shuffled[:rem], axis=0)\n",
    "            res_y = np.append(res_y, self.y_shuffled[:rem])\n",
    "            self.current_i = rem\n",
    "        else:\n",
    "            res_X = self.X_shuffled[self.current_i:end_i]\n",
    "            res_y = self.y_shuffled[self.current_i:end_i]\n",
    "            self.current_i = end_i\n",
    "        \n",
    "        return res_X, res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 5,  2, 10]), array(['e', 'b', 'j'],\n",
      "      dtype='<U1'))\n",
      "(array([9, 3, 4]), array(['i', 'c', 'd'],\n",
      "      dtype='<U1'))\n",
      "(array([6, 8, 7]), array(['f', 'h', 'g'],\n",
      "      dtype='<U1'))\n",
      "(array([1, 8, 6]), array(['a', 'h', 'f'],\n",
      "      dtype='<U1'))\n",
      "(array([5, 7, 1]), array(['e', 'g', 'a'],\n",
      "      dtype='<U1'))\n",
      "(array([ 5,  4, 10,  1,  8]), array(['e', 'd', 'j', 'a', 'h'],\n",
      "      dtype='<U1'))\n",
      "(array([9, 7, 6, 2, 3]), array(['i', 'g', 'f', 'b', 'c'],\n",
      "      dtype='<U1'))\n",
      "(array([4, 9, 6, 7, 8]), array(['d', 'i', 'f', 'g', 'h'],\n",
      "      dtype='<U1'))\n",
      "(array([10,  5,  3,  2,  1]), array(['j', 'e', 'c', 'b', 'a'],\n",
      "      dtype='<U1'))\n",
      "(array([9, 6, 7, 3, 5]), array(['i', 'f', 'g', 'c', 'e'],\n",
      "      dtype='<U1'))\n"
     ]
    }
   ],
   "source": [
    "batch_gen = MiniBatchGenerator(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "                               np.array(list(\"abcdefghij\")),\n",
    "                               batch_size = 3);\n",
    "\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "\n",
    "batch_gen = MiniBatchGenerator(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "                               np.array(list(\"abcdefghij\")),\n",
    "                               batch_size = 5);\n",
    "\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())\n",
    "print(batch_gen.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:7.2218\n",
      "train accuracy: 0.12284\n",
      "test accuracy: 0.1264\n",
      "\n",
      "loss at iter 1:6.8838\n",
      "train accuracy: 0.12944\n",
      "test accuracy: 0.1334\n",
      "\n",
      "loss at iter 2:6.5931\n",
      "train accuracy: 0.1362\n",
      "test accuracy: 0.1382\n",
      "\n",
      "loss at iter 3:6.3349\n",
      "train accuracy: 0.1429\n",
      "test accuracy: 0.1439\n",
      "\n",
      "loss at iter 4:6.1004\n",
      "train accuracy: 0.14826\n",
      "test accuracy: 0.1501\n",
      "\n",
      "loss at iter 5:5.8849\n",
      "train accuracy: 0.1532\n",
      "test accuracy: 0.1551\n",
      "\n",
      "loss at iter 6:5.6856\n",
      "train accuracy: 0.15802\n",
      "test accuracy: 0.1608\n",
      "\n",
      "loss at iter 7:5.5007\n",
      "train accuracy: 0.16238\n",
      "test accuracy: 0.1652\n",
      "\n",
      "loss at iter 8:5.3290\n",
      "train accuracy: 0.16624\n",
      "test accuracy: 0.1689\n",
      "\n",
      "loss at iter 9:5.1698\n",
      "train accuracy: 0.17002\n",
      "test accuracy: 0.1714\n",
      "\n",
      "loss at iter 10:5.0221\n",
      "train accuracy: 0.17318\n",
      "test accuracy: 0.1751\n",
      "\n",
      "loss at iter 11:4.8853\n",
      "train accuracy: 0.17616\n",
      "test accuracy: 0.1778\n",
      "\n",
      "loss at iter 12:4.7587\n",
      "train accuracy: 0.17888\n",
      "test accuracy: 0.1817\n",
      "\n",
      "loss at iter 13:4.6416\n",
      "train accuracy: 0.18166\n",
      "test accuracy: 0.1856\n",
      "\n",
      "loss at iter 14:4.5333\n",
      "train accuracy: 0.18504\n",
      "test accuracy: 0.1876\n",
      "\n",
      "loss at iter 15:4.4329\n",
      "train accuracy: 0.1877\n",
      "test accuracy: 0.1904\n",
      "\n",
      "loss at iter 16:4.3399\n",
      "train accuracy: 0.19052\n",
      "test accuracy: 0.194\n",
      "\n",
      "loss at iter 17:4.2533\n",
      "train accuracy: 0.19284\n",
      "test accuracy: 0.1974\n",
      "\n",
      "loss at iter 18:4.1727\n",
      "train accuracy: 0.19502\n",
      "test accuracy: 0.2\n",
      "\n",
      "loss at iter 19:4.0973\n",
      "train accuracy: 0.19758\n",
      "test accuracy: 0.2032\n",
      "\n",
      "loss at iter 20:4.0265\n",
      "train accuracy: 0.20016\n",
      "test accuracy: 0.2071\n",
      "\n",
      "loss at iter 21:3.9599\n",
      "train accuracy: 0.20264\n",
      "test accuracy: 0.2082\n",
      "\n",
      "loss at iter 22:3.8970\n",
      "train accuracy: 0.20516\n",
      "test accuracy: 0.2114\n",
      "\n",
      "loss at iter 23:3.8374\n",
      "train accuracy: 0.2077\n",
      "test accuracy: 0.2143\n",
      "\n",
      "loss at iter 24:3.7807\n",
      "train accuracy: 0.20992\n",
      "test accuracy: 0.2154\n",
      "\n",
      "loss at iter 25:3.7267\n",
      "train accuracy: 0.21248\n",
      "test accuracy: 0.2176\n",
      "\n",
      "loss at iter 26:3.6751\n",
      "train accuracy: 0.21488\n",
      "test accuracy: 0.2207\n",
      "\n",
      "loss at iter 27:3.6257\n",
      "train accuracy: 0.21718\n",
      "test accuracy: 0.2235\n",
      "\n",
      "loss at iter 28:3.5784\n",
      "train accuracy: 0.21934\n",
      "test accuracy: 0.2273\n",
      "\n",
      "loss at iter 29:3.5329\n",
      "train accuracy: 0.22156\n",
      "test accuracy: 0.2297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "for i in range(30):\n",
    "    s.run(optimizer, {input_x: X_train, label_y: y_train})\n",
    "    loss_i = s.run(loss,  {input_x: X_train, label_y: y_train})\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train accuracy:\", s.run(accuracy, {input_x:X_train, label_y: y_train}))\n",
    "    print(\"test accuracy:\", s.run(accuracy, {input_x:X_test, label_y: y_test}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:2.3026\n",
      "train accuracy: 0.09864\n",
      "loss at iter 1:2.3021\n",
      "train accuracy: 0.11356\n",
      "loss at iter 2:2.3018\n",
      "train accuracy: 0.11356\n",
      "loss at iter 3:2.3016\n",
      "train accuracy: 0.11356\n",
      "loss at iter 4:2.3015\n",
      "train accuracy: 0.11356\n",
      "loss at iter 5:2.3014\n",
      "train accuracy: 0.11356\n",
      "loss at iter 6:2.3013\n",
      "train accuracy: 0.11356\n",
      "loss at iter 7:2.3012\n",
      "train accuracy: 0.11356\n",
      "loss at iter 8:2.3012\n",
      "train accuracy: 0.11356\n",
      "loss at iter 9:2.3012\n",
      "train accuracy: 0.11356\n",
      "loss at iter 10:2.3011\n",
      "train accuracy: 0.11356\n",
      "loss at iter 11:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 12:2.3011\n",
      "train accuracy: 0.11356\n",
      "loss at iter 13:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 14:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 15:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 16:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 17:2.3012\n",
      "train accuracy: 0.11356\n",
      "loss at iter 18:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 19:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 20:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 21:2.3008\n",
      "train accuracy: 0.11356\n",
      "loss at iter 22:2.3009\n",
      "train accuracy: 0.11356\n",
      "loss at iter 23:2.3009\n",
      "train accuracy: 0.11356\n",
      "loss at iter 24:2.3009\n",
      "train accuracy: 0.11356\n",
      "loss at iter 25:2.3009\n",
      "train accuracy: 0.11356\n",
      "loss at iter 26:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 27:2.3010\n",
      "train accuracy: 0.11356\n",
      "loss at iter 28:2.3009\n",
      "train accuracy: 0.11356\n",
      "loss at iter 29:2.3008\n",
      "train accuracy: 0.11356\n",
      "test_accuracy: [0.1135]\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(30):\n",
    "    loss_i, accuracy_i, optimizer_i = s.run([\n",
    "        loss, accuracy, optimizer\n",
    "    ],  {input_x: X_train, \n",
    "                     label_y: y_train})\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train accuracy:\", accuracy_i)\n",
    "\n",
    "accuracy_i = s.run([\n",
    "    accuracy\n",
    "],  {input_x: X_test, \n",
    "     label_y: y_test})\n",
    "\n",
    "print(\"test_accuracy:\", accuracy_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(50000,)\n",
      "loss at iter 0:2.3025\n",
      "train accuracy: 0.109\n",
      "loss at iter 1:2.3017\n",
      "train accuracy: 0.11\n",
      "loss at iter 2:2.3025\n",
      "train accuracy: 0.113\n",
      "loss at iter 3:2.3016\n",
      "train accuracy: 0.109\n",
      "loss at iter 4:2.3013\n",
      "train accuracy: 0.125\n",
      "loss at iter 5:2.3016\n",
      "train accuracy: 0.119\n",
      "loss at iter 6:2.3047\n",
      "train accuracy: 0.102\n",
      "loss at iter 7:2.3019\n",
      "train accuracy: 0.109\n",
      "loss at iter 8:2.3001\n",
      "train accuracy: 0.123\n",
      "loss at iter 9:2.3029\n",
      "train accuracy: 0.117\n",
      "loss at iter 10:2.3032\n",
      "train accuracy: 0.098\n",
      "loss at iter 11:2.3020\n",
      "train accuracy: 0.108\n",
      "loss at iter 12:2.3004\n",
      "train accuracy: 0.116\n",
      "loss at iter 13:2.2986\n",
      "train accuracy: 0.127\n",
      "loss at iter 14:2.3013\n",
      "train accuracy: 0.096\n",
      "loss at iter 15:2.3016\n",
      "train accuracy: 0.11\n",
      "loss at iter 16:2.2995\n",
      "train accuracy: 0.131\n",
      "loss at iter 17:2.2985\n",
      "train accuracy: 0.115\n",
      "loss at iter 18:2.3023\n",
      "train accuracy: 0.117\n",
      "loss at iter 19:2.3018\n",
      "train accuracy: 0.109\n",
      "loss at iter 20:2.3022\n",
      "train accuracy: 0.11\n",
      "loss at iter 21:2.3037\n",
      "train accuracy: 0.111\n",
      "loss at iter 22:2.3022\n",
      "train accuracy: 0.1\n",
      "loss at iter 23:2.3000\n",
      "train accuracy: 0.127\n",
      "loss at iter 24:2.3043\n",
      "train accuracy: 0.098\n",
      "loss at iter 25:2.2987\n",
      "train accuracy: 0.125\n",
      "loss at iter 26:2.3026\n",
      "train accuracy: 0.113\n",
      "loss at iter 27:2.3029\n",
      "train accuracy: 0.116\n",
      "loss at iter 28:2.2979\n",
      "train accuracy: 0.122\n",
      "loss at iter 29:2.3035\n",
      "train accuracy: 0.102\n",
      "loss at iter 30:2.3022\n",
      "train accuracy: 0.112\n",
      "loss at iter 31:2.2987\n",
      "train accuracy: 0.138\n",
      "loss at iter 32:2.3022\n",
      "train accuracy: 0.107\n",
      "loss at iter 33:2.2997\n",
      "train accuracy: 0.11\n",
      "loss at iter 34:2.3036\n",
      "train accuracy: 0.103\n",
      "loss at iter 35:2.3001\n",
      "train accuracy: 0.118\n",
      "loss at iter 36:2.3020\n",
      "train accuracy: 0.12\n",
      "loss at iter 37:2.2970\n",
      "train accuracy: 0.137\n",
      "loss at iter 38:2.3012\n",
      "train accuracy: 0.105\n",
      "loss at iter 39:2.3021\n",
      "train accuracy: 0.121\n",
      "loss at iter 40:2.3046\n",
      "train accuracy: 0.098\n",
      "loss at iter 41:2.3023\n",
      "train accuracy: 0.106\n",
      "loss at iter 42:2.3010\n",
      "train accuracy: 0.119\n",
      "loss at iter 43:2.3041\n",
      "train accuracy: 0.092\n",
      "loss at iter 44:2.3007\n",
      "train accuracy: 0.121\n",
      "loss at iter 45:2.2987\n",
      "train accuracy: 0.129\n",
      "loss at iter 46:2.3020\n",
      "train accuracy: 0.115\n",
      "loss at iter 47:2.3008\n",
      "train accuracy: 0.107\n",
      "loss at iter 48:2.3047\n",
      "train accuracy: 0.11\n",
      "loss at iter 49:2.3002\n",
      "train accuracy: 0.118\n",
      "loss at iter 50:2.2969\n",
      "train accuracy: 0.137\n",
      "loss at iter 51:2.3013\n",
      "train accuracy: 0.117\n",
      "loss at iter 52:2.3042\n",
      "train accuracy: 0.107\n",
      "loss at iter 53:2.2988\n",
      "train accuracy: 0.132\n",
      "loss at iter 54:2.3019\n",
      "train accuracy: 0.113\n",
      "loss at iter 55:2.3001\n",
      "train accuracy: 0.117\n",
      "loss at iter 56:2.3037\n",
      "train accuracy: 0.098\n",
      "loss at iter 57:2.3004\n",
      "train accuracy: 0.106\n",
      "loss at iter 58:2.3034\n",
      "train accuracy: 0.113\n",
      "loss at iter 59:2.2984\n",
      "train accuracy: 0.122\n",
      "loss at iter 60:2.2996\n",
      "train accuracy: 0.113\n",
      "loss at iter 61:2.3032\n",
      "train accuracy: 0.108\n",
      "loss at iter 62:2.3042\n",
      "train accuracy: 0.1\n",
      "loss at iter 63:2.3014\n",
      "train accuracy: 0.101\n",
      "loss at iter 64:2.3018\n",
      "train accuracy: 0.105\n",
      "loss at iter 65:2.3041\n",
      "train accuracy: 0.109\n",
      "loss at iter 66:2.3013\n",
      "train accuracy: 0.115\n",
      "loss at iter 67:2.3036\n",
      "train accuracy: 0.104\n",
      "loss at iter 68:2.2996\n",
      "train accuracy: 0.138\n",
      "loss at iter 69:2.3009\n",
      "train accuracy: 0.117\n",
      "loss at iter 70:2.3048\n",
      "train accuracy: 0.105\n",
      "loss at iter 71:2.3024\n",
      "train accuracy: 0.104\n",
      "loss at iter 72:2.3027\n",
      "train accuracy: 0.107\n",
      "loss at iter 73:2.3004\n",
      "train accuracy: 0.125\n",
      "loss at iter 74:2.3033\n",
      "train accuracy: 0.095\n",
      "loss at iter 75:2.3010\n",
      "train accuracy: 0.099\n",
      "loss at iter 76:2.3011\n",
      "train accuracy: 0.119\n",
      "loss at iter 77:2.3013\n",
      "train accuracy: 0.114\n",
      "loss at iter 78:2.2976\n",
      "train accuracy: 0.114\n",
      "loss at iter 79:2.3022\n",
      "train accuracy: 0.121\n",
      "loss at iter 80:2.2991\n",
      "train accuracy: 0.12\n",
      "loss at iter 81:2.2997\n",
      "train accuracy: 0.113\n",
      "loss at iter 82:2.3000\n",
      "train accuracy: 0.124\n",
      "loss at iter 83:2.3006\n",
      "train accuracy: 0.105\n",
      "loss at iter 84:2.2999\n",
      "train accuracy: 0.122\n",
      "loss at iter 85:2.3011\n",
      "train accuracy: 0.121\n",
      "loss at iter 86:2.3016\n",
      "train accuracy: 0.105\n",
      "loss at iter 87:2.3033\n",
      "train accuracy: 0.109\n",
      "loss at iter 88:2.3014\n",
      "train accuracy: 0.115\n",
      "loss at iter 89:2.2993\n",
      "train accuracy: 0.124\n",
      "loss at iter 90:2.2981\n",
      "train accuracy: 0.12\n",
      "loss at iter 91:2.2952\n",
      "train accuracy: 0.138\n",
      "loss at iter 92:2.3027\n",
      "train accuracy: 0.109\n",
      "loss at iter 93:2.2984\n",
      "train accuracy: 0.115\n",
      "loss at iter 94:2.3047\n",
      "train accuracy: 0.112\n",
      "loss at iter 95:2.3017\n",
      "train accuracy: 0.112\n",
      "loss at iter 96:2.3016\n",
      "train accuracy: 0.107\n",
      "loss at iter 97:2.3028\n",
      "train accuracy: 0.109\n",
      "loss at iter 98:2.3012\n",
      "train accuracy: 0.115\n",
      "loss at iter 99:2.3021\n",
      "train accuracy: 0.108\n",
      "loss at iter 100:2.2989\n",
      "train accuracy: 0.114\n",
      "loss at iter 101:2.2983\n",
      "train accuracy: 0.121\n",
      "loss at iter 102:2.3027\n",
      "train accuracy: 0.108\n",
      "loss at iter 103:2.3025\n",
      "train accuracy: 0.109\n",
      "loss at iter 104:2.3038\n",
      "train accuracy: 0.107\n",
      "loss at iter 105:2.3004\n",
      "train accuracy: 0.112\n",
      "loss at iter 106:2.2985\n",
      "train accuracy: 0.125\n",
      "loss at iter 107:2.2991\n",
      "train accuracy: 0.116\n",
      "loss at iter 108:2.2997\n",
      "train accuracy: 0.126\n",
      "loss at iter 109:2.2997\n",
      "train accuracy: 0.12\n",
      "loss at iter 110:2.3018\n",
      "train accuracy: 0.116\n",
      "loss at iter 111:2.3014\n",
      "train accuracy: 0.119\n",
      "loss at iter 112:2.2992\n",
      "train accuracy: 0.126\n",
      "loss at iter 113:2.3022\n",
      "train accuracy: 0.112\n",
      "loss at iter 114:2.3021\n",
      "train accuracy: 0.108\n",
      "loss at iter 115:2.3045\n",
      "train accuracy: 0.101\n",
      "loss at iter 116:2.3042\n",
      "train accuracy: 0.104\n",
      "loss at iter 117:2.3012\n",
      "train accuracy: 0.114\n",
      "loss at iter 118:2.2988\n",
      "train accuracy: 0.129\n",
      "loss at iter 119:2.3007\n",
      "train accuracy: 0.119\n",
      "loss at iter 120:2.3015\n",
      "train accuracy: 0.109\n",
      "loss at iter 121:2.2993\n",
      "train accuracy: 0.122\n",
      "loss at iter 122:2.3003\n",
      "train accuracy: 0.118\n",
      "loss at iter 123:2.2988\n",
      "train accuracy: 0.119\n",
      "loss at iter 124:2.3002\n",
      "train accuracy: 0.122\n",
      "loss at iter 125:2.2983\n",
      "train accuracy: 0.112\n",
      "loss at iter 126:2.2994\n",
      "train accuracy: 0.125\n",
      "loss at iter 127:2.3003\n",
      "train accuracy: 0.113\n",
      "loss at iter 128:2.3056\n",
      "train accuracy: 0.103\n",
      "loss at iter 129:2.3020\n",
      "train accuracy: 0.105\n",
      "loss at iter 130:2.2982\n",
      "train accuracy: 0.125\n",
      "loss at iter 131:2.2986\n",
      "train accuracy: 0.121\n",
      "loss at iter 132:2.2980\n",
      "train accuracy: 0.109\n",
      "loss at iter 133:2.3018\n",
      "train accuracy: 0.107\n",
      "loss at iter 134:2.2982\n",
      "train accuracy: 0.123\n",
      "loss at iter 135:2.3069\n",
      "train accuracy: 0.088\n",
      "loss at iter 136:2.3034\n",
      "train accuracy: 0.104\n",
      "loss at iter 137:2.3016\n",
      "train accuracy: 0.106\n",
      "loss at iter 138:2.3004\n",
      "train accuracy: 0.107\n",
      "loss at iter 139:2.2986\n",
      "train accuracy: 0.127\n",
      "loss at iter 140:2.3025\n",
      "train accuracy: 0.112\n",
      "loss at iter 141:2.2983\n",
      "train accuracy: 0.123\n",
      "loss at iter 142:2.3018\n",
      "train accuracy: 0.105\n",
      "loss at iter 143:2.3023\n",
      "train accuracy: 0.107\n",
      "loss at iter 144:2.2977\n",
      "train accuracy: 0.129\n",
      "loss at iter 145:2.3009\n",
      "train accuracy: 0.111\n",
      "loss at iter 146:2.3026\n",
      "train accuracy: 0.097\n",
      "loss at iter 147:2.2988\n",
      "train accuracy: 0.109\n",
      "loss at iter 148:2.3045\n",
      "train accuracy: 0.105\n",
      "loss at iter 149:2.3014\n",
      "train accuracy: 0.109\n",
      "loss at iter 150:2.2994\n",
      "train accuracy: 0.113\n",
      "loss at iter 151:2.3000\n",
      "train accuracy: 0.117\n",
      "loss at iter 152:2.3000\n",
      "train accuracy: 0.111\n",
      "loss at iter 153:2.2994\n",
      "train accuracy: 0.125\n",
      "loss at iter 154:2.2982\n",
      "train accuracy: 0.119\n",
      "loss at iter 155:2.2980\n",
      "train accuracy: 0.125\n",
      "loss at iter 156:2.2955\n",
      "train accuracy: 0.136\n",
      "loss at iter 157:2.2968\n",
      "train accuracy: 0.127\n",
      "loss at iter 158:2.3008\n",
      "train accuracy: 0.106\n",
      "loss at iter 159:2.3030\n",
      "train accuracy: 0.112\n",
      "loss at iter 160:2.3017\n",
      "train accuracy: 0.105\n",
      "loss at iter 161:2.3030\n",
      "train accuracy: 0.105\n",
      "loss at iter 162:2.2978\n",
      "train accuracy: 0.125\n",
      "loss at iter 163:2.2975\n",
      "train accuracy: 0.133\n",
      "loss at iter 164:2.2988\n",
      "train accuracy: 0.123\n",
      "loss at iter 165:2.2977\n",
      "train accuracy: 0.121\n",
      "loss at iter 166:2.2989\n",
      "train accuracy: 0.119\n",
      "loss at iter 167:2.2975\n",
      "train accuracy: 0.115\n",
      "loss at iter 168:2.3031\n",
      "train accuracy: 0.108\n",
      "loss at iter 169:2.3030\n",
      "train accuracy: 0.103\n",
      "loss at iter 170:2.2988\n",
      "train accuracy: 0.117\n",
      "loss at iter 171:2.3063\n",
      "train accuracy: 0.095\n",
      "loss at iter 172:2.3030\n",
      "train accuracy: 0.1\n",
      "loss at iter 173:2.3002\n",
      "train accuracy: 0.115\n",
      "loss at iter 174:2.3000\n",
      "train accuracy: 0.112\n",
      "loss at iter 175:2.2977\n",
      "train accuracy: 0.126\n",
      "loss at iter 176:2.3008\n",
      "train accuracy: 0.101\n",
      "loss at iter 177:2.3022\n",
      "train accuracy: 0.107\n",
      "loss at iter 178:2.2985\n",
      "train accuracy: 0.117\n",
      "loss at iter 179:2.3021\n",
      "train accuracy: 0.111\n",
      "loss at iter 180:2.2981\n",
      "train accuracy: 0.116\n",
      "loss at iter 181:2.3038\n",
      "train accuracy: 0.098\n",
      "loss at iter 182:2.3005\n",
      "train accuracy: 0.104\n",
      "loss at iter 183:2.2994\n",
      "train accuracy: 0.114\n",
      "loss at iter 184:2.2999\n",
      "train accuracy: 0.112\n",
      "loss at iter 185:2.2987\n",
      "train accuracy: 0.119\n",
      "loss at iter 186:2.2998\n",
      "train accuracy: 0.121\n",
      "loss at iter 187:2.3034\n",
      "train accuracy: 0.093\n",
      "loss at iter 188:2.3009\n",
      "train accuracy: 0.107\n",
      "loss at iter 189:2.2993\n",
      "train accuracy: 0.11\n",
      "loss at iter 190:2.3000\n",
      "train accuracy: 0.107\n",
      "loss at iter 191:2.2969\n",
      "train accuracy: 0.131\n",
      "loss at iter 192:2.2977\n",
      "train accuracy: 0.112\n",
      "loss at iter 193:2.3032\n",
      "train accuracy: 0.099\n",
      "loss at iter 194:2.3004\n",
      "train accuracy: 0.101\n",
      "loss at iter 195:2.3001\n",
      "train accuracy: 0.123\n",
      "loss at iter 196:2.2991\n",
      "train accuracy: 0.106\n",
      "loss at iter 197:2.2969\n",
      "train accuracy: 0.123\n",
      "loss at iter 198:2.2961\n",
      "train accuracy: 0.122\n",
      "loss at iter 199:2.2966\n",
      "train accuracy: 0.111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 200:2.3031\n",
      "train accuracy: 0.1\n",
      "loss at iter 201:2.2986\n",
      "train accuracy: 0.102\n",
      "loss at iter 202:2.2980\n",
      "train accuracy: 0.113\n",
      "loss at iter 203:2.2943\n",
      "train accuracy: 0.139\n",
      "loss at iter 204:2.2987\n",
      "train accuracy: 0.121\n",
      "loss at iter 205:2.2974\n",
      "train accuracy: 0.112\n",
      "loss at iter 206:2.2954\n",
      "train accuracy: 0.118\n",
      "loss at iter 207:2.3023\n",
      "train accuracy: 0.105\n",
      "loss at iter 208:2.2966\n",
      "train accuracy: 0.123\n",
      "loss at iter 209:2.2984\n",
      "train accuracy: 0.105\n",
      "loss at iter 210:2.3017\n",
      "train accuracy: 0.1\n",
      "loss at iter 211:2.2974\n",
      "train accuracy: 0.117\n",
      "loss at iter 212:2.2984\n",
      "train accuracy: 0.108\n",
      "loss at iter 213:2.2959\n",
      "train accuracy: 0.125\n",
      "loss at iter 214:2.3004\n",
      "train accuracy: 0.102\n",
      "loss at iter 215:2.2954\n",
      "train accuracy: 0.122\n",
      "loss at iter 216:2.2980\n",
      "train accuracy: 0.115\n",
      "loss at iter 217:2.2988\n",
      "train accuracy: 0.115\n",
      "loss at iter 218:2.2993\n",
      "train accuracy: 0.098\n",
      "loss at iter 219:2.2951\n",
      "train accuracy: 0.117\n",
      "loss at iter 220:2.3001\n",
      "train accuracy: 0.104\n",
      "loss at iter 221:2.3004\n",
      "train accuracy: 0.102\n",
      "loss at iter 222:2.2980\n",
      "train accuracy: 0.122\n",
      "loss at iter 223:2.2929\n",
      "train accuracy: 0.127\n",
      "loss at iter 224:2.2962\n",
      "train accuracy: 0.121\n",
      "loss at iter 225:2.2937\n",
      "train accuracy: 0.127\n",
      "loss at iter 226:2.2949\n",
      "train accuracy: 0.116\n",
      "loss at iter 227:2.2939\n",
      "train accuracy: 0.124\n",
      "loss at iter 228:2.2982\n",
      "train accuracy: 0.105\n",
      "loss at iter 229:2.2975\n",
      "train accuracy: 0.101\n",
      "loss at iter 230:2.2976\n",
      "train accuracy: 0.113\n",
      "loss at iter 231:2.2939\n",
      "train accuracy: 0.107\n",
      "loss at iter 232:2.2956\n",
      "train accuracy: 0.12\n",
      "loss at iter 233:2.2902\n",
      "train accuracy: 0.132\n",
      "loss at iter 234:2.2977\n",
      "train accuracy: 0.115\n",
      "loss at iter 235:2.3000\n",
      "train accuracy: 0.106\n",
      "loss at iter 236:2.2994\n",
      "train accuracy: 0.102\n",
      "loss at iter 237:2.2988\n",
      "train accuracy: 0.091\n",
      "loss at iter 238:2.2989\n",
      "train accuracy: 0.105\n",
      "loss at iter 239:2.2953\n",
      "train accuracy: 0.113\n",
      "loss at iter 240:2.2956\n",
      "train accuracy: 0.112\n",
      "loss at iter 241:2.2936\n",
      "train accuracy: 0.13\n",
      "loss at iter 242:2.2899\n",
      "train accuracy: 0.131\n",
      "loss at iter 243:2.2930\n",
      "train accuracy: 0.125\n",
      "loss at iter 244:2.2985\n",
      "train accuracy: 0.111\n",
      "loss at iter 245:2.2947\n",
      "train accuracy: 0.111\n",
      "loss at iter 246:2.2982\n",
      "train accuracy: 0.1\n",
      "loss at iter 247:2.2902\n",
      "train accuracy: 0.126\n",
      "loss at iter 248:2.2947\n",
      "train accuracy: 0.109\n",
      "loss at iter 249:2.2914\n",
      "train accuracy: 0.113\n",
      "loss at iter 250:2.2887\n",
      "train accuracy: 0.136\n",
      "loss at iter 251:2.2945\n",
      "train accuracy: 0.107\n",
      "loss at iter 252:2.2871\n",
      "train accuracy: 0.13\n",
      "loss at iter 253:2.2859\n",
      "train accuracy: 0.143\n",
      "loss at iter 254:2.2928\n",
      "train accuracy: 0.112\n",
      "loss at iter 255:2.2869\n",
      "train accuracy: 0.131\n",
      "loss at iter 256:2.2875\n",
      "train accuracy: 0.122\n",
      "loss at iter 257:2.2958\n",
      "train accuracy: 0.104\n",
      "loss at iter 258:2.2942\n",
      "train accuracy: 0.107\n",
      "loss at iter 259:2.2924\n",
      "train accuracy: 0.112\n",
      "loss at iter 260:2.2929\n",
      "train accuracy: 0.108\n",
      "loss at iter 261:2.2913\n",
      "train accuracy: 0.114\n",
      "loss at iter 262:2.2892\n",
      "train accuracy: 0.115\n",
      "loss at iter 263:2.2970\n",
      "train accuracy: 0.089\n",
      "loss at iter 264:2.2933\n",
      "train accuracy: 0.095\n",
      "loss at iter 265:2.2901\n",
      "train accuracy: 0.12\n",
      "loss at iter 266:2.2899\n",
      "train accuracy: 0.11\n",
      "loss at iter 267:2.2849\n",
      "train accuracy: 0.132\n",
      "loss at iter 268:2.2887\n",
      "train accuracy: 0.111\n",
      "loss at iter 269:2.2904\n",
      "train accuracy: 0.113\n",
      "loss at iter 270:2.2878\n",
      "train accuracy: 0.112\n",
      "loss at iter 271:2.2939\n",
      "train accuracy: 0.096\n",
      "loss at iter 272:2.2856\n",
      "train accuracy: 0.118\n",
      "loss at iter 273:2.2877\n",
      "train accuracy: 0.114\n",
      "loss at iter 274:2.2850\n",
      "train accuracy: 0.118\n",
      "loss at iter 275:2.2850\n",
      "train accuracy: 0.123\n",
      "loss at iter 276:2.2881\n",
      "train accuracy: 0.104\n",
      "loss at iter 277:2.2862\n",
      "train accuracy: 0.12\n",
      "loss at iter 278:2.2865\n",
      "train accuracy: 0.106\n",
      "loss at iter 279:2.2879\n",
      "train accuracy: 0.111\n",
      "loss at iter 280:2.2886\n",
      "train accuracy: 0.102\n",
      "loss at iter 281:2.2850\n",
      "train accuracy: 0.122\n",
      "loss at iter 282:2.2824\n",
      "train accuracy: 0.123\n",
      "loss at iter 283:2.2838\n",
      "train accuracy: 0.114\n",
      "loss at iter 284:2.2851\n",
      "train accuracy: 0.119\n",
      "loss at iter 285:2.2802\n",
      "train accuracy: 0.125\n",
      "loss at iter 286:2.2820\n",
      "train accuracy: 0.113\n",
      "loss at iter 287:2.2869\n",
      "train accuracy: 0.121\n",
      "loss at iter 288:2.2835\n",
      "train accuracy: 0.135\n",
      "loss at iter 289:2.2814\n",
      "train accuracy: 0.138\n",
      "loss at iter 290:2.2779\n",
      "train accuracy: 0.127\n",
      "loss at iter 291:2.2869\n",
      "train accuracy: 0.106\n",
      "loss at iter 292:2.2788\n",
      "train accuracy: 0.146\n",
      "loss at iter 293:2.2791\n",
      "train accuracy: 0.139\n",
      "loss at iter 294:2.2783\n",
      "train accuracy: 0.161\n",
      "loss at iter 295:2.2749\n",
      "train accuracy: 0.178\n",
      "loss at iter 296:2.2743\n",
      "train accuracy: 0.2\n",
      "loss at iter 297:2.2753\n",
      "train accuracy: 0.173\n",
      "loss at iter 298:2.2778\n",
      "train accuracy: 0.145\n",
      "loss at iter 299:2.2711\n",
      "train accuracy: 0.156\n",
      "loss at iter 300:2.2778\n",
      "train accuracy: 0.139\n",
      "loss at iter 301:2.2768\n",
      "train accuracy: 0.135\n",
      "loss at iter 302:2.2745\n",
      "train accuracy: 0.147\n",
      "loss at iter 303:2.2743\n",
      "train accuracy: 0.154\n",
      "loss at iter 304:2.2674\n",
      "train accuracy: 0.212\n",
      "loss at iter 305:2.2662\n",
      "train accuracy: 0.228\n",
      "loss at iter 306:2.2688\n",
      "train accuracy: 0.186\n",
      "loss at iter 307:2.2682\n",
      "train accuracy: 0.195\n",
      "loss at iter 308:2.2666\n",
      "train accuracy: 0.182\n",
      "loss at iter 309:2.2704\n",
      "train accuracy: 0.158\n",
      "loss at iter 310:2.2657\n",
      "train accuracy: 0.175\n",
      "loss at iter 311:2.2675\n",
      "train accuracy: 0.185\n",
      "loss at iter 312:2.2711\n",
      "train accuracy: 0.179\n",
      "loss at iter 313:2.2606\n",
      "train accuracy: 0.196\n",
      "loss at iter 314:2.2680\n",
      "train accuracy: 0.172\n",
      "loss at iter 315:2.2620\n",
      "train accuracy: 0.189\n",
      "loss at iter 316:2.2579\n",
      "train accuracy: 0.205\n",
      "loss at iter 317:2.2652\n",
      "train accuracy: 0.162\n",
      "loss at iter 318:2.2653\n",
      "train accuracy: 0.175\n",
      "loss at iter 319:2.2578\n",
      "train accuracy: 0.196\n",
      "loss at iter 320:2.2550\n",
      "train accuracy: 0.2\n",
      "loss at iter 321:2.2617\n",
      "train accuracy: 0.189\n",
      "loss at iter 322:2.2604\n",
      "train accuracy: 0.19\n",
      "loss at iter 323:2.2493\n",
      "train accuracy: 0.233\n",
      "loss at iter 324:2.2535\n",
      "train accuracy: 0.191\n",
      "loss at iter 325:2.2529\n",
      "train accuracy: 0.184\n",
      "loss at iter 326:2.2566\n",
      "train accuracy: 0.181\n",
      "loss at iter 327:2.2538\n",
      "train accuracy: 0.196\n",
      "loss at iter 328:2.2509\n",
      "train accuracy: 0.191\n",
      "loss at iter 329:2.2501\n",
      "train accuracy: 0.192\n",
      "loss at iter 330:2.2448\n",
      "train accuracy: 0.216\n",
      "loss at iter 331:2.2453\n",
      "train accuracy: 0.206\n",
      "loss at iter 332:2.2478\n",
      "train accuracy: 0.199\n",
      "loss at iter 333:2.2506\n",
      "train accuracy: 0.207\n",
      "loss at iter 334:2.2414\n",
      "train accuracy: 0.26\n",
      "loss at iter 335:2.2391\n",
      "train accuracy: 0.223\n",
      "loss at iter 336:2.2348\n",
      "train accuracy: 0.226\n",
      "loss at iter 337:2.2401\n",
      "train accuracy: 0.203\n",
      "loss at iter 338:2.2328\n",
      "train accuracy: 0.232\n",
      "loss at iter 339:2.2290\n",
      "train accuracy: 0.225\n",
      "loss at iter 340:2.2435\n",
      "train accuracy: 0.192\n",
      "loss at iter 341:2.2293\n",
      "train accuracy: 0.236\n",
      "loss at iter 342:2.2347\n",
      "train accuracy: 0.207\n",
      "loss at iter 343:2.2276\n",
      "train accuracy: 0.214\n",
      "loss at iter 344:2.2301\n",
      "train accuracy: 0.205\n",
      "loss at iter 345:2.2264\n",
      "train accuracy: 0.214\n",
      "loss at iter 346:2.2237\n",
      "train accuracy: 0.198\n",
      "loss at iter 347:2.2234\n",
      "train accuracy: 0.208\n",
      "loss at iter 348:2.2127\n",
      "train accuracy: 0.241\n",
      "loss at iter 349:2.2268\n",
      "train accuracy: 0.198\n",
      "loss at iter 350:2.2234\n",
      "train accuracy: 0.222\n",
      "loss at iter 351:2.2198\n",
      "train accuracy: 0.233\n",
      "loss at iter 352:2.2201\n",
      "train accuracy: 0.26\n",
      "loss at iter 353:2.2189\n",
      "train accuracy: 0.262\n",
      "loss at iter 354:2.2083\n",
      "train accuracy: 0.301\n",
      "loss at iter 355:2.2020\n",
      "train accuracy: 0.263\n",
      "loss at iter 356:2.2054\n",
      "train accuracy: 0.247\n",
      "loss at iter 357:2.2068\n",
      "train accuracy: 0.233\n",
      "loss at iter 358:2.2101\n",
      "train accuracy: 0.221\n",
      "loss at iter 359:2.1927\n",
      "train accuracy: 0.276\n",
      "loss at iter 360:2.2058\n",
      "train accuracy: 0.211\n",
      "loss at iter 361:2.2035\n",
      "train accuracy: 0.26\n",
      "loss at iter 362:2.1977\n",
      "train accuracy: 0.27\n",
      "loss at iter 363:2.1908\n",
      "train accuracy: 0.291\n",
      "loss at iter 364:2.1786\n",
      "train accuracy: 0.277\n",
      "loss at iter 365:2.1916\n",
      "train accuracy: 0.251\n",
      "loss at iter 366:2.1824\n",
      "train accuracy: 0.235\n",
      "loss at iter 367:2.1859\n",
      "train accuracy: 0.239\n",
      "loss at iter 368:2.1790\n",
      "train accuracy: 0.235\n",
      "loss at iter 369:2.1863\n",
      "train accuracy: 0.221\n",
      "loss at iter 370:2.1842\n",
      "train accuracy: 0.225\n",
      "loss at iter 371:2.1732\n",
      "train accuracy: 0.257\n",
      "loss at iter 372:2.1811\n",
      "train accuracy: 0.235\n",
      "loss at iter 373:2.1746\n",
      "train accuracy: 0.252\n",
      "loss at iter 374:2.1784\n",
      "train accuracy: 0.234\n",
      "loss at iter 375:2.1659\n",
      "train accuracy: 0.28\n",
      "loss at iter 376:2.1555\n",
      "train accuracy: 0.3\n",
      "loss at iter 377:2.1570\n",
      "train accuracy: 0.262\n",
      "loss at iter 378:2.1589\n",
      "train accuracy: 0.255\n",
      "loss at iter 379:2.1458\n",
      "train accuracy: 0.27\n",
      "loss at iter 380:2.1469\n",
      "train accuracy: 0.274\n",
      "loss at iter 381:2.1555\n",
      "train accuracy: 0.249\n",
      "loss at iter 382:2.1492\n",
      "train accuracy: 0.281\n",
      "loss at iter 383:2.1454\n",
      "train accuracy: 0.28\n",
      "loss at iter 384:2.1341\n",
      "train accuracy: 0.296\n",
      "loss at iter 385:2.1248\n",
      "train accuracy: 0.316\n",
      "loss at iter 386:2.1369\n",
      "train accuracy: 0.258\n",
      "loss at iter 387:2.1189\n",
      "train accuracy: 0.299\n",
      "loss at iter 388:2.1199\n",
      "train accuracy: 0.318\n",
      "loss at iter 389:2.1225\n",
      "train accuracy: 0.314\n",
      "loss at iter 390:2.1257\n",
      "train accuracy: 0.275\n",
      "loss at iter 391:2.1251\n",
      "train accuracy: 0.302\n",
      "loss at iter 392:2.1095\n",
      "train accuracy: 0.299\n",
      "loss at iter 393:2.1178\n",
      "train accuracy: 0.303\n",
      "loss at iter 394:2.1120\n",
      "train accuracy: 0.29\n",
      "loss at iter 395:2.1012\n",
      "train accuracy: 0.312\n",
      "loss at iter 396:2.1051\n",
      "train accuracy: 0.307\n",
      "loss at iter 397:2.1128\n",
      "train accuracy: 0.269\n",
      "loss at iter 398:2.1067\n",
      "train accuracy: 0.314\n",
      "loss at iter 399:2.1006\n",
      "train accuracy: 0.305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 400:2.1159\n",
      "train accuracy: 0.279\n",
      "loss at iter 401:2.0939\n",
      "train accuracy: 0.322\n",
      "loss at iter 402:2.0970\n",
      "train accuracy: 0.316\n",
      "loss at iter 403:2.0973\n",
      "train accuracy: 0.352\n",
      "loss at iter 404:2.0854\n",
      "train accuracy: 0.37\n",
      "loss at iter 405:2.0807\n",
      "train accuracy: 0.359\n",
      "loss at iter 406:2.0596\n",
      "train accuracy: 0.392\n",
      "loss at iter 407:2.0557\n",
      "train accuracy: 0.368\n",
      "loss at iter 408:2.0731\n",
      "train accuracy: 0.334\n",
      "loss at iter 409:2.0570\n",
      "train accuracy: 0.368\n",
      "loss at iter 410:2.0613\n",
      "train accuracy: 0.374\n",
      "loss at iter 411:2.0582\n",
      "train accuracy: 0.356\n",
      "loss at iter 412:2.0468\n",
      "train accuracy: 0.381\n",
      "loss at iter 413:2.0246\n",
      "train accuracy: 0.377\n",
      "loss at iter 414:2.0415\n",
      "train accuracy: 0.349\n",
      "loss at iter 415:2.0452\n",
      "train accuracy: 0.354\n",
      "loss at iter 416:2.0392\n",
      "train accuracy: 0.343\n",
      "loss at iter 417:2.0427\n",
      "train accuracy: 0.331\n",
      "loss at iter 418:2.0252\n",
      "train accuracy: 0.371\n",
      "loss at iter 419:2.0239\n",
      "train accuracy: 0.356\n",
      "loss at iter 420:2.0279\n",
      "train accuracy: 0.36\n",
      "loss at iter 421:2.0124\n",
      "train accuracy: 0.371\n",
      "loss at iter 422:2.0162\n",
      "train accuracy: 0.341\n",
      "loss at iter 423:2.0133\n",
      "train accuracy: 0.367\n",
      "loss at iter 424:2.0161\n",
      "train accuracy: 0.348\n",
      "loss at iter 425:1.9985\n",
      "train accuracy: 0.367\n",
      "loss at iter 426:1.9973\n",
      "train accuracy: 0.357\n",
      "loss at iter 427:1.9871\n",
      "train accuracy: 0.382\n",
      "loss at iter 428:2.0037\n",
      "train accuracy: 0.349\n",
      "loss at iter 429:1.9840\n",
      "train accuracy: 0.362\n",
      "loss at iter 430:1.9764\n",
      "train accuracy: 0.348\n",
      "loss at iter 431:1.9862\n",
      "train accuracy: 0.37\n",
      "loss at iter 432:1.9816\n",
      "train accuracy: 0.37\n",
      "loss at iter 433:1.9632\n",
      "train accuracy: 0.378\n",
      "loss at iter 434:1.9646\n",
      "train accuracy: 0.374\n",
      "loss at iter 435:1.9518\n",
      "train accuracy: 0.382\n",
      "loss at iter 436:1.9622\n",
      "train accuracy: 0.398\n",
      "loss at iter 437:1.9502\n",
      "train accuracy: 0.4\n",
      "loss at iter 438:1.9295\n",
      "train accuracy: 0.424\n",
      "loss at iter 439:1.9478\n",
      "train accuracy: 0.397\n",
      "loss at iter 440:1.9335\n",
      "train accuracy: 0.405\n",
      "loss at iter 441:1.9344\n",
      "train accuracy: 0.411\n",
      "loss at iter 442:1.9512\n",
      "train accuracy: 0.375\n",
      "loss at iter 443:1.9411\n",
      "train accuracy: 0.394\n",
      "loss at iter 444:1.9225\n",
      "train accuracy: 0.429\n",
      "loss at iter 445:1.9150\n",
      "train accuracy: 0.454\n",
      "loss at iter 446:1.9225\n",
      "train accuracy: 0.396\n",
      "loss at iter 447:1.9108\n",
      "train accuracy: 0.403\n",
      "loss at iter 448:1.9044\n",
      "train accuracy: 0.426\n",
      "loss at iter 449:1.9219\n",
      "train accuracy: 0.408\n",
      "loss at iter 450:1.8939\n",
      "train accuracy: 0.448\n",
      "loss at iter 451:1.8788\n",
      "train accuracy: 0.438\n",
      "loss at iter 452:1.9068\n",
      "train accuracy: 0.382\n",
      "loss at iter 453:1.8659\n",
      "train accuracy: 0.448\n",
      "loss at iter 454:1.8827\n",
      "train accuracy: 0.418\n",
      "loss at iter 455:1.8585\n",
      "train accuracy: 0.461\n",
      "loss at iter 456:1.8808\n",
      "train accuracy: 0.399\n",
      "loss at iter 457:1.8809\n",
      "train accuracy: 0.398\n",
      "loss at iter 458:1.8749\n",
      "train accuracy: 0.436\n",
      "loss at iter 459:1.8535\n",
      "train accuracy: 0.438\n",
      "loss at iter 460:1.8497\n",
      "train accuracy: 0.453\n",
      "loss at iter 461:1.8694\n",
      "train accuracy: 0.414\n",
      "loss at iter 462:1.8683\n",
      "train accuracy: 0.406\n",
      "loss at iter 463:1.8528\n",
      "train accuracy: 0.448\n",
      "loss at iter 464:1.8218\n",
      "train accuracy: 0.466\n",
      "loss at iter 465:1.8482\n",
      "train accuracy: 0.423\n",
      "loss at iter 466:1.8560\n",
      "train accuracy: 0.441\n",
      "loss at iter 467:1.8463\n",
      "train accuracy: 0.428\n",
      "loss at iter 468:1.8271\n",
      "train accuracy: 0.462\n",
      "loss at iter 469:1.8288\n",
      "train accuracy: 0.482\n",
      "loss at iter 470:1.8028\n",
      "train accuracy: 0.48\n",
      "loss at iter 471:1.8214\n",
      "train accuracy: 0.445\n",
      "loss at iter 472:1.7872\n",
      "train accuracy: 0.481\n",
      "loss at iter 473:1.8090\n",
      "train accuracy: 0.457\n",
      "loss at iter 474:1.8056\n",
      "train accuracy: 0.459\n",
      "loss at iter 475:1.8215\n",
      "train accuracy: 0.431\n",
      "loss at iter 476:1.7685\n",
      "train accuracy: 0.475\n",
      "loss at iter 477:1.8088\n",
      "train accuracy: 0.444\n",
      "loss at iter 478:1.7682\n",
      "train accuracy: 0.474\n",
      "loss at iter 479:1.7764\n",
      "train accuracy: 0.492\n",
      "loss at iter 480:1.7673\n",
      "train accuracy: 0.489\n",
      "loss at iter 481:1.7768\n",
      "train accuracy: 0.477\n",
      "loss at iter 482:1.7415\n",
      "train accuracy: 0.491\n",
      "loss at iter 483:1.7610\n",
      "train accuracy: 0.486\n",
      "loss at iter 484:1.7474\n",
      "train accuracy: 0.477\n",
      "loss at iter 485:1.7233\n",
      "train accuracy: 0.52\n",
      "loss at iter 486:1.7343\n",
      "train accuracy: 0.492\n",
      "loss at iter 487:1.7441\n",
      "train accuracy: 0.498\n",
      "loss at iter 488:1.7720\n",
      "train accuracy: 0.468\n",
      "loss at iter 489:1.7418\n",
      "train accuracy: 0.504\n",
      "loss at iter 490:1.7320\n",
      "train accuracy: 0.477\n",
      "loss at iter 491:1.7071\n",
      "train accuracy: 0.511\n",
      "loss at iter 492:1.7273\n",
      "train accuracy: 0.496\n",
      "loss at iter 493:1.7029\n",
      "train accuracy: 0.501\n",
      "loss at iter 494:1.7262\n",
      "train accuracy: 0.51\n",
      "loss at iter 495:1.7035\n",
      "train accuracy: 0.529\n",
      "loss at iter 496:1.6934\n",
      "train accuracy: 0.505\n",
      "loss at iter 497:1.7108\n",
      "train accuracy: 0.477\n",
      "loss at iter 498:1.6919\n",
      "train accuracy: 0.477\n",
      "loss at iter 499:1.6899\n",
      "train accuracy: 0.5\n",
      "loss at iter 500:1.6765\n",
      "train accuracy: 0.512\n",
      "loss at iter 501:1.6534\n",
      "train accuracy: 0.5\n",
      "loss at iter 502:1.6699\n",
      "train accuracy: 0.492\n",
      "loss at iter 503:1.6517\n",
      "train accuracy: 0.535\n",
      "loss at iter 504:1.6945\n",
      "train accuracy: 0.502\n",
      "loss at iter 505:1.6788\n",
      "train accuracy: 0.492\n",
      "loss at iter 506:1.6624\n",
      "train accuracy: 0.504\n",
      "loss at iter 507:1.6864\n",
      "train accuracy: 0.503\n",
      "loss at iter 508:1.6470\n",
      "train accuracy: 0.519\n",
      "loss at iter 509:1.6574\n",
      "train accuracy: 0.511\n",
      "loss at iter 510:1.6801\n",
      "train accuracy: 0.479\n",
      "loss at iter 511:1.6508\n",
      "train accuracy: 0.532\n",
      "loss at iter 512:1.6466\n",
      "train accuracy: 0.536\n",
      "loss at iter 513:1.6533\n",
      "train accuracy: 0.494\n",
      "loss at iter 514:1.6055\n",
      "train accuracy: 0.535\n",
      "loss at iter 515:1.6427\n",
      "train accuracy: 0.503\n",
      "loss at iter 516:1.6259\n",
      "train accuracy: 0.518\n",
      "loss at iter 517:1.5961\n",
      "train accuracy: 0.529\n",
      "loss at iter 518:1.6318\n",
      "train accuracy: 0.483\n",
      "loss at iter 519:1.6103\n",
      "train accuracy: 0.54\n",
      "loss at iter 520:1.6072\n",
      "train accuracy: 0.527\n",
      "loss at iter 521:1.5808\n",
      "train accuracy: 0.546\n",
      "loss at iter 522:1.5858\n",
      "train accuracy: 0.553\n",
      "loss at iter 523:1.6050\n",
      "train accuracy: 0.518\n",
      "loss at iter 524:1.6297\n",
      "train accuracy: 0.498\n",
      "loss at iter 525:1.5447\n",
      "train accuracy: 0.556\n",
      "loss at iter 526:1.5936\n",
      "train accuracy: 0.541\n",
      "loss at iter 527:1.5689\n",
      "train accuracy: 0.543\n",
      "loss at iter 528:1.5670\n",
      "train accuracy: 0.558\n",
      "loss at iter 529:1.5903\n",
      "train accuracy: 0.521\n",
      "loss at iter 530:1.5888\n",
      "train accuracy: 0.514\n",
      "loss at iter 531:1.5488\n",
      "train accuracy: 0.552\n",
      "loss at iter 532:1.5617\n",
      "train accuracy: 0.53\n",
      "loss at iter 533:1.5475\n",
      "train accuracy: 0.553\n",
      "loss at iter 534:1.4969\n",
      "train accuracy: 0.592\n",
      "loss at iter 535:1.5590\n",
      "train accuracy: 0.544\n",
      "loss at iter 536:1.5316\n",
      "train accuracy: 0.543\n",
      "loss at iter 537:1.5569\n",
      "train accuracy: 0.52\n",
      "loss at iter 538:1.5169\n",
      "train accuracy: 0.567\n",
      "loss at iter 539:1.5164\n",
      "train accuracy: 0.55\n",
      "loss at iter 540:1.5113\n",
      "train accuracy: 0.522\n",
      "loss at iter 541:1.5289\n",
      "train accuracy: 0.534\n",
      "loss at iter 542:1.5049\n",
      "train accuracy: 0.56\n",
      "loss at iter 543:1.5048\n",
      "train accuracy: 0.539\n",
      "loss at iter 544:1.5146\n",
      "train accuracy: 0.532\n",
      "loss at iter 545:1.4699\n",
      "train accuracy: 0.555\n",
      "loss at iter 546:1.5030\n",
      "train accuracy: 0.513\n",
      "loss at iter 547:1.4985\n",
      "train accuracy: 0.542\n",
      "loss at iter 548:1.5020\n",
      "train accuracy: 0.546\n",
      "loss at iter 549:1.4960\n",
      "train accuracy: 0.559\n",
      "loss at iter 550:1.5006\n",
      "train accuracy: 0.559\n",
      "loss at iter 551:1.4874\n",
      "train accuracy: 0.561\n",
      "loss at iter 552:1.5036\n",
      "train accuracy: 0.534\n",
      "loss at iter 553:1.4801\n",
      "train accuracy: 0.566\n",
      "loss at iter 554:1.4579\n",
      "train accuracy: 0.555\n",
      "loss at iter 555:1.4811\n",
      "train accuracy: 0.551\n",
      "loss at iter 556:1.4933\n",
      "train accuracy: 0.545\n",
      "loss at iter 557:1.4967\n",
      "train accuracy: 0.546\n",
      "loss at iter 558:1.4650\n",
      "train accuracy: 0.55\n",
      "loss at iter 559:1.4699\n",
      "train accuracy: 0.559\n",
      "loss at iter 560:1.4347\n",
      "train accuracy: 0.571\n",
      "loss at iter 561:1.4669\n",
      "train accuracy: 0.566\n",
      "loss at iter 562:1.4328\n",
      "train accuracy: 0.577\n",
      "loss at iter 563:1.4033\n",
      "train accuracy: 0.575\n",
      "loss at iter 564:1.4300\n",
      "train accuracy: 0.568\n",
      "loss at iter 565:1.4226\n",
      "train accuracy: 0.576\n",
      "loss at iter 566:1.4279\n",
      "train accuracy: 0.566\n",
      "loss at iter 567:1.4356\n",
      "train accuracy: 0.562\n",
      "loss at iter 568:1.4421\n",
      "train accuracy: 0.55\n",
      "loss at iter 569:1.4140\n",
      "train accuracy: 0.562\n",
      "loss at iter 570:1.4494\n",
      "train accuracy: 0.566\n",
      "loss at iter 571:1.4061\n",
      "train accuracy: 0.593\n",
      "loss at iter 572:1.3840\n",
      "train accuracy: 0.607\n",
      "loss at iter 573:1.3822\n",
      "train accuracy: 0.581\n",
      "loss at iter 574:1.4071\n",
      "train accuracy: 0.574\n",
      "loss at iter 575:1.4237\n",
      "train accuracy: 0.566\n",
      "loss at iter 576:1.4094\n",
      "train accuracy: 0.577\n",
      "loss at iter 577:1.3845\n",
      "train accuracy: 0.582\n",
      "loss at iter 578:1.4030\n",
      "train accuracy: 0.577\n",
      "loss at iter 579:1.3658\n",
      "train accuracy: 0.59\n",
      "loss at iter 580:1.3980\n",
      "train accuracy: 0.583\n",
      "loss at iter 581:1.3852\n",
      "train accuracy: 0.58\n",
      "loss at iter 582:1.3994\n",
      "train accuracy: 0.592\n",
      "loss at iter 583:1.3875\n",
      "train accuracy: 0.589\n",
      "loss at iter 584:1.3794\n",
      "train accuracy: 0.589\n",
      "loss at iter 585:1.3947\n",
      "train accuracy: 0.565\n",
      "loss at iter 586:1.3493\n",
      "train accuracy: 0.588\n",
      "loss at iter 587:1.3611\n",
      "train accuracy: 0.608\n",
      "loss at iter 588:1.3684\n",
      "train accuracy: 0.593\n",
      "loss at iter 589:1.3872\n",
      "train accuracy: 0.588\n",
      "loss at iter 590:1.3675\n",
      "train accuracy: 0.583\n",
      "loss at iter 591:1.3404\n",
      "train accuracy: 0.582\n",
      "loss at iter 592:1.3423\n",
      "train accuracy: 0.597\n",
      "loss at iter 593:1.3543\n",
      "train accuracy: 0.588\n",
      "loss at iter 594:1.3457\n",
      "train accuracy: 0.568\n",
      "loss at iter 595:1.3344\n",
      "train accuracy: 0.6\n",
      "loss at iter 596:1.3413\n",
      "train accuracy: 0.583\n",
      "loss at iter 597:1.2925\n",
      "train accuracy: 0.598\n",
      "loss at iter 598:1.3188\n",
      "train accuracy: 0.58\n",
      "loss at iter 599:1.3237\n",
      "train accuracy: 0.583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 600:1.3283\n",
      "train accuracy: 0.602\n",
      "loss at iter 601:1.3146\n",
      "train accuracy: 0.587\n",
      "loss at iter 602:1.3139\n",
      "train accuracy: 0.611\n",
      "loss at iter 603:1.3352\n",
      "train accuracy: 0.555\n",
      "loss at iter 604:1.2913\n",
      "train accuracy: 0.586\n",
      "loss at iter 605:1.2945\n",
      "train accuracy: 0.611\n",
      "loss at iter 606:1.3495\n",
      "train accuracy: 0.559\n",
      "loss at iter 607:1.3274\n",
      "train accuracy: 0.58\n",
      "loss at iter 608:1.3130\n",
      "train accuracy: 0.599\n",
      "loss at iter 609:1.3193\n",
      "train accuracy: 0.597\n",
      "loss at iter 610:1.2819\n",
      "train accuracy: 0.643\n",
      "loss at iter 611:1.3194\n",
      "train accuracy: 0.61\n",
      "loss at iter 612:1.3095\n",
      "train accuracy: 0.595\n",
      "loss at iter 613:1.2776\n",
      "train accuracy: 0.616\n",
      "loss at iter 614:1.2908\n",
      "train accuracy: 0.61\n",
      "loss at iter 615:1.2687\n",
      "train accuracy: 0.617\n",
      "loss at iter 616:1.3195\n",
      "train accuracy: 0.588\n",
      "loss at iter 617:1.2731\n",
      "train accuracy: 0.639\n",
      "loss at iter 618:1.2844\n",
      "train accuracy: 0.603\n",
      "loss at iter 619:1.2627\n",
      "train accuracy: 0.622\n",
      "loss at iter 620:1.2650\n",
      "train accuracy: 0.611\n",
      "loss at iter 621:1.2763\n",
      "train accuracy: 0.628\n",
      "loss at iter 622:1.2762\n",
      "train accuracy: 0.627\n",
      "loss at iter 623:1.2601\n",
      "train accuracy: 0.623\n",
      "loss at iter 624:1.2668\n",
      "train accuracy: 0.615\n",
      "loss at iter 625:1.2476\n",
      "train accuracy: 0.642\n",
      "loss at iter 626:1.2871\n",
      "train accuracy: 0.583\n",
      "loss at iter 627:1.2722\n",
      "train accuracy: 0.603\n",
      "loss at iter 628:1.2575\n",
      "train accuracy: 0.596\n",
      "loss at iter 629:1.2594\n",
      "train accuracy: 0.614\n",
      "loss at iter 630:1.2318\n",
      "train accuracy: 0.624\n",
      "loss at iter 631:1.2328\n",
      "train accuracy: 0.617\n",
      "loss at iter 632:1.2293\n",
      "train accuracy: 0.61\n",
      "loss at iter 633:1.2010\n",
      "train accuracy: 0.639\n",
      "loss at iter 634:1.2184\n",
      "train accuracy: 0.618\n",
      "loss at iter 635:1.2383\n",
      "train accuracy: 0.61\n",
      "loss at iter 636:1.2084\n",
      "train accuracy: 0.645\n",
      "loss at iter 637:1.2454\n",
      "train accuracy: 0.599\n",
      "loss at iter 638:1.2135\n",
      "train accuracy: 0.625\n",
      "loss at iter 639:1.2324\n",
      "train accuracy: 0.615\n",
      "loss at iter 640:1.2014\n",
      "train accuracy: 0.649\n",
      "loss at iter 641:1.2283\n",
      "train accuracy: 0.639\n",
      "loss at iter 642:1.2465\n",
      "train accuracy: 0.617\n",
      "loss at iter 643:1.2078\n",
      "train accuracy: 0.645\n",
      "loss at iter 644:1.2102\n",
      "train accuracy: 0.607\n",
      "loss at iter 645:1.1994\n",
      "train accuracy: 0.628\n",
      "loss at iter 646:1.2002\n",
      "train accuracy: 0.643\n",
      "loss at iter 647:1.2136\n",
      "train accuracy: 0.626\n",
      "loss at iter 648:1.2131\n",
      "train accuracy: 0.597\n",
      "loss at iter 649:1.2312\n",
      "train accuracy: 0.618\n",
      "loss at iter 650:1.1892\n",
      "train accuracy: 0.635\n",
      "loss at iter 651:1.2205\n",
      "train accuracy: 0.614\n",
      "loss at iter 652:1.1878\n",
      "train accuracy: 0.636\n",
      "loss at iter 653:1.1985\n",
      "train accuracy: 0.642\n",
      "loss at iter 654:1.1836\n",
      "train accuracy: 0.627\n",
      "loss at iter 655:1.1771\n",
      "train accuracy: 0.651\n",
      "loss at iter 656:1.1649\n",
      "train accuracy: 0.641\n",
      "loss at iter 657:1.1895\n",
      "train accuracy: 0.65\n",
      "loss at iter 658:1.1899\n",
      "train accuracy: 0.637\n",
      "loss at iter 659:1.1552\n",
      "train accuracy: 0.666\n",
      "loss at iter 660:1.1479\n",
      "train accuracy: 0.648\n",
      "loss at iter 661:1.1426\n",
      "train accuracy: 0.65\n",
      "loss at iter 662:1.1890\n",
      "train accuracy: 0.614\n",
      "loss at iter 663:1.1719\n",
      "train accuracy: 0.629\n",
      "loss at iter 664:1.2106\n",
      "train accuracy: 0.632\n",
      "loss at iter 665:1.1751\n",
      "train accuracy: 0.632\n",
      "loss at iter 666:1.1865\n",
      "train accuracy: 0.633\n",
      "loss at iter 667:1.1851\n",
      "train accuracy: 0.626\n",
      "loss at iter 668:1.1556\n",
      "train accuracy: 0.653\n",
      "loss at iter 669:1.1648\n",
      "train accuracy: 0.634\n",
      "loss at iter 670:1.1830\n",
      "train accuracy: 0.64\n",
      "loss at iter 671:1.1710\n",
      "train accuracy: 0.625\n",
      "loss at iter 672:1.1151\n",
      "train accuracy: 0.692\n",
      "loss at iter 673:1.1731\n",
      "train accuracy: 0.648\n",
      "loss at iter 674:1.1583\n",
      "train accuracy: 0.668\n",
      "loss at iter 675:1.1567\n",
      "train accuracy: 0.664\n",
      "loss at iter 676:1.1795\n",
      "train accuracy: 0.639\n",
      "loss at iter 677:1.1411\n",
      "train accuracy: 0.651\n",
      "loss at iter 678:1.1318\n",
      "train accuracy: 0.651\n",
      "loss at iter 679:1.1389\n",
      "train accuracy: 0.67\n",
      "loss at iter 680:1.1314\n",
      "train accuracy: 0.671\n",
      "loss at iter 681:1.1755\n",
      "train accuracy: 0.626\n",
      "loss at iter 682:1.1633\n",
      "train accuracy: 0.651\n",
      "loss at iter 683:1.1280\n",
      "train accuracy: 0.662\n",
      "loss at iter 684:1.0935\n",
      "train accuracy: 0.685\n",
      "loss at iter 685:1.1346\n",
      "train accuracy: 0.64\n",
      "loss at iter 686:1.0977\n",
      "train accuracy: 0.679\n",
      "loss at iter 687:1.1258\n",
      "train accuracy: 0.668\n",
      "loss at iter 688:1.1168\n",
      "train accuracy: 0.652\n",
      "loss at iter 689:1.1161\n",
      "train accuracy: 0.658\n",
      "loss at iter 690:1.1490\n",
      "train accuracy: 0.654\n",
      "loss at iter 691:1.1537\n",
      "train accuracy: 0.649\n",
      "loss at iter 692:1.1074\n",
      "train accuracy: 0.656\n",
      "loss at iter 693:1.1116\n",
      "train accuracy: 0.663\n",
      "loss at iter 694:1.0792\n",
      "train accuracy: 0.694\n",
      "loss at iter 695:1.1162\n",
      "train accuracy: 0.634\n",
      "loss at iter 696:1.1162\n",
      "train accuracy: 0.649\n",
      "loss at iter 697:1.0886\n",
      "train accuracy: 0.642\n",
      "loss at iter 698:1.0758\n",
      "train accuracy: 0.689\n",
      "loss at iter 699:1.1035\n",
      "train accuracy: 0.664\n",
      "loss at iter 700:1.0931\n",
      "train accuracy: 0.653\n",
      "loss at iter 701:1.0603\n",
      "train accuracy: 0.692\n",
      "loss at iter 702:1.1316\n",
      "train accuracy: 0.66\n",
      "loss at iter 703:1.0995\n",
      "train accuracy: 0.683\n",
      "loss at iter 704:1.1237\n",
      "train accuracy: 0.638\n",
      "loss at iter 705:1.0900\n",
      "train accuracy: 0.675\n",
      "loss at iter 706:1.0688\n",
      "train accuracy: 0.654\n",
      "loss at iter 707:1.0820\n",
      "train accuracy: 0.681\n",
      "loss at iter 708:1.1079\n",
      "train accuracy: 0.642\n",
      "loss at iter 709:1.0989\n",
      "train accuracy: 0.665\n",
      "loss at iter 710:1.0970\n",
      "train accuracy: 0.664\n",
      "loss at iter 711:1.0621\n",
      "train accuracy: 0.666\n",
      "loss at iter 712:1.0836\n",
      "train accuracy: 0.667\n",
      "loss at iter 713:1.0828\n",
      "train accuracy: 0.698\n",
      "loss at iter 714:1.0833\n",
      "train accuracy: 0.686\n",
      "loss at iter 715:1.0952\n",
      "train accuracy: 0.668\n",
      "loss at iter 716:1.0952\n",
      "train accuracy: 0.682\n",
      "loss at iter 717:1.0449\n",
      "train accuracy: 0.727\n",
      "loss at iter 718:1.0853\n",
      "train accuracy: 0.672\n",
      "loss at iter 719:1.0848\n",
      "train accuracy: 0.668\n",
      "loss at iter 720:1.0556\n",
      "train accuracy: 0.699\n",
      "loss at iter 721:1.0603\n",
      "train accuracy: 0.692\n",
      "loss at iter 722:1.0898\n",
      "train accuracy: 0.691\n",
      "loss at iter 723:1.0510\n",
      "train accuracy: 0.688\n",
      "loss at iter 724:1.0737\n",
      "train accuracy: 0.706\n",
      "loss at iter 725:1.0762\n",
      "train accuracy: 0.658\n",
      "loss at iter 726:1.0582\n",
      "train accuracy: 0.688\n",
      "loss at iter 727:1.0327\n",
      "train accuracy: 0.717\n",
      "loss at iter 728:1.0701\n",
      "train accuracy: 0.688\n",
      "loss at iter 729:1.0384\n",
      "train accuracy: 0.693\n",
      "loss at iter 730:1.0773\n",
      "train accuracy: 0.677\n",
      "loss at iter 731:1.0241\n",
      "train accuracy: 0.715\n",
      "loss at iter 732:0.9978\n",
      "train accuracy: 0.713\n",
      "loss at iter 733:1.0560\n",
      "train accuracy: 0.687\n",
      "loss at iter 734:1.0617\n",
      "train accuracy: 0.696\n",
      "loss at iter 735:1.0468\n",
      "train accuracy: 0.687\n",
      "loss at iter 736:1.0556\n",
      "train accuracy: 0.681\n",
      "loss at iter 737:0.9967\n",
      "train accuracy: 0.718\n",
      "loss at iter 738:1.0202\n",
      "train accuracy: 0.702\n",
      "loss at iter 739:1.0068\n",
      "train accuracy: 0.718\n",
      "loss at iter 740:1.0302\n",
      "train accuracy: 0.682\n",
      "loss at iter 741:1.0034\n",
      "train accuracy: 0.691\n",
      "loss at iter 742:1.0260\n",
      "train accuracy: 0.699\n",
      "loss at iter 743:1.0211\n",
      "train accuracy: 0.695\n",
      "loss at iter 744:0.9822\n",
      "train accuracy: 0.706\n",
      "loss at iter 745:1.0370\n",
      "train accuracy: 0.692\n",
      "loss at iter 746:1.0340\n",
      "train accuracy: 0.693\n",
      "loss at iter 747:1.0189\n",
      "train accuracy: 0.702\n",
      "loss at iter 748:1.0471\n",
      "train accuracy: 0.697\n",
      "loss at iter 749:1.0514\n",
      "train accuracy: 0.682\n",
      "loss at iter 750:0.9928\n",
      "train accuracy: 0.717\n",
      "loss at iter 751:1.0094\n",
      "train accuracy: 0.721\n",
      "loss at iter 752:1.0242\n",
      "train accuracy: 0.707\n",
      "loss at iter 753:0.9853\n",
      "train accuracy: 0.725\n",
      "loss at iter 754:1.0407\n",
      "train accuracy: 0.671\n",
      "loss at iter 755:0.9772\n",
      "train accuracy: 0.712\n",
      "loss at iter 756:1.0122\n",
      "train accuracy: 0.716\n",
      "loss at iter 757:1.0854\n",
      "train accuracy: 0.651\n",
      "loss at iter 758:1.0199\n",
      "train accuracy: 0.717\n",
      "loss at iter 759:1.0007\n",
      "train accuracy: 0.708\n",
      "loss at iter 760:1.0212\n",
      "train accuracy: 0.699\n",
      "loss at iter 761:1.0401\n",
      "train accuracy: 0.702\n",
      "loss at iter 762:0.9938\n",
      "train accuracy: 0.684\n",
      "loss at iter 763:0.9551\n",
      "train accuracy: 0.745\n",
      "loss at iter 764:0.9916\n",
      "train accuracy: 0.718\n",
      "loss at iter 765:1.0087\n",
      "train accuracy: 0.706\n",
      "loss at iter 766:0.9532\n",
      "train accuracy: 0.732\n",
      "loss at iter 767:1.0139\n",
      "train accuracy: 0.686\n",
      "loss at iter 768:0.9727\n",
      "train accuracy: 0.732\n",
      "loss at iter 769:1.0032\n",
      "train accuracy: 0.686\n",
      "loss at iter 770:1.0130\n",
      "train accuracy: 0.705\n",
      "loss at iter 771:1.0216\n",
      "train accuracy: 0.707\n",
      "loss at iter 772:0.9863\n",
      "train accuracy: 0.725\n",
      "loss at iter 773:1.0198\n",
      "train accuracy: 0.698\n",
      "loss at iter 774:0.9466\n",
      "train accuracy: 0.719\n",
      "loss at iter 775:0.9505\n",
      "train accuracy: 0.728\n",
      "loss at iter 776:0.9671\n",
      "train accuracy: 0.712\n",
      "loss at iter 777:0.9869\n",
      "train accuracy: 0.705\n",
      "loss at iter 778:0.9604\n",
      "train accuracy: 0.731\n",
      "loss at iter 779:0.9590\n",
      "train accuracy: 0.725\n",
      "loss at iter 780:0.9790\n",
      "train accuracy: 0.714\n",
      "loss at iter 781:0.9582\n",
      "train accuracy: 0.736\n",
      "loss at iter 782:0.9411\n",
      "train accuracy: 0.726\n",
      "loss at iter 783:0.9601\n",
      "train accuracy: 0.718\n",
      "loss at iter 784:0.9840\n",
      "train accuracy: 0.702\n",
      "loss at iter 785:0.9607\n",
      "train accuracy: 0.711\n",
      "loss at iter 786:0.9659\n",
      "train accuracy: 0.73\n",
      "loss at iter 787:0.9567\n",
      "train accuracy: 0.73\n",
      "loss at iter 788:0.9444\n",
      "train accuracy: 0.74\n",
      "loss at iter 789:0.9690\n",
      "train accuracy: 0.706\n",
      "loss at iter 790:0.9920\n",
      "train accuracy: 0.719\n",
      "loss at iter 791:0.9394\n",
      "train accuracy: 0.71\n",
      "loss at iter 792:0.9615\n",
      "train accuracy: 0.732\n",
      "loss at iter 793:0.9541\n",
      "train accuracy: 0.73\n",
      "loss at iter 794:0.9421\n",
      "train accuracy: 0.736\n",
      "loss at iter 795:0.9352\n",
      "train accuracy: 0.727\n",
      "loss at iter 796:1.0002\n",
      "train accuracy: 0.711\n",
      "loss at iter 797:0.9616\n",
      "train accuracy: 0.736\n",
      "loss at iter 798:0.9418\n",
      "train accuracy: 0.747\n",
      "loss at iter 799:0.9680\n",
      "train accuracy: 0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 800:0.9390\n",
      "train accuracy: 0.746\n",
      "loss at iter 801:0.9606\n",
      "train accuracy: 0.723\n",
      "loss at iter 802:0.9550\n",
      "train accuracy: 0.726\n",
      "loss at iter 803:0.9331\n",
      "train accuracy: 0.739\n",
      "loss at iter 804:0.9605\n",
      "train accuracy: 0.724\n",
      "loss at iter 805:0.9110\n",
      "train accuracy: 0.758\n",
      "loss at iter 806:0.9154\n",
      "train accuracy: 0.756\n",
      "loss at iter 807:0.9279\n",
      "train accuracy: 0.732\n",
      "loss at iter 808:0.9361\n",
      "train accuracy: 0.723\n",
      "loss at iter 809:0.9750\n",
      "train accuracy: 0.733\n",
      "loss at iter 810:0.9401\n",
      "train accuracy: 0.728\n",
      "loss at iter 811:0.9294\n",
      "train accuracy: 0.729\n",
      "loss at iter 812:0.9130\n",
      "train accuracy: 0.741\n",
      "loss at iter 813:0.9150\n",
      "train accuracy: 0.731\n",
      "loss at iter 814:0.9540\n",
      "train accuracy: 0.715\n",
      "loss at iter 815:0.9572\n",
      "train accuracy: 0.732\n",
      "loss at iter 816:0.9406\n",
      "train accuracy: 0.733\n",
      "loss at iter 817:0.9406\n",
      "train accuracy: 0.732\n",
      "loss at iter 818:0.9520\n",
      "train accuracy: 0.73\n",
      "loss at iter 819:0.8940\n",
      "train accuracy: 0.756\n",
      "loss at iter 820:0.9391\n",
      "train accuracy: 0.754\n",
      "loss at iter 821:0.9515\n",
      "train accuracy: 0.716\n",
      "loss at iter 822:0.9044\n",
      "train accuracy: 0.753\n",
      "loss at iter 823:0.8873\n",
      "train accuracy: 0.768\n",
      "loss at iter 824:0.8924\n",
      "train accuracy: 0.758\n",
      "loss at iter 825:0.9038\n",
      "train accuracy: 0.759\n",
      "loss at iter 826:0.9378\n",
      "train accuracy: 0.745\n",
      "loss at iter 827:0.9284\n",
      "train accuracy: 0.734\n",
      "loss at iter 828:0.8736\n",
      "train accuracy: 0.761\n",
      "loss at iter 829:0.8655\n",
      "train accuracy: 0.757\n",
      "loss at iter 830:0.9225\n",
      "train accuracy: 0.742\n",
      "loss at iter 831:0.9081\n",
      "train accuracy: 0.739\n",
      "loss at iter 832:0.9135\n",
      "train accuracy: 0.731\n",
      "loss at iter 833:0.8573\n",
      "train accuracy: 0.761\n",
      "loss at iter 834:0.8773\n",
      "train accuracy: 0.763\n",
      "loss at iter 835:0.8719\n",
      "train accuracy: 0.759\n",
      "loss at iter 836:0.8649\n",
      "train accuracy: 0.774\n",
      "loss at iter 837:0.8843\n",
      "train accuracy: 0.774\n",
      "loss at iter 838:0.9264\n",
      "train accuracy: 0.719\n",
      "loss at iter 839:0.9157\n",
      "train accuracy: 0.751\n",
      "loss at iter 840:0.8944\n",
      "train accuracy: 0.75\n",
      "loss at iter 841:0.9018\n",
      "train accuracy: 0.74\n",
      "loss at iter 842:0.9113\n",
      "train accuracy: 0.737\n",
      "loss at iter 843:0.9156\n",
      "train accuracy: 0.728\n",
      "loss at iter 844:0.8899\n",
      "train accuracy: 0.742\n",
      "loss at iter 845:0.8773\n",
      "train accuracy: 0.77\n",
      "loss at iter 846:0.8865\n",
      "train accuracy: 0.748\n",
      "loss at iter 847:0.9263\n",
      "train accuracy: 0.728\n",
      "loss at iter 848:0.8924\n",
      "train accuracy: 0.774\n",
      "loss at iter 849:0.8960\n",
      "train accuracy: 0.727\n",
      "loss at iter 850:0.8694\n",
      "train accuracy: 0.759\n",
      "loss at iter 851:0.8815\n",
      "train accuracy: 0.754\n",
      "loss at iter 852:0.8633\n",
      "train accuracy: 0.767\n",
      "loss at iter 853:0.8766\n",
      "train accuracy: 0.74\n",
      "loss at iter 854:0.8844\n",
      "train accuracy: 0.769\n",
      "loss at iter 855:0.8762\n",
      "train accuracy: 0.754\n",
      "loss at iter 856:0.8600\n",
      "train accuracy: 0.775\n",
      "loss at iter 857:0.8749\n",
      "train accuracy: 0.736\n",
      "loss at iter 858:0.9023\n",
      "train accuracy: 0.737\n",
      "loss at iter 859:0.8217\n",
      "train accuracy: 0.8\n",
      "loss at iter 860:0.8618\n",
      "train accuracy: 0.765\n",
      "loss at iter 861:0.8952\n",
      "train accuracy: 0.769\n",
      "loss at iter 862:0.8636\n",
      "train accuracy: 0.77\n",
      "loss at iter 863:0.8463\n",
      "train accuracy: 0.752\n",
      "loss at iter 864:0.8296\n",
      "train accuracy: 0.763\n",
      "loss at iter 865:0.8576\n",
      "train accuracy: 0.756\n",
      "loss at iter 866:0.8485\n",
      "train accuracy: 0.771\n",
      "loss at iter 867:0.8583\n",
      "train accuracy: 0.764\n",
      "loss at iter 868:0.8529\n",
      "train accuracy: 0.753\n",
      "loss at iter 869:0.8547\n",
      "train accuracy: 0.759\n",
      "loss at iter 870:0.8836\n",
      "train accuracy: 0.749\n",
      "loss at iter 871:0.8416\n",
      "train accuracy: 0.776\n",
      "loss at iter 872:0.8611\n",
      "train accuracy: 0.769\n",
      "loss at iter 873:0.8574\n",
      "train accuracy: 0.766\n",
      "loss at iter 874:0.8501\n",
      "train accuracy: 0.772\n",
      "loss at iter 875:0.8485\n",
      "train accuracy: 0.779\n",
      "loss at iter 876:0.8563\n",
      "train accuracy: 0.764\n",
      "loss at iter 877:0.8542\n",
      "train accuracy: 0.777\n",
      "loss at iter 878:0.8525\n",
      "train accuracy: 0.755\n",
      "loss at iter 879:0.8708\n",
      "train accuracy: 0.755\n",
      "loss at iter 880:0.8922\n",
      "train accuracy: 0.751\n",
      "loss at iter 881:0.8650\n",
      "train accuracy: 0.77\n",
      "loss at iter 882:0.8583\n",
      "train accuracy: 0.767\n",
      "loss at iter 883:0.8594\n",
      "train accuracy: 0.765\n",
      "loss at iter 884:0.8733\n",
      "train accuracy: 0.757\n",
      "loss at iter 885:0.8217\n",
      "train accuracy: 0.792\n",
      "loss at iter 886:0.8316\n",
      "train accuracy: 0.786\n",
      "loss at iter 887:0.8091\n",
      "train accuracy: 0.773\n",
      "loss at iter 888:0.8676\n",
      "train accuracy: 0.747\n",
      "loss at iter 889:0.8350\n",
      "train accuracy: 0.777\n",
      "loss at iter 890:0.8343\n",
      "train accuracy: 0.785\n",
      "loss at iter 891:0.8267\n",
      "train accuracy: 0.795\n",
      "loss at iter 892:0.8479\n",
      "train accuracy: 0.778\n",
      "loss at iter 893:0.8903\n",
      "train accuracy: 0.755\n",
      "loss at iter 894:0.8209\n",
      "train accuracy: 0.771\n",
      "loss at iter 895:0.8597\n",
      "train accuracy: 0.765\n",
      "loss at iter 896:0.7989\n",
      "train accuracy: 0.795\n",
      "loss at iter 897:0.8276\n",
      "train accuracy: 0.792\n",
      "loss at iter 898:0.8486\n",
      "train accuracy: 0.749\n",
      "loss at iter 899:0.8488\n",
      "train accuracy: 0.752\n",
      "loss at iter 900:0.8380\n",
      "train accuracy: 0.769\n",
      "loss at iter 901:0.8461\n",
      "train accuracy: 0.776\n",
      "loss at iter 902:0.8087\n",
      "train accuracy: 0.796\n",
      "loss at iter 903:0.8517\n",
      "train accuracy: 0.771\n",
      "loss at iter 904:0.8507\n",
      "train accuracy: 0.764\n",
      "loss at iter 905:0.8259\n",
      "train accuracy: 0.77\n",
      "loss at iter 906:0.8058\n",
      "train accuracy: 0.763\n",
      "loss at iter 907:0.8292\n",
      "train accuracy: 0.781\n",
      "loss at iter 908:0.8150\n",
      "train accuracy: 0.778\n",
      "loss at iter 909:0.8152\n",
      "train accuracy: 0.777\n",
      "loss at iter 910:0.8046\n",
      "train accuracy: 0.782\n",
      "loss at iter 911:0.7961\n",
      "train accuracy: 0.785\n",
      "loss at iter 912:0.8419\n",
      "train accuracy: 0.761\n",
      "loss at iter 913:0.8376\n",
      "train accuracy: 0.764\n",
      "loss at iter 914:0.8223\n",
      "train accuracy: 0.777\n",
      "loss at iter 915:0.8058\n",
      "train accuracy: 0.772\n",
      "loss at iter 916:0.8037\n",
      "train accuracy: 0.769\n",
      "loss at iter 917:0.8348\n",
      "train accuracy: 0.774\n",
      "loss at iter 918:0.7953\n",
      "train accuracy: 0.798\n",
      "loss at iter 919:0.8290\n",
      "train accuracy: 0.781\n",
      "loss at iter 920:0.8021\n",
      "train accuracy: 0.789\n",
      "loss at iter 921:0.8270\n",
      "train accuracy: 0.762\n",
      "loss at iter 922:0.8203\n",
      "train accuracy: 0.773\n",
      "loss at iter 923:0.7952\n",
      "train accuracy: 0.775\n",
      "loss at iter 924:0.8354\n",
      "train accuracy: 0.769\n",
      "loss at iter 925:0.8532\n",
      "train accuracy: 0.768\n",
      "loss at iter 926:0.7912\n",
      "train accuracy: 0.796\n",
      "loss at iter 927:0.7814\n",
      "train accuracy: 0.789\n",
      "loss at iter 928:0.8111\n",
      "train accuracy: 0.784\n",
      "loss at iter 929:0.7873\n",
      "train accuracy: 0.788\n",
      "loss at iter 930:0.7932\n",
      "train accuracy: 0.789\n",
      "loss at iter 931:0.8303\n",
      "train accuracy: 0.769\n",
      "loss at iter 932:0.7681\n",
      "train accuracy: 0.806\n",
      "loss at iter 933:0.7830\n",
      "train accuracy: 0.798\n",
      "loss at iter 934:0.7865\n",
      "train accuracy: 0.789\n",
      "loss at iter 935:0.7460\n",
      "train accuracy: 0.809\n",
      "loss at iter 936:0.7832\n",
      "train accuracy: 0.778\n",
      "loss at iter 937:0.7784\n",
      "train accuracy: 0.793\n",
      "loss at iter 938:0.7726\n",
      "train accuracy: 0.812\n",
      "loss at iter 939:0.7747\n",
      "train accuracy: 0.813\n",
      "loss at iter 940:0.8325\n",
      "train accuracy: 0.758\n",
      "loss at iter 941:0.8059\n",
      "train accuracy: 0.791\n",
      "loss at iter 942:0.7632\n",
      "train accuracy: 0.807\n",
      "loss at iter 943:0.7607\n",
      "train accuracy: 0.816\n",
      "loss at iter 944:0.7509\n",
      "train accuracy: 0.811\n",
      "loss at iter 945:0.7587\n",
      "train accuracy: 0.789\n",
      "loss at iter 946:0.7607\n",
      "train accuracy: 0.807\n",
      "loss at iter 947:0.7570\n",
      "train accuracy: 0.804\n",
      "loss at iter 948:0.7828\n",
      "train accuracy: 0.789\n",
      "loss at iter 949:0.7536\n",
      "train accuracy: 0.784\n",
      "loss at iter 950:0.7950\n",
      "train accuracy: 0.788\n",
      "loss at iter 951:0.7645\n",
      "train accuracy: 0.808\n",
      "loss at iter 952:0.7683\n",
      "train accuracy: 0.794\n",
      "loss at iter 953:0.7841\n",
      "train accuracy: 0.782\n",
      "loss at iter 954:0.7611\n",
      "train accuracy: 0.792\n",
      "loss at iter 955:0.7514\n",
      "train accuracy: 0.802\n",
      "loss at iter 956:0.7533\n",
      "train accuracy: 0.823\n",
      "loss at iter 957:0.7484\n",
      "train accuracy: 0.802\n",
      "loss at iter 958:0.7756\n",
      "train accuracy: 0.797\n",
      "loss at iter 959:0.7549\n",
      "train accuracy: 0.816\n",
      "loss at iter 960:0.7563\n",
      "train accuracy: 0.8\n",
      "loss at iter 961:0.7472\n",
      "train accuracy: 0.808\n",
      "loss at iter 962:0.7865\n",
      "train accuracy: 0.783\n",
      "loss at iter 963:0.7850\n",
      "train accuracy: 0.78\n",
      "loss at iter 964:0.7456\n",
      "train accuracy: 0.813\n",
      "loss at iter 965:0.7955\n",
      "train accuracy: 0.788\n",
      "loss at iter 966:0.7587\n",
      "train accuracy: 0.785\n",
      "loss at iter 967:0.7456\n",
      "train accuracy: 0.812\n",
      "loss at iter 968:0.7592\n",
      "train accuracy: 0.779\n",
      "loss at iter 969:0.7601\n",
      "train accuracy: 0.789\n",
      "loss at iter 970:0.7513\n",
      "train accuracy: 0.8\n",
      "loss at iter 971:0.7776\n",
      "train accuracy: 0.8\n",
      "loss at iter 972:0.7568\n",
      "train accuracy: 0.794\n",
      "loss at iter 973:0.7661\n",
      "train accuracy: 0.797\n",
      "loss at iter 974:0.7493\n",
      "train accuracy: 0.804\n",
      "loss at iter 975:0.7445\n",
      "train accuracy: 0.803\n",
      "loss at iter 976:0.7361\n",
      "train accuracy: 0.804\n",
      "loss at iter 977:0.7664\n",
      "train accuracy: 0.802\n",
      "loss at iter 978:0.7744\n",
      "train accuracy: 0.794\n",
      "loss at iter 979:0.7678\n",
      "train accuracy: 0.797\n",
      "loss at iter 980:0.7470\n",
      "train accuracy: 0.803\n",
      "loss at iter 981:0.7542\n",
      "train accuracy: 0.806\n",
      "loss at iter 982:0.7476\n",
      "train accuracy: 0.805\n",
      "loss at iter 983:0.7381\n",
      "train accuracy: 0.8\n",
      "loss at iter 984:0.7061\n",
      "train accuracy: 0.82\n",
      "loss at iter 985:0.7409\n",
      "train accuracy: 0.807\n",
      "loss at iter 986:0.7505\n",
      "train accuracy: 0.807\n",
      "loss at iter 987:0.7415\n",
      "train accuracy: 0.801\n",
      "loss at iter 988:0.7566\n",
      "train accuracy: 0.793\n",
      "loss at iter 989:0.7545\n",
      "train accuracy: 0.806\n",
      "loss at iter 990:0.7394\n",
      "train accuracy: 0.816\n",
      "loss at iter 991:0.7198\n",
      "train accuracy: 0.803\n",
      "loss at iter 992:0.7725\n",
      "train accuracy: 0.796\n",
      "loss at iter 993:0.7475\n",
      "train accuracy: 0.807\n",
      "loss at iter 994:0.7509\n",
      "train accuracy: 0.785\n",
      "loss at iter 995:0.7347\n",
      "train accuracy: 0.81\n",
      "loss at iter 996:0.7472\n",
      "train accuracy: 0.785\n",
      "loss at iter 997:0.7300\n",
      "train accuracy: 0.814\n",
      "loss at iter 998:0.7587\n",
      "train accuracy: 0.778\n",
      "loss at iter 999:0.7175\n",
      "train accuracy: 0.806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1000:0.7309\n",
      "train accuracy: 0.811\n",
      "loss at iter 1001:0.7235\n",
      "train accuracy: 0.802\n",
      "loss at iter 1002:0.7288\n",
      "train accuracy: 0.814\n",
      "loss at iter 1003:0.7199\n",
      "train accuracy: 0.792\n",
      "loss at iter 1004:0.7256\n",
      "train accuracy: 0.815\n",
      "loss at iter 1005:0.7340\n",
      "train accuracy: 0.799\n",
      "loss at iter 1006:0.7307\n",
      "train accuracy: 0.801\n",
      "loss at iter 1007:0.7145\n",
      "train accuracy: 0.803\n",
      "loss at iter 1008:0.6997\n",
      "train accuracy: 0.826\n",
      "loss at iter 1009:0.7308\n",
      "train accuracy: 0.812\n",
      "loss at iter 1010:0.7455\n",
      "train accuracy: 0.783\n",
      "loss at iter 1011:0.7810\n",
      "train accuracy: 0.785\n",
      "loss at iter 1012:0.7070\n",
      "train accuracy: 0.818\n",
      "loss at iter 1013:0.7153\n",
      "train accuracy: 0.812\n",
      "loss at iter 1014:0.7186\n",
      "train accuracy: 0.801\n",
      "loss at iter 1015:0.7704\n",
      "train accuracy: 0.777\n",
      "loss at iter 1016:0.7093\n",
      "train accuracy: 0.822\n",
      "loss at iter 1017:0.7047\n",
      "train accuracy: 0.821\n",
      "loss at iter 1018:0.7404\n",
      "train accuracy: 0.814\n",
      "loss at iter 1019:0.6878\n",
      "train accuracy: 0.83\n",
      "loss at iter 1020:0.7365\n",
      "train accuracy: 0.815\n",
      "loss at iter 1021:0.7226\n",
      "train accuracy: 0.799\n",
      "loss at iter 1022:0.6812\n",
      "train accuracy: 0.83\n",
      "loss at iter 1023:0.7073\n",
      "train accuracy: 0.817\n",
      "loss at iter 1024:0.7088\n",
      "train accuracy: 0.826\n",
      "loss at iter 1025:0.7150\n",
      "train accuracy: 0.807\n",
      "loss at iter 1026:0.6954\n",
      "train accuracy: 0.82\n",
      "loss at iter 1027:0.7003\n",
      "train accuracy: 0.82\n",
      "loss at iter 1028:0.7118\n",
      "train accuracy: 0.829\n",
      "loss at iter 1029:0.7020\n",
      "train accuracy: 0.815\n",
      "loss at iter 1030:0.6960\n",
      "train accuracy: 0.833\n",
      "loss at iter 1031:0.7599\n",
      "train accuracy: 0.791\n",
      "loss at iter 1032:0.7064\n",
      "train accuracy: 0.827\n",
      "loss at iter 1033:0.7296\n",
      "train accuracy: 0.807\n",
      "loss at iter 1034:0.7062\n",
      "train accuracy: 0.813\n",
      "loss at iter 1035:0.6644\n",
      "train accuracy: 0.82\n",
      "loss at iter 1036:0.7426\n",
      "train accuracy: 0.796\n",
      "loss at iter 1037:0.6669\n",
      "train accuracy: 0.84\n",
      "loss at iter 1038:0.6701\n",
      "train accuracy: 0.826\n",
      "loss at iter 1039:0.6911\n",
      "train accuracy: 0.81\n",
      "loss at iter 1040:0.6673\n",
      "train accuracy: 0.83\n",
      "loss at iter 1041:0.6898\n",
      "train accuracy: 0.815\n",
      "loss at iter 1042:0.6744\n",
      "train accuracy: 0.816\n",
      "loss at iter 1043:0.6999\n",
      "train accuracy: 0.808\n",
      "loss at iter 1044:0.6936\n",
      "train accuracy: 0.813\n",
      "loss at iter 1045:0.6995\n",
      "train accuracy: 0.811\n",
      "loss at iter 1046:0.7691\n",
      "train accuracy: 0.78\n",
      "loss at iter 1047:0.7108\n",
      "train accuracy: 0.817\n",
      "loss at iter 1048:0.7280\n",
      "train accuracy: 0.808\n",
      "loss at iter 1049:0.6996\n",
      "train accuracy: 0.817\n",
      "loss at iter 1050:0.6797\n",
      "train accuracy: 0.835\n",
      "loss at iter 1051:0.6858\n",
      "train accuracy: 0.833\n",
      "loss at iter 1052:0.6745\n",
      "train accuracy: 0.834\n",
      "loss at iter 1053:0.6603\n",
      "train accuracy: 0.827\n",
      "loss at iter 1054:0.6676\n",
      "train accuracy: 0.826\n",
      "loss at iter 1055:0.6807\n",
      "train accuracy: 0.812\n",
      "loss at iter 1056:0.6936\n",
      "train accuracy: 0.819\n",
      "loss at iter 1057:0.7105\n",
      "train accuracy: 0.811\n",
      "loss at iter 1058:0.6716\n",
      "train accuracy: 0.805\n",
      "loss at iter 1059:0.6848\n",
      "train accuracy: 0.832\n",
      "loss at iter 1060:0.7079\n",
      "train accuracy: 0.803\n",
      "loss at iter 1061:0.6976\n",
      "train accuracy: 0.815\n",
      "loss at iter 1062:0.6488\n",
      "train accuracy: 0.843\n",
      "loss at iter 1063:0.6940\n",
      "train accuracy: 0.815\n",
      "loss at iter 1064:0.7071\n",
      "train accuracy: 0.806\n",
      "loss at iter 1065:0.6991\n",
      "train accuracy: 0.821\n",
      "loss at iter 1066:0.6870\n",
      "train accuracy: 0.816\n",
      "loss at iter 1067:0.6830\n",
      "train accuracy: 0.829\n",
      "loss at iter 1068:0.7063\n",
      "train accuracy: 0.815\n",
      "loss at iter 1069:0.6923\n",
      "train accuracy: 0.816\n",
      "loss at iter 1070:0.6768\n",
      "train accuracy: 0.829\n",
      "loss at iter 1071:0.6682\n",
      "train accuracy: 0.833\n",
      "loss at iter 1072:0.6830\n",
      "train accuracy: 0.812\n",
      "loss at iter 1073:0.6785\n",
      "train accuracy: 0.836\n",
      "loss at iter 1074:0.6927\n",
      "train accuracy: 0.829\n",
      "loss at iter 1075:0.6628\n",
      "train accuracy: 0.822\n",
      "loss at iter 1076:0.6458\n",
      "train accuracy: 0.831\n",
      "loss at iter 1077:0.6416\n",
      "train accuracy: 0.844\n",
      "loss at iter 1078:0.6607\n",
      "train accuracy: 0.849\n",
      "loss at iter 1079:0.6658\n",
      "train accuracy: 0.82\n",
      "loss at iter 1080:0.6334\n",
      "train accuracy: 0.837\n",
      "loss at iter 1081:0.6861\n",
      "train accuracy: 0.805\n",
      "loss at iter 1082:0.6775\n",
      "train accuracy: 0.825\n",
      "loss at iter 1083:0.6790\n",
      "train accuracy: 0.822\n",
      "loss at iter 1084:0.6614\n",
      "train accuracy: 0.816\n",
      "loss at iter 1085:0.6650\n",
      "train accuracy: 0.821\n",
      "loss at iter 1086:0.6679\n",
      "train accuracy: 0.827\n",
      "loss at iter 1087:0.6434\n",
      "train accuracy: 0.827\n",
      "loss at iter 1088:0.6736\n",
      "train accuracy: 0.821\n",
      "loss at iter 1089:0.6729\n",
      "train accuracy: 0.821\n",
      "loss at iter 1090:0.6349\n",
      "train accuracy: 0.84\n",
      "loss at iter 1091:0.6493\n",
      "train accuracy: 0.841\n",
      "loss at iter 1092:0.6750\n",
      "train accuracy: 0.812\n",
      "loss at iter 1093:0.6630\n",
      "train accuracy: 0.825\n",
      "loss at iter 1094:0.6757\n",
      "train accuracy: 0.831\n",
      "loss at iter 1095:0.6842\n",
      "train accuracy: 0.817\n",
      "loss at iter 1096:0.6858\n",
      "train accuracy: 0.813\n",
      "loss at iter 1097:0.6702\n",
      "train accuracy: 0.807\n",
      "loss at iter 1098:0.7089\n",
      "train accuracy: 0.797\n",
      "loss at iter 1099:0.7236\n",
      "train accuracy: 0.808\n",
      "loss at iter 1100:0.6609\n",
      "train accuracy: 0.833\n",
      "loss at iter 1101:0.6943\n",
      "train accuracy: 0.801\n",
      "loss at iter 1102:0.6500\n",
      "train accuracy: 0.818\n",
      "loss at iter 1103:0.6701\n",
      "train accuracy: 0.804\n",
      "loss at iter 1104:0.6481\n",
      "train accuracy: 0.825\n",
      "loss at iter 1105:0.6355\n",
      "train accuracy: 0.836\n",
      "loss at iter 1106:0.6442\n",
      "train accuracy: 0.834\n",
      "loss at iter 1107:0.6586\n",
      "train accuracy: 0.828\n",
      "loss at iter 1108:0.6338\n",
      "train accuracy: 0.838\n",
      "loss at iter 1109:0.6552\n",
      "train accuracy: 0.827\n",
      "loss at iter 1110:0.6413\n",
      "train accuracy: 0.832\n",
      "loss at iter 1111:0.6658\n",
      "train accuracy: 0.823\n",
      "loss at iter 1112:0.6681\n",
      "train accuracy: 0.827\n",
      "loss at iter 1113:0.6192\n",
      "train accuracy: 0.842\n",
      "loss at iter 1114:0.6430\n",
      "train accuracy: 0.834\n",
      "loss at iter 1115:0.6412\n",
      "train accuracy: 0.829\n",
      "loss at iter 1116:0.6575\n",
      "train accuracy: 0.811\n",
      "loss at iter 1117:0.6501\n",
      "train accuracy: 0.84\n",
      "loss at iter 1118:0.6776\n",
      "train accuracy: 0.823\n",
      "loss at iter 1119:0.6586\n",
      "train accuracy: 0.839\n",
      "loss at iter 1120:0.6652\n",
      "train accuracy: 0.823\n",
      "loss at iter 1121:0.6686\n",
      "train accuracy: 0.829\n",
      "loss at iter 1122:0.6298\n",
      "train accuracy: 0.843\n",
      "loss at iter 1123:0.6372\n",
      "train accuracy: 0.817\n",
      "loss at iter 1124:0.6314\n",
      "train accuracy: 0.825\n",
      "loss at iter 1125:0.6452\n",
      "train accuracy: 0.833\n",
      "loss at iter 1126:0.6543\n",
      "train accuracy: 0.823\n",
      "loss at iter 1127:0.6209\n",
      "train accuracy: 0.853\n",
      "loss at iter 1128:0.6860\n",
      "train accuracy: 0.819\n",
      "loss at iter 1129:0.6327\n",
      "train accuracy: 0.834\n",
      "loss at iter 1130:0.6701\n",
      "train accuracy: 0.821\n",
      "loss at iter 1131:0.6332\n",
      "train accuracy: 0.844\n",
      "loss at iter 1132:0.6456\n",
      "train accuracy: 0.833\n",
      "loss at iter 1133:0.6389\n",
      "train accuracy: 0.841\n",
      "loss at iter 1134:0.6411\n",
      "train accuracy: 0.838\n",
      "loss at iter 1135:0.6204\n",
      "train accuracy: 0.842\n",
      "loss at iter 1136:0.6219\n",
      "train accuracy: 0.838\n",
      "loss at iter 1137:0.6555\n",
      "train accuracy: 0.828\n",
      "loss at iter 1138:0.6356\n",
      "train accuracy: 0.824\n",
      "loss at iter 1139:0.6038\n",
      "train accuracy: 0.842\n",
      "loss at iter 1140:0.6244\n",
      "train accuracy: 0.845\n",
      "loss at iter 1141:0.6229\n",
      "train accuracy: 0.828\n",
      "loss at iter 1142:0.6004\n",
      "train accuracy: 0.863\n",
      "loss at iter 1143:0.6574\n",
      "train accuracy: 0.821\n",
      "loss at iter 1144:0.6283\n",
      "train accuracy: 0.846\n",
      "loss at iter 1145:0.6379\n",
      "train accuracy: 0.839\n",
      "loss at iter 1146:0.6555\n",
      "train accuracy: 0.83\n",
      "loss at iter 1147:0.6250\n",
      "train accuracy: 0.832\n",
      "loss at iter 1148:0.6562\n",
      "train accuracy: 0.823\n",
      "loss at iter 1149:0.6143\n",
      "train accuracy: 0.839\n",
      "loss at iter 1150:0.6515\n",
      "train accuracy: 0.831\n",
      "loss at iter 1151:0.6448\n",
      "train accuracy: 0.83\n",
      "loss at iter 1152:0.6026\n",
      "train accuracy: 0.848\n",
      "loss at iter 1153:0.6206\n",
      "train accuracy: 0.831\n",
      "loss at iter 1154:0.6282\n",
      "train accuracy: 0.816\n",
      "loss at iter 1155:0.6446\n",
      "train accuracy: 0.82\n",
      "loss at iter 1156:0.6043\n",
      "train accuracy: 0.852\n",
      "loss at iter 1157:0.6318\n",
      "train accuracy: 0.836\n",
      "loss at iter 1158:0.6458\n",
      "train accuracy: 0.826\n",
      "loss at iter 1159:0.6227\n",
      "train accuracy: 0.85\n",
      "loss at iter 1160:0.6200\n",
      "train accuracy: 0.836\n",
      "loss at iter 1161:0.6311\n",
      "train accuracy: 0.834\n",
      "loss at iter 1162:0.6477\n",
      "train accuracy: 0.82\n",
      "loss at iter 1163:0.6565\n",
      "train accuracy: 0.819\n",
      "loss at iter 1164:0.5764\n",
      "train accuracy: 0.858\n",
      "loss at iter 1165:0.6278\n",
      "train accuracy: 0.833\n",
      "loss at iter 1166:0.6437\n",
      "train accuracy: 0.829\n",
      "loss at iter 1167:0.6393\n",
      "train accuracy: 0.839\n",
      "loss at iter 1168:0.6327\n",
      "train accuracy: 0.837\n",
      "loss at iter 1169:0.5962\n",
      "train accuracy: 0.831\n",
      "loss at iter 1170:0.6354\n",
      "train accuracy: 0.836\n",
      "loss at iter 1171:0.5822\n",
      "train accuracy: 0.862\n",
      "loss at iter 1172:0.6483\n",
      "train accuracy: 0.822\n",
      "loss at iter 1173:0.6274\n",
      "train accuracy: 0.834\n",
      "loss at iter 1174:0.6382\n",
      "train accuracy: 0.829\n",
      "loss at iter 1175:0.6166\n",
      "train accuracy: 0.84\n",
      "loss at iter 1176:0.5864\n",
      "train accuracy: 0.841\n",
      "loss at iter 1177:0.6272\n",
      "train accuracy: 0.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1178:0.5891\n",
      "train accuracy: 0.848\n",
      "loss at iter 1179:0.6298\n",
      "train accuracy: 0.836\n",
      "loss at iter 1180:0.6491\n",
      "train accuracy: 0.824\n",
      "loss at iter 1181:0.5922\n",
      "train accuracy: 0.848\n",
      "loss at iter 1182:0.5741\n",
      "train accuracy: 0.859\n",
      "loss at iter 1183:0.6052\n",
      "train accuracy: 0.841\n",
      "loss at iter 1184:0.6043\n",
      "train accuracy: 0.834\n",
      "loss at iter 1185:0.5988\n",
      "train accuracy: 0.864\n",
      "loss at iter 1186:0.6012\n",
      "train accuracy: 0.833\n",
      "loss at iter 1187:0.5911\n",
      "train accuracy: 0.841\n",
      "loss at iter 1188:0.5969\n",
      "train accuracy: 0.849\n",
      "loss at iter 1189:0.5753\n",
      "train accuracy: 0.847\n",
      "loss at iter 1190:0.5857\n",
      "train accuracy: 0.853\n",
      "loss at iter 1191:0.6131\n",
      "train accuracy: 0.845\n",
      "loss at iter 1192:0.6198\n",
      "train accuracy: 0.832\n",
      "loss at iter 1193:0.5900\n",
      "train accuracy: 0.848\n",
      "loss at iter 1194:0.5976\n",
      "train accuracy: 0.843\n",
      "loss at iter 1195:0.5971\n",
      "train accuracy: 0.834\n",
      "loss at iter 1196:0.6045\n",
      "train accuracy: 0.839\n",
      "loss at iter 1197:0.6390\n",
      "train accuracy: 0.829\n",
      "loss at iter 1198:0.6226\n",
      "train accuracy: 0.835\n",
      "loss at iter 1199:0.6091\n",
      "train accuracy: 0.849\n",
      "loss at iter 1200:0.5915\n",
      "train accuracy: 0.842\n",
      "loss at iter 1201:0.6218\n",
      "train accuracy: 0.836\n",
      "loss at iter 1202:0.6569\n",
      "train accuracy: 0.837\n",
      "loss at iter 1203:0.5773\n",
      "train accuracy: 0.864\n",
      "loss at iter 1204:0.5998\n",
      "train accuracy: 0.844\n",
      "loss at iter 1205:0.5677\n",
      "train accuracy: 0.844\n",
      "loss at iter 1206:0.5865\n",
      "train accuracy: 0.859\n",
      "loss at iter 1207:0.5853\n",
      "train accuracy: 0.839\n",
      "loss at iter 1208:0.5676\n",
      "train accuracy: 0.854\n",
      "loss at iter 1209:0.6226\n",
      "train accuracy: 0.84\n",
      "loss at iter 1210:0.6321\n",
      "train accuracy: 0.82\n",
      "loss at iter 1211:0.6204\n",
      "train accuracy: 0.837\n",
      "loss at iter 1212:0.5856\n",
      "train accuracy: 0.848\n",
      "loss at iter 1213:0.5628\n",
      "train accuracy: 0.85\n",
      "loss at iter 1214:0.6029\n",
      "train accuracy: 0.844\n",
      "loss at iter 1215:0.5718\n",
      "train accuracy: 0.861\n",
      "loss at iter 1216:0.5983\n",
      "train accuracy: 0.849\n",
      "loss at iter 1217:0.6247\n",
      "train accuracy: 0.839\n",
      "loss at iter 1218:0.5519\n",
      "train accuracy: 0.856\n",
      "loss at iter 1219:0.6395\n",
      "train accuracy: 0.827\n",
      "loss at iter 1220:0.6100\n",
      "train accuracy: 0.835\n",
      "loss at iter 1221:0.5905\n",
      "train accuracy: 0.839\n",
      "loss at iter 1222:0.6015\n",
      "train accuracy: 0.835\n",
      "loss at iter 1223:0.6302\n",
      "train accuracy: 0.816\n",
      "loss at iter 1224:0.5575\n",
      "train accuracy: 0.858\n",
      "loss at iter 1225:0.5557\n",
      "train accuracy: 0.851\n",
      "loss at iter 1226:0.6301\n",
      "train accuracy: 0.826\n",
      "loss at iter 1227:0.5734\n",
      "train accuracy: 0.844\n",
      "loss at iter 1228:0.5983\n",
      "train accuracy: 0.852\n",
      "loss at iter 1229:0.5841\n",
      "train accuracy: 0.848\n",
      "loss at iter 1230:0.6081\n",
      "train accuracy: 0.828\n",
      "loss at iter 1231:0.5905\n",
      "train accuracy: 0.848\n",
      "loss at iter 1232:0.6029\n",
      "train accuracy: 0.838\n",
      "loss at iter 1233:0.5788\n",
      "train accuracy: 0.862\n",
      "loss at iter 1234:0.5637\n",
      "train accuracy: 0.863\n",
      "loss at iter 1235:0.5313\n",
      "train accuracy: 0.86\n",
      "loss at iter 1236:0.6026\n",
      "train accuracy: 0.833\n",
      "loss at iter 1237:0.6086\n",
      "train accuracy: 0.832\n",
      "loss at iter 1238:0.5829\n",
      "train accuracy: 0.851\n",
      "loss at iter 1239:0.5602\n",
      "train accuracy: 0.862\n",
      "loss at iter 1240:0.6057\n",
      "train accuracy: 0.843\n",
      "loss at iter 1241:0.6368\n",
      "train accuracy: 0.828\n",
      "loss at iter 1242:0.6012\n",
      "train accuracy: 0.842\n",
      "loss at iter 1243:0.5508\n",
      "train accuracy: 0.862\n",
      "loss at iter 1244:0.5699\n",
      "train accuracy: 0.845\n",
      "loss at iter 1245:0.5935\n",
      "train accuracy: 0.845\n",
      "loss at iter 1246:0.5768\n",
      "train accuracy: 0.857\n",
      "loss at iter 1247:0.5378\n",
      "train accuracy: 0.867\n",
      "loss at iter 1248:0.5749\n",
      "train accuracy: 0.842\n",
      "loss at iter 1249:0.5814\n",
      "train accuracy: 0.842\n",
      "loss at iter 1250:0.5794\n",
      "train accuracy: 0.856\n",
      "loss at iter 1251:0.6052\n",
      "train accuracy: 0.838\n",
      "loss at iter 1252:0.5830\n",
      "train accuracy: 0.845\n",
      "loss at iter 1253:0.5875\n",
      "train accuracy: 0.853\n",
      "loss at iter 1254:0.5469\n",
      "train accuracy: 0.86\n",
      "loss at iter 1255:0.5695\n",
      "train accuracy: 0.842\n",
      "loss at iter 1256:0.5551\n",
      "train accuracy: 0.855\n",
      "loss at iter 1257:0.5331\n",
      "train accuracy: 0.862\n",
      "loss at iter 1258:0.5917\n",
      "train accuracy: 0.844\n",
      "loss at iter 1259:0.5840\n",
      "train accuracy: 0.849\n",
      "loss at iter 1260:0.5677\n",
      "train accuracy: 0.858\n",
      "loss at iter 1261:0.6055\n",
      "train accuracy: 0.836\n",
      "loss at iter 1262:0.5796\n",
      "train accuracy: 0.841\n",
      "loss at iter 1263:0.5827\n",
      "train accuracy: 0.836\n",
      "loss at iter 1264:0.5650\n",
      "train accuracy: 0.836\n",
      "loss at iter 1265:0.5955\n",
      "train accuracy: 0.847\n",
      "loss at iter 1266:0.5881\n",
      "train accuracy: 0.839\n",
      "loss at iter 1267:0.5879\n",
      "train accuracy: 0.841\n",
      "loss at iter 1268:0.5688\n",
      "train accuracy: 0.848\n",
      "loss at iter 1269:0.5653\n",
      "train accuracy: 0.846\n",
      "loss at iter 1270:0.5505\n",
      "train accuracy: 0.851\n",
      "loss at iter 1271:0.5788\n",
      "train accuracy: 0.849\n",
      "loss at iter 1272:0.5639\n",
      "train accuracy: 0.856\n",
      "loss at iter 1273:0.5722\n",
      "train accuracy: 0.85\n",
      "loss at iter 1274:0.5510\n",
      "train accuracy: 0.868\n",
      "loss at iter 1275:0.5604\n",
      "train accuracy: 0.865\n",
      "loss at iter 1276:0.5435\n",
      "train accuracy: 0.861\n",
      "loss at iter 1277:0.5664\n",
      "train accuracy: 0.85\n",
      "loss at iter 1278:0.5284\n",
      "train accuracy: 0.859\n",
      "loss at iter 1279:0.6097\n",
      "train accuracy: 0.832\n",
      "loss at iter 1280:0.5210\n",
      "train accuracy: 0.865\n",
      "loss at iter 1281:0.5978\n",
      "train accuracy: 0.839\n",
      "loss at iter 1282:0.6100\n",
      "train accuracy: 0.828\n",
      "loss at iter 1283:0.5534\n",
      "train accuracy: 0.866\n",
      "loss at iter 1284:0.5834\n",
      "train accuracy: 0.846\n",
      "loss at iter 1285:0.5747\n",
      "train accuracy: 0.836\n",
      "loss at iter 1286:0.5870\n",
      "train accuracy: 0.846\n",
      "loss at iter 1287:0.5816\n",
      "train accuracy: 0.852\n",
      "loss at iter 1288:0.5511\n",
      "train accuracy: 0.869\n",
      "loss at iter 1289:0.5237\n",
      "train accuracy: 0.872\n",
      "loss at iter 1290:0.5460\n",
      "train accuracy: 0.866\n",
      "loss at iter 1291:0.5450\n",
      "train accuracy: 0.863\n",
      "loss at iter 1292:0.5729\n",
      "train accuracy: 0.849\n",
      "loss at iter 1293:0.5734\n",
      "train accuracy: 0.847\n",
      "loss at iter 1294:0.5468\n",
      "train accuracy: 0.853\n",
      "loss at iter 1295:0.5428\n",
      "train accuracy: 0.853\n",
      "loss at iter 1296:0.5812\n",
      "train accuracy: 0.835\n",
      "loss at iter 1297:0.5573\n",
      "train accuracy: 0.864\n",
      "loss at iter 1298:0.5584\n",
      "train accuracy: 0.854\n",
      "loss at iter 1299:0.5641\n",
      "train accuracy: 0.858\n",
      "loss at iter 1300:0.5522\n",
      "train accuracy: 0.863\n",
      "loss at iter 1301:0.5668\n",
      "train accuracy: 0.853\n",
      "loss at iter 1302:0.6075\n",
      "train accuracy: 0.833\n",
      "loss at iter 1303:0.5410\n",
      "train accuracy: 0.856\n",
      "loss at iter 1304:0.5778\n",
      "train accuracy: 0.846\n",
      "loss at iter 1305:0.5157\n",
      "train accuracy: 0.871\n",
      "loss at iter 1306:0.5231\n",
      "train accuracy: 0.872\n",
      "loss at iter 1307:0.5568\n",
      "train accuracy: 0.84\n",
      "loss at iter 1308:0.5630\n",
      "train accuracy: 0.854\n",
      "loss at iter 1309:0.5529\n",
      "train accuracy: 0.864\n",
      "loss at iter 1310:0.5605\n",
      "train accuracy: 0.849\n",
      "loss at iter 1311:0.5853\n",
      "train accuracy: 0.843\n",
      "loss at iter 1312:0.5632\n",
      "train accuracy: 0.847\n",
      "loss at iter 1313:0.5309\n",
      "train accuracy: 0.866\n",
      "loss at iter 1314:0.5323\n",
      "train accuracy: 0.863\n",
      "loss at iter 1315:0.5491\n",
      "train accuracy: 0.859\n",
      "loss at iter 1316:0.5277\n",
      "train accuracy: 0.863\n",
      "loss at iter 1317:0.5785\n",
      "train accuracy: 0.855\n",
      "loss at iter 1318:0.5130\n",
      "train accuracy: 0.873\n",
      "loss at iter 1319:0.5374\n",
      "train accuracy: 0.872\n",
      "loss at iter 1320:0.5298\n",
      "train accuracy: 0.865\n",
      "loss at iter 1321:0.5403\n",
      "train accuracy: 0.859\n",
      "loss at iter 1322:0.5674\n",
      "train accuracy: 0.845\n",
      "loss at iter 1323:0.5137\n",
      "train accuracy: 0.87\n",
      "loss at iter 1324:0.5343\n",
      "train accuracy: 0.858\n",
      "loss at iter 1325:0.5490\n",
      "train accuracy: 0.851\n",
      "loss at iter 1326:0.4880\n",
      "train accuracy: 0.88\n",
      "loss at iter 1327:0.5476\n",
      "train accuracy: 0.85\n",
      "loss at iter 1328:0.5313\n",
      "train accuracy: 0.863\n",
      "loss at iter 1329:0.5424\n",
      "train accuracy: 0.861\n",
      "loss at iter 1330:0.5477\n",
      "train accuracy: 0.847\n",
      "loss at iter 1331:0.5509\n",
      "train accuracy: 0.854\n",
      "loss at iter 1332:0.5844\n",
      "train accuracy: 0.844\n",
      "loss at iter 1333:0.5844\n",
      "train accuracy: 0.847\n",
      "loss at iter 1334:0.5950\n",
      "train accuracy: 0.841\n",
      "loss at iter 1335:0.5735\n",
      "train accuracy: 0.836\n",
      "loss at iter 1336:0.5426\n",
      "train accuracy: 0.852\n",
      "loss at iter 1337:0.5383\n",
      "train accuracy: 0.854\n",
      "loss at iter 1338:0.5704\n",
      "train accuracy: 0.85\n",
      "loss at iter 1339:0.4691\n",
      "train accuracy: 0.881\n",
      "loss at iter 1340:0.5918\n",
      "train accuracy: 0.825\n",
      "loss at iter 1341:0.5324\n",
      "train accuracy: 0.844\n",
      "loss at iter 1342:0.4928\n",
      "train accuracy: 0.874\n",
      "loss at iter 1343:0.5350\n",
      "train accuracy: 0.871\n",
      "loss at iter 1344:0.5480\n",
      "train accuracy: 0.849\n",
      "loss at iter 1345:0.6030\n",
      "train accuracy: 0.839\n",
      "loss at iter 1346:0.6102\n",
      "train accuracy: 0.836\n",
      "loss at iter 1347:0.5314\n",
      "train accuracy: 0.866\n",
      "loss at iter 1348:0.5103\n",
      "train accuracy: 0.882\n",
      "loss at iter 1349:0.5350\n",
      "train accuracy: 0.848\n",
      "loss at iter 1350:0.5355\n",
      "train accuracy: 0.863\n",
      "loss at iter 1351:0.5301\n",
      "train accuracy: 0.868\n",
      "loss at iter 1352:0.5666\n",
      "train accuracy: 0.842\n",
      "loss at iter 1353:0.5295\n",
      "train accuracy: 0.862\n",
      "loss at iter 1354:0.5137\n",
      "train accuracy: 0.867\n",
      "loss at iter 1355:0.4971\n",
      "train accuracy: 0.884\n",
      "loss at iter 1356:0.5091\n",
      "train accuracy: 0.871\n",
      "loss at iter 1357:0.5365\n",
      "train accuracy: 0.872\n",
      "loss at iter 1358:0.5566\n",
      "train accuracy: 0.837\n",
      "loss at iter 1359:0.5617\n",
      "train accuracy: 0.851\n",
      "loss at iter 1360:0.5504\n",
      "train accuracy: 0.846\n",
      "loss at iter 1361:0.5236\n",
      "train accuracy: 0.873\n",
      "loss at iter 1362:0.5072\n",
      "train accuracy: 0.873\n",
      "loss at iter 1363:0.5176\n",
      "train accuracy: 0.864\n",
      "loss at iter 1364:0.5447\n",
      "train accuracy: 0.861\n",
      "loss at iter 1365:0.5437\n",
      "train accuracy: 0.871\n",
      "loss at iter 1366:0.5277\n",
      "train accuracy: 0.864\n",
      "loss at iter 1367:0.5267\n",
      "train accuracy: 0.851\n",
      "loss at iter 1368:0.5369\n",
      "train accuracy: 0.86\n",
      "loss at iter 1369:0.5420\n",
      "train accuracy: 0.845\n",
      "loss at iter 1370:0.5188\n",
      "train accuracy: 0.861\n",
      "loss at iter 1371:0.5651\n",
      "train accuracy: 0.84\n",
      "loss at iter 1372:0.5459\n",
      "train accuracy: 0.86\n",
      "loss at iter 1373:0.5377\n",
      "train accuracy: 0.868\n",
      "loss at iter 1374:0.5081\n",
      "train accuracy: 0.873\n",
      "loss at iter 1375:0.5145\n",
      "train accuracy: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1376:0.5521\n",
      "train accuracy: 0.849\n",
      "loss at iter 1377:0.5245\n",
      "train accuracy: 0.86\n",
      "loss at iter 1378:0.5279\n",
      "train accuracy: 0.862\n",
      "loss at iter 1379:0.5234\n",
      "train accuracy: 0.871\n",
      "loss at iter 1380:0.4934\n",
      "train accuracy: 0.876\n",
      "loss at iter 1381:0.5078\n",
      "train accuracy: 0.866\n",
      "loss at iter 1382:0.5102\n",
      "train accuracy: 0.86\n",
      "loss at iter 1383:0.5575\n",
      "train accuracy: 0.848\n",
      "loss at iter 1384:0.5482\n",
      "train accuracy: 0.867\n",
      "loss at iter 1385:0.4910\n",
      "train accuracy: 0.875\n",
      "loss at iter 1386:0.5360\n",
      "train accuracy: 0.837\n",
      "loss at iter 1387:0.5292\n",
      "train accuracy: 0.855\n",
      "loss at iter 1388:0.5290\n",
      "train accuracy: 0.872\n",
      "loss at iter 1389:0.5583\n",
      "train accuracy: 0.855\n",
      "loss at iter 1390:0.5209\n",
      "train accuracy: 0.848\n",
      "loss at iter 1391:0.5330\n",
      "train accuracy: 0.857\n",
      "loss at iter 1392:0.5169\n",
      "train accuracy: 0.857\n",
      "loss at iter 1393:0.5660\n",
      "train accuracy: 0.834\n",
      "loss at iter 1394:0.5453\n",
      "train accuracy: 0.853\n",
      "loss at iter 1395:0.5284\n",
      "train accuracy: 0.861\n",
      "loss at iter 1396:0.4968\n",
      "train accuracy: 0.868\n",
      "loss at iter 1397:0.4901\n",
      "train accuracy: 0.875\n",
      "loss at iter 1398:0.5774\n",
      "train accuracy: 0.842\n",
      "loss at iter 1399:0.5056\n",
      "train accuracy: 0.881\n",
      "loss at iter 1400:0.5118\n",
      "train accuracy: 0.874\n",
      "loss at iter 1401:0.5223\n",
      "train accuracy: 0.855\n",
      "loss at iter 1402:0.5029\n",
      "train accuracy: 0.885\n",
      "loss at iter 1403:0.5301\n",
      "train accuracy: 0.858\n",
      "loss at iter 1404:0.5233\n",
      "train accuracy: 0.859\n",
      "loss at iter 1405:0.4825\n",
      "train accuracy: 0.881\n",
      "loss at iter 1406:0.5672\n",
      "train accuracy: 0.84\n",
      "loss at iter 1407:0.5094\n",
      "train accuracy: 0.857\n",
      "loss at iter 1408:0.5172\n",
      "train accuracy: 0.862\n",
      "loss at iter 1409:0.4745\n",
      "train accuracy: 0.883\n",
      "loss at iter 1410:0.5259\n",
      "train accuracy: 0.869\n",
      "loss at iter 1411:0.5078\n",
      "train accuracy: 0.857\n",
      "loss at iter 1412:0.5121\n",
      "train accuracy: 0.864\n",
      "loss at iter 1413:0.5422\n",
      "train accuracy: 0.851\n",
      "loss at iter 1414:0.5030\n",
      "train accuracy: 0.873\n",
      "loss at iter 1415:0.5146\n",
      "train accuracy: 0.874\n",
      "loss at iter 1416:0.5355\n",
      "train accuracy: 0.861\n",
      "loss at iter 1417:0.5332\n",
      "train accuracy: 0.847\n",
      "loss at iter 1418:0.5066\n",
      "train accuracy: 0.863\n",
      "loss at iter 1419:0.5086\n",
      "train accuracy: 0.866\n",
      "loss at iter 1420:0.5275\n",
      "train accuracy: 0.862\n",
      "loss at iter 1421:0.4922\n",
      "train accuracy: 0.868\n",
      "loss at iter 1422:0.4914\n",
      "train accuracy: 0.867\n",
      "loss at iter 1423:0.5550\n",
      "train accuracy: 0.862\n",
      "loss at iter 1424:0.5115\n",
      "train accuracy: 0.873\n",
      "loss at iter 1425:0.5472\n",
      "train accuracy: 0.859\n",
      "loss at iter 1426:0.5236\n",
      "train accuracy: 0.861\n",
      "loss at iter 1427:0.4969\n",
      "train accuracy: 0.876\n",
      "loss at iter 1428:0.4858\n",
      "train accuracy: 0.872\n",
      "loss at iter 1429:0.5288\n",
      "train accuracy: 0.852\n",
      "loss at iter 1430:0.5172\n",
      "train accuracy: 0.861\n",
      "loss at iter 1431:0.4943\n",
      "train accuracy: 0.865\n",
      "loss at iter 1432:0.4997\n",
      "train accuracy: 0.88\n",
      "loss at iter 1433:0.4957\n",
      "train accuracy: 0.873\n",
      "loss at iter 1434:0.5089\n",
      "train accuracy: 0.874\n",
      "loss at iter 1435:0.5000\n",
      "train accuracy: 0.87\n",
      "loss at iter 1436:0.5049\n",
      "train accuracy: 0.866\n",
      "loss at iter 1437:0.5048\n",
      "train accuracy: 0.868\n",
      "loss at iter 1438:0.5246\n",
      "train accuracy: 0.862\n",
      "loss at iter 1439:0.5305\n",
      "train accuracy: 0.857\n",
      "loss at iter 1440:0.5501\n",
      "train accuracy: 0.849\n",
      "loss at iter 1441:0.4814\n",
      "train accuracy: 0.875\n",
      "loss at iter 1442:0.5795\n",
      "train accuracy: 0.834\n",
      "loss at iter 1443:0.5231\n",
      "train accuracy: 0.867\n",
      "loss at iter 1444:0.4738\n",
      "train accuracy: 0.881\n",
      "loss at iter 1445:0.4998\n",
      "train accuracy: 0.877\n",
      "loss at iter 1446:0.4832\n",
      "train accuracy: 0.87\n",
      "loss at iter 1447:0.4942\n",
      "train accuracy: 0.872\n",
      "loss at iter 1448:0.5386\n",
      "train accuracy: 0.864\n",
      "loss at iter 1449:0.4935\n",
      "train accuracy: 0.876\n",
      "loss at iter 1450:0.4799\n",
      "train accuracy: 0.877\n",
      "loss at iter 1451:0.5120\n",
      "train accuracy: 0.873\n",
      "loss at iter 1452:0.4873\n",
      "train accuracy: 0.885\n",
      "loss at iter 1453:0.4994\n",
      "train accuracy: 0.88\n",
      "loss at iter 1454:0.4907\n",
      "train accuracy: 0.878\n",
      "loss at iter 1455:0.5200\n",
      "train accuracy: 0.85\n",
      "loss at iter 1456:0.4937\n",
      "train accuracy: 0.863\n",
      "loss at iter 1457:0.5180\n",
      "train accuracy: 0.876\n",
      "loss at iter 1458:0.4504\n",
      "train accuracy: 0.882\n",
      "loss at iter 1459:0.5244\n",
      "train accuracy: 0.875\n",
      "loss at iter 1460:0.5069\n",
      "train accuracy: 0.862\n",
      "loss at iter 1461:0.4698\n",
      "train accuracy: 0.877\n",
      "loss at iter 1462:0.5104\n",
      "train accuracy: 0.86\n",
      "loss at iter 1463:0.4770\n",
      "train accuracy: 0.888\n",
      "loss at iter 1464:0.4971\n",
      "train accuracy: 0.865\n",
      "loss at iter 1465:0.5276\n",
      "train accuracy: 0.858\n",
      "loss at iter 1466:0.4516\n",
      "train accuracy: 0.893\n",
      "loss at iter 1467:0.5657\n",
      "train accuracy: 0.842\n",
      "loss at iter 1468:0.5192\n",
      "train accuracy: 0.87\n",
      "loss at iter 1469:0.5241\n",
      "train accuracy: 0.86\n",
      "loss at iter 1470:0.5264\n",
      "train accuracy: 0.861\n",
      "loss at iter 1471:0.5022\n",
      "train accuracy: 0.869\n",
      "loss at iter 1472:0.5364\n",
      "train accuracy: 0.839\n",
      "loss at iter 1473:0.5283\n",
      "train accuracy: 0.866\n",
      "loss at iter 1474:0.4840\n",
      "train accuracy: 0.87\n",
      "loss at iter 1475:0.5053\n",
      "train accuracy: 0.866\n",
      "loss at iter 1476:0.4821\n",
      "train accuracy: 0.879\n",
      "loss at iter 1477:0.4524\n",
      "train accuracy: 0.895\n",
      "loss at iter 1478:0.5233\n",
      "train accuracy: 0.857\n",
      "loss at iter 1479:0.4626\n",
      "train accuracy: 0.878\n",
      "loss at iter 1480:0.5435\n",
      "train accuracy: 0.85\n",
      "loss at iter 1481:0.5071\n",
      "train accuracy: 0.877\n",
      "loss at iter 1482:0.4533\n",
      "train accuracy: 0.881\n",
      "loss at iter 1483:0.5365\n",
      "train accuracy: 0.853\n",
      "loss at iter 1484:0.4832\n",
      "train accuracy: 0.883\n",
      "loss at iter 1485:0.4861\n",
      "train accuracy: 0.873\n",
      "loss at iter 1486:0.4842\n",
      "train accuracy: 0.873\n",
      "loss at iter 1487:0.4956\n",
      "train accuracy: 0.87\n",
      "loss at iter 1488:0.4719\n",
      "train accuracy: 0.884\n",
      "loss at iter 1489:0.4547\n",
      "train accuracy: 0.881\n",
      "loss at iter 1490:0.4666\n",
      "train accuracy: 0.874\n",
      "loss at iter 1491:0.4552\n",
      "train accuracy: 0.891\n",
      "loss at iter 1492:0.5238\n",
      "train accuracy: 0.854\n",
      "loss at iter 1493:0.5095\n",
      "train accuracy: 0.853\n",
      "loss at iter 1494:0.4498\n",
      "train accuracy: 0.893\n",
      "loss at iter 1495:0.5179\n",
      "train accuracy: 0.858\n",
      "loss at iter 1496:0.5033\n",
      "train accuracy: 0.868\n",
      "loss at iter 1497:0.5269\n",
      "train accuracy: 0.852\n",
      "loss at iter 1498:0.5248\n",
      "train accuracy: 0.849\n",
      "loss at iter 1499:0.5139\n",
      "train accuracy: 0.865\n",
      "loss at iter 1500:0.5154\n",
      "train accuracy: 0.866\n",
      "loss at iter 1501:0.4691\n",
      "train accuracy: 0.88\n",
      "loss at iter 1502:0.5091\n",
      "train accuracy: 0.866\n",
      "loss at iter 1503:0.4652\n",
      "train accuracy: 0.884\n",
      "loss at iter 1504:0.5080\n",
      "train accuracy: 0.864\n",
      "loss at iter 1505:0.4733\n",
      "train accuracy: 0.874\n",
      "loss at iter 1506:0.5248\n",
      "train accuracy: 0.861\n",
      "loss at iter 1507:0.5157\n",
      "train accuracy: 0.862\n",
      "loss at iter 1508:0.5232\n",
      "train accuracy: 0.857\n",
      "loss at iter 1509:0.4814\n",
      "train accuracy: 0.874\n",
      "loss at iter 1510:0.5098\n",
      "train accuracy: 0.873\n",
      "loss at iter 1511:0.4305\n",
      "train accuracy: 0.904\n",
      "loss at iter 1512:0.5064\n",
      "train accuracy: 0.857\n",
      "loss at iter 1513:0.4844\n",
      "train accuracy: 0.861\n",
      "loss at iter 1514:0.4842\n",
      "train accuracy: 0.871\n",
      "loss at iter 1515:0.4898\n",
      "train accuracy: 0.872\n",
      "loss at iter 1516:0.4816\n",
      "train accuracy: 0.869\n",
      "loss at iter 1517:0.4696\n",
      "train accuracy: 0.874\n",
      "loss at iter 1518:0.4903\n",
      "train accuracy: 0.862\n",
      "loss at iter 1519:0.4391\n",
      "train accuracy: 0.893\n",
      "loss at iter 1520:0.4989\n",
      "train accuracy: 0.871\n",
      "loss at iter 1521:0.5007\n",
      "train accuracy: 0.866\n",
      "loss at iter 1522:0.5337\n",
      "train accuracy: 0.859\n",
      "loss at iter 1523:0.4899\n",
      "train accuracy: 0.87\n",
      "loss at iter 1524:0.4973\n",
      "train accuracy: 0.871\n",
      "loss at iter 1525:0.4721\n",
      "train accuracy: 0.875\n",
      "loss at iter 1526:0.5022\n",
      "train accuracy: 0.87\n",
      "loss at iter 1527:0.4533\n",
      "train accuracy: 0.877\n",
      "loss at iter 1528:0.4390\n",
      "train accuracy: 0.886\n",
      "loss at iter 1529:0.4969\n",
      "train accuracy: 0.878\n",
      "loss at iter 1530:0.5066\n",
      "train accuracy: 0.868\n",
      "loss at iter 1531:0.4790\n",
      "train accuracy: 0.873\n",
      "loss at iter 1532:0.4669\n",
      "train accuracy: 0.879\n",
      "loss at iter 1533:0.4599\n",
      "train accuracy: 0.876\n",
      "loss at iter 1534:0.5138\n",
      "train accuracy: 0.858\n",
      "loss at iter 1535:0.4805\n",
      "train accuracy: 0.876\n",
      "loss at iter 1536:0.4435\n",
      "train accuracy: 0.888\n",
      "loss at iter 1537:0.4403\n",
      "train accuracy: 0.883\n",
      "loss at iter 1538:0.5010\n",
      "train accuracy: 0.872\n",
      "loss at iter 1539:0.4509\n",
      "train accuracy: 0.881\n",
      "loss at iter 1540:0.5040\n",
      "train accuracy: 0.873\n",
      "loss at iter 1541:0.4926\n",
      "train accuracy: 0.868\n",
      "loss at iter 1542:0.4438\n",
      "train accuracy: 0.885\n",
      "loss at iter 1543:0.4860\n",
      "train accuracy: 0.878\n",
      "loss at iter 1544:0.4893\n",
      "train accuracy: 0.869\n",
      "loss at iter 1545:0.4720\n",
      "train accuracy: 0.889\n",
      "loss at iter 1546:0.5132\n",
      "train accuracy: 0.861\n",
      "loss at iter 1547:0.4734\n",
      "train accuracy: 0.89\n",
      "loss at iter 1548:0.4905\n",
      "train accuracy: 0.871\n",
      "loss at iter 1549:0.4864\n",
      "train accuracy: 0.869\n",
      "loss at iter 1550:0.4976\n",
      "train accuracy: 0.869\n",
      "loss at iter 1551:0.4865\n",
      "train accuracy: 0.873\n",
      "loss at iter 1552:0.4768\n",
      "train accuracy: 0.873\n",
      "loss at iter 1553:0.4426\n",
      "train accuracy: 0.884\n",
      "loss at iter 1554:0.4323\n",
      "train accuracy: 0.894\n",
      "loss at iter 1555:0.4718\n",
      "train accuracy: 0.892\n",
      "loss at iter 1556:0.4770\n",
      "train accuracy: 0.878\n",
      "loss at iter 1557:0.4663\n",
      "train accuracy: 0.874\n",
      "loss at iter 1558:0.4909\n",
      "train accuracy: 0.88\n",
      "loss at iter 1559:0.4739\n",
      "train accuracy: 0.878\n",
      "loss at iter 1560:0.4889\n",
      "train accuracy: 0.873\n",
      "loss at iter 1561:0.4596\n",
      "train accuracy: 0.873\n",
      "loss at iter 1562:0.4526\n",
      "train accuracy: 0.882\n",
      "loss at iter 1563:0.4102\n",
      "train accuracy: 0.904\n",
      "loss at iter 1564:0.4989\n",
      "train accuracy: 0.859\n",
      "loss at iter 1565:0.4601\n",
      "train accuracy: 0.875\n",
      "loss at iter 1566:0.4480\n",
      "train accuracy: 0.886\n",
      "loss at iter 1567:0.4614\n",
      "train accuracy: 0.873\n",
      "loss at iter 1568:0.4794\n",
      "train accuracy: 0.873\n",
      "loss at iter 1569:0.4534\n",
      "train accuracy: 0.886\n",
      "loss at iter 1570:0.4550\n",
      "train accuracy: 0.871\n",
      "loss at iter 1571:0.4912\n",
      "train accuracy: 0.871\n",
      "loss at iter 1572:0.4913\n",
      "train accuracy: 0.872\n",
      "loss at iter 1573:0.4998\n",
      "train accuracy: 0.875\n",
      "loss at iter 1574:0.4858\n",
      "train accuracy: 0.868\n",
      "loss at iter 1575:0.5166\n",
      "train accuracy: 0.864\n",
      "loss at iter 1576:0.4751\n",
      "train accuracy: 0.876\n",
      "loss at iter 1577:0.5054\n",
      "train accuracy: 0.862\n",
      "loss at iter 1578:0.4893\n",
      "train accuracy: 0.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1579:0.4402\n",
      "train accuracy: 0.887\n",
      "loss at iter 1580:0.4575\n",
      "train accuracy: 0.871\n",
      "loss at iter 1581:0.4457\n",
      "train accuracy: 0.873\n",
      "loss at iter 1582:0.4661\n",
      "train accuracy: 0.886\n",
      "loss at iter 1583:0.4497\n",
      "train accuracy: 0.878\n",
      "loss at iter 1584:0.4725\n",
      "train accuracy: 0.88\n",
      "loss at iter 1585:0.4841\n",
      "train accuracy: 0.873\n",
      "loss at iter 1586:0.4567\n",
      "train accuracy: 0.887\n",
      "loss at iter 1587:0.4818\n",
      "train accuracy: 0.887\n",
      "loss at iter 1588:0.4282\n",
      "train accuracy: 0.886\n",
      "loss at iter 1589:0.4747\n",
      "train accuracy: 0.876\n",
      "loss at iter 1590:0.4668\n",
      "train accuracy: 0.874\n",
      "loss at iter 1591:0.4854\n",
      "train accuracy: 0.867\n",
      "loss at iter 1592:0.4940\n",
      "train accuracy: 0.864\n",
      "loss at iter 1593:0.4589\n",
      "train accuracy: 0.885\n",
      "loss at iter 1594:0.4864\n",
      "train accuracy: 0.875\n",
      "loss at iter 1595:0.4656\n",
      "train accuracy: 0.875\n",
      "loss at iter 1596:0.5078\n",
      "train accuracy: 0.86\n",
      "loss at iter 1597:0.4996\n",
      "train accuracy: 0.864\n",
      "loss at iter 1598:0.5005\n",
      "train accuracy: 0.87\n",
      "loss at iter 1599:0.4597\n",
      "train accuracy: 0.881\n",
      "loss at iter 1600:0.4441\n",
      "train accuracy: 0.876\n",
      "loss at iter 1601:0.4736\n",
      "train accuracy: 0.867\n",
      "loss at iter 1602:0.4743\n",
      "train accuracy: 0.876\n",
      "loss at iter 1603:0.4734\n",
      "train accuracy: 0.878\n",
      "loss at iter 1604:0.4695\n",
      "train accuracy: 0.879\n",
      "loss at iter 1605:0.4400\n",
      "train accuracy: 0.889\n",
      "loss at iter 1606:0.4390\n",
      "train accuracy: 0.901\n",
      "loss at iter 1607:0.4637\n",
      "train accuracy: 0.884\n",
      "loss at iter 1608:0.4388\n",
      "train accuracy: 0.88\n",
      "loss at iter 1609:0.4852\n",
      "train accuracy: 0.874\n",
      "loss at iter 1610:0.4491\n",
      "train accuracy: 0.881\n",
      "loss at iter 1611:0.4772\n",
      "train accuracy: 0.884\n",
      "loss at iter 1612:0.4256\n",
      "train accuracy: 0.894\n",
      "loss at iter 1613:0.4560\n",
      "train accuracy: 0.878\n",
      "loss at iter 1614:0.4806\n",
      "train accuracy: 0.874\n",
      "loss at iter 1615:0.4690\n",
      "train accuracy: 0.879\n",
      "loss at iter 1616:0.4866\n",
      "train accuracy: 0.857\n",
      "loss at iter 1617:0.4750\n",
      "train accuracy: 0.867\n",
      "loss at iter 1618:0.4705\n",
      "train accuracy: 0.868\n",
      "loss at iter 1619:0.4646\n",
      "train accuracy: 0.883\n",
      "loss at iter 1620:0.4641\n",
      "train accuracy: 0.877\n",
      "loss at iter 1621:0.4939\n",
      "train accuracy: 0.866\n",
      "loss at iter 1622:0.4778\n",
      "train accuracy: 0.874\n",
      "loss at iter 1623:0.4592\n",
      "train accuracy: 0.875\n",
      "loss at iter 1624:0.4446\n",
      "train accuracy: 0.884\n",
      "loss at iter 1625:0.4755\n",
      "train accuracy: 0.878\n",
      "loss at iter 1626:0.4895\n",
      "train accuracy: 0.859\n",
      "loss at iter 1627:0.4876\n",
      "train accuracy: 0.878\n",
      "loss at iter 1628:0.5108\n",
      "train accuracy: 0.872\n",
      "loss at iter 1629:0.4197\n",
      "train accuracy: 0.895\n",
      "loss at iter 1630:0.4729\n",
      "train accuracy: 0.874\n",
      "loss at iter 1631:0.4758\n",
      "train accuracy: 0.863\n",
      "loss at iter 1632:0.4905\n",
      "train accuracy: 0.873\n",
      "loss at iter 1633:0.4515\n",
      "train accuracy: 0.88\n",
      "loss at iter 1634:0.4599\n",
      "train accuracy: 0.895\n",
      "loss at iter 1635:0.4457\n",
      "train accuracy: 0.877\n",
      "loss at iter 1636:0.4331\n",
      "train accuracy: 0.881\n",
      "loss at iter 1637:0.4303\n",
      "train accuracy: 0.895\n",
      "loss at iter 1638:0.4743\n",
      "train accuracy: 0.87\n",
      "loss at iter 1639:0.4636\n",
      "train accuracy: 0.871\n",
      "loss at iter 1640:0.4608\n",
      "train accuracy: 0.868\n",
      "loss at iter 1641:0.4472\n",
      "train accuracy: 0.889\n",
      "loss at iter 1642:0.4029\n",
      "train accuracy: 0.899\n",
      "loss at iter 1643:0.4452\n",
      "train accuracy: 0.892\n",
      "loss at iter 1644:0.4443\n",
      "train accuracy: 0.892\n",
      "loss at iter 1645:0.4494\n",
      "train accuracy: 0.892\n",
      "loss at iter 1646:0.4602\n",
      "train accuracy: 0.877\n",
      "loss at iter 1647:0.4273\n",
      "train accuracy: 0.886\n",
      "loss at iter 1648:0.4567\n",
      "train accuracy: 0.884\n",
      "loss at iter 1649:0.4691\n",
      "train accuracy: 0.874\n",
      "loss at iter 1650:0.4416\n",
      "train accuracy: 0.879\n",
      "loss at iter 1651:0.4344\n",
      "train accuracy: 0.894\n",
      "loss at iter 1652:0.4272\n",
      "train accuracy: 0.884\n",
      "loss at iter 1653:0.4805\n",
      "train accuracy: 0.871\n",
      "loss at iter 1654:0.4284\n",
      "train accuracy: 0.887\n",
      "loss at iter 1655:0.4685\n",
      "train accuracy: 0.865\n",
      "loss at iter 1656:0.4853\n",
      "train accuracy: 0.873\n",
      "loss at iter 1657:0.4306\n",
      "train accuracy: 0.893\n",
      "loss at iter 1658:0.4633\n",
      "train accuracy: 0.88\n",
      "loss at iter 1659:0.4524\n",
      "train accuracy: 0.89\n",
      "loss at iter 1660:0.4813\n",
      "train accuracy: 0.879\n",
      "loss at iter 1661:0.4821\n",
      "train accuracy: 0.872\n",
      "loss at iter 1662:0.4710\n",
      "train accuracy: 0.871\n",
      "loss at iter 1663:0.4106\n",
      "train accuracy: 0.899\n",
      "loss at iter 1664:0.4312\n",
      "train accuracy: 0.895\n",
      "loss at iter 1665:0.4578\n",
      "train accuracy: 0.884\n",
      "loss at iter 1666:0.4432\n",
      "train accuracy: 0.886\n",
      "loss at iter 1667:0.4337\n",
      "train accuracy: 0.888\n",
      "loss at iter 1668:0.4722\n",
      "train accuracy: 0.867\n",
      "loss at iter 1669:0.4594\n",
      "train accuracy: 0.884\n",
      "loss at iter 1670:0.4611\n",
      "train accuracy: 0.886\n",
      "loss at iter 1671:0.4321\n",
      "train accuracy: 0.888\n",
      "loss at iter 1672:0.4617\n",
      "train accuracy: 0.885\n",
      "loss at iter 1673:0.4397\n",
      "train accuracy: 0.878\n",
      "loss at iter 1674:0.4213\n",
      "train accuracy: 0.899\n",
      "loss at iter 1675:0.4382\n",
      "train accuracy: 0.884\n",
      "loss at iter 1676:0.4494\n",
      "train accuracy: 0.879\n",
      "loss at iter 1677:0.4410\n",
      "train accuracy: 0.884\n",
      "loss at iter 1678:0.4274\n",
      "train accuracy: 0.896\n",
      "loss at iter 1679:0.4525\n",
      "train accuracy: 0.876\n",
      "loss at iter 1680:0.4889\n",
      "train accuracy: 0.86\n",
      "loss at iter 1681:0.4810\n",
      "train accuracy: 0.872\n",
      "loss at iter 1682:0.4946\n",
      "train accuracy: 0.872\n",
      "loss at iter 1683:0.4385\n",
      "train accuracy: 0.887\n",
      "loss at iter 1684:0.4114\n",
      "train accuracy: 0.899\n",
      "loss at iter 1685:0.4538\n",
      "train accuracy: 0.883\n",
      "loss at iter 1686:0.4562\n",
      "train accuracy: 0.879\n",
      "loss at iter 1687:0.4534\n",
      "train accuracy: 0.883\n",
      "loss at iter 1688:0.4355\n",
      "train accuracy: 0.881\n",
      "loss at iter 1689:0.4332\n",
      "train accuracy: 0.88\n",
      "loss at iter 1690:0.4578\n",
      "train accuracy: 0.873\n",
      "loss at iter 1691:0.4717\n",
      "train accuracy: 0.872\n",
      "loss at iter 1692:0.4351\n",
      "train accuracy: 0.888\n",
      "loss at iter 1693:0.4398\n",
      "train accuracy: 0.886\n",
      "loss at iter 1694:0.4058\n",
      "train accuracy: 0.9\n",
      "loss at iter 1695:0.4300\n",
      "train accuracy: 0.885\n",
      "loss at iter 1696:0.4248\n",
      "train accuracy: 0.896\n",
      "loss at iter 1697:0.4353\n",
      "train accuracy: 0.884\n",
      "loss at iter 1698:0.5070\n",
      "train accuracy: 0.863\n",
      "loss at iter 1699:0.4720\n",
      "train accuracy: 0.874\n",
      "loss at iter 1700:0.4465\n",
      "train accuracy: 0.87\n",
      "loss at iter 1701:0.4196\n",
      "train accuracy: 0.89\n",
      "loss at iter 1702:0.4499\n",
      "train accuracy: 0.878\n",
      "loss at iter 1703:0.4282\n",
      "train accuracy: 0.896\n",
      "loss at iter 1704:0.4437\n",
      "train accuracy: 0.884\n",
      "loss at iter 1705:0.4495\n",
      "train accuracy: 0.88\n",
      "loss at iter 1706:0.4997\n",
      "train accuracy: 0.871\n",
      "loss at iter 1707:0.4762\n",
      "train accuracy: 0.875\n",
      "loss at iter 1708:0.4168\n",
      "train accuracy: 0.903\n",
      "loss at iter 1709:0.4815\n",
      "train accuracy: 0.869\n",
      "loss at iter 1710:0.4403\n",
      "train accuracy: 0.887\n",
      "loss at iter 1711:0.4534\n",
      "train accuracy: 0.886\n",
      "loss at iter 1712:0.4213\n",
      "train accuracy: 0.897\n",
      "loss at iter 1713:0.4403\n",
      "train accuracy: 0.896\n",
      "loss at iter 1714:0.4015\n",
      "train accuracy: 0.888\n",
      "loss at iter 1715:0.4330\n",
      "train accuracy: 0.877\n",
      "loss at iter 1716:0.4372\n",
      "train accuracy: 0.883\n",
      "loss at iter 1717:0.4196\n",
      "train accuracy: 0.884\n",
      "loss at iter 1718:0.4415\n",
      "train accuracy: 0.887\n",
      "loss at iter 1719:0.4537\n",
      "train accuracy: 0.878\n",
      "loss at iter 1720:0.4356\n",
      "train accuracy: 0.88\n",
      "loss at iter 1721:0.4036\n",
      "train accuracy: 0.904\n",
      "loss at iter 1722:0.4616\n",
      "train accuracy: 0.874\n",
      "loss at iter 1723:0.4452\n",
      "train accuracy: 0.877\n",
      "loss at iter 1724:0.4423\n",
      "train accuracy: 0.884\n",
      "loss at iter 1725:0.4455\n",
      "train accuracy: 0.891\n",
      "loss at iter 1726:0.4673\n",
      "train accuracy: 0.879\n",
      "loss at iter 1727:0.4474\n",
      "train accuracy: 0.892\n",
      "loss at iter 1728:0.4374\n",
      "train accuracy: 0.883\n",
      "loss at iter 1729:0.4659\n",
      "train accuracy: 0.88\n",
      "loss at iter 1730:0.4287\n",
      "train accuracy: 0.889\n",
      "loss at iter 1731:0.4554\n",
      "train accuracy: 0.876\n",
      "loss at iter 1732:0.4694\n",
      "train accuracy: 0.879\n",
      "loss at iter 1733:0.4509\n",
      "train accuracy: 0.867\n",
      "loss at iter 1734:0.4021\n",
      "train accuracy: 0.889\n",
      "loss at iter 1735:0.4063\n",
      "train accuracy: 0.895\n",
      "loss at iter 1736:0.4531\n",
      "train accuracy: 0.876\n",
      "loss at iter 1737:0.4223\n",
      "train accuracy: 0.891\n",
      "loss at iter 1738:0.4624\n",
      "train accuracy: 0.873\n",
      "loss at iter 1739:0.4349\n",
      "train accuracy: 0.89\n",
      "loss at iter 1740:0.4394\n",
      "train accuracy: 0.88\n",
      "loss at iter 1741:0.4662\n",
      "train accuracy: 0.875\n",
      "loss at iter 1742:0.4307\n",
      "train accuracy: 0.879\n",
      "loss at iter 1743:0.4336\n",
      "train accuracy: 0.882\n",
      "loss at iter 1744:0.4382\n",
      "train accuracy: 0.888\n",
      "loss at iter 1745:0.4346\n",
      "train accuracy: 0.88\n",
      "loss at iter 1746:0.4458\n",
      "train accuracy: 0.889\n",
      "loss at iter 1747:0.4227\n",
      "train accuracy: 0.898\n",
      "loss at iter 1748:0.4277\n",
      "train accuracy: 0.89\n",
      "loss at iter 1749:0.3827\n",
      "train accuracy: 0.899\n",
      "loss at iter 1750:0.4712\n",
      "train accuracy: 0.871\n",
      "loss at iter 1751:0.4881\n",
      "train accuracy: 0.87\n",
      "loss at iter 1752:0.3793\n",
      "train accuracy: 0.913\n",
      "loss at iter 1753:0.3902\n",
      "train accuracy: 0.905\n",
      "loss at iter 1754:0.4554\n",
      "train accuracy: 0.883\n",
      "loss at iter 1755:0.4188\n",
      "train accuracy: 0.891\n",
      "loss at iter 1756:0.4279\n",
      "train accuracy: 0.885\n",
      "loss at iter 1757:0.4486\n",
      "train accuracy: 0.888\n",
      "loss at iter 1758:0.4370\n",
      "train accuracy: 0.889\n",
      "loss at iter 1759:0.4089\n",
      "train accuracy: 0.89\n",
      "loss at iter 1760:0.5039\n",
      "train accuracy: 0.867\n",
      "loss at iter 1761:0.4351\n",
      "train accuracy: 0.876\n",
      "loss at iter 1762:0.4421\n",
      "train accuracy: 0.879\n",
      "loss at iter 1763:0.3920\n",
      "train accuracy: 0.893\n",
      "loss at iter 1764:0.4301\n",
      "train accuracy: 0.878\n",
      "loss at iter 1765:0.4504\n",
      "train accuracy: 0.887\n",
      "loss at iter 1766:0.4671\n",
      "train accuracy: 0.885\n",
      "loss at iter 1767:0.4063\n",
      "train accuracy: 0.886\n",
      "loss at iter 1768:0.4516\n",
      "train accuracy: 0.881\n",
      "loss at iter 1769:0.4576\n",
      "train accuracy: 0.878\n",
      "loss at iter 1770:0.4150\n",
      "train accuracy: 0.895\n",
      "loss at iter 1771:0.4511\n",
      "train accuracy: 0.875\n",
      "loss at iter 1772:0.4011\n",
      "train accuracy: 0.897\n",
      "loss at iter 1773:0.4512\n",
      "train accuracy: 0.882\n",
      "loss at iter 1774:0.4551\n",
      "train accuracy: 0.873\n",
      "loss at iter 1775:0.4180\n",
      "train accuracy: 0.883\n",
      "loss at iter 1776:0.4145\n",
      "train accuracy: 0.894\n",
      "loss at iter 1777:0.4039\n",
      "train accuracy: 0.892\n",
      "loss at iter 1778:0.4370\n",
      "train accuracy: 0.889\n",
      "loss at iter 1779:0.4593\n",
      "train accuracy: 0.878\n",
      "loss at iter 1780:0.3890\n",
      "train accuracy: 0.903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1781:0.4234\n",
      "train accuracy: 0.899\n",
      "loss at iter 1782:0.4334\n",
      "train accuracy: 0.878\n",
      "loss at iter 1783:0.4541\n",
      "train accuracy: 0.874\n",
      "loss at iter 1784:0.4743\n",
      "train accuracy: 0.868\n",
      "loss at iter 1785:0.4050\n",
      "train accuracy: 0.897\n",
      "loss at iter 1786:0.3967\n",
      "train accuracy: 0.902\n",
      "loss at iter 1787:0.4244\n",
      "train accuracy: 0.895\n",
      "loss at iter 1788:0.4177\n",
      "train accuracy: 0.885\n",
      "loss at iter 1789:0.4545\n",
      "train accuracy: 0.88\n",
      "loss at iter 1790:0.4187\n",
      "train accuracy: 0.899\n",
      "loss at iter 1791:0.4271\n",
      "train accuracy: 0.882\n",
      "loss at iter 1792:0.4294\n",
      "train accuracy: 0.902\n",
      "loss at iter 1793:0.4347\n",
      "train accuracy: 0.89\n",
      "loss at iter 1794:0.3959\n",
      "train accuracy: 0.891\n",
      "loss at iter 1795:0.4262\n",
      "train accuracy: 0.884\n",
      "loss at iter 1796:0.4299\n",
      "train accuracy: 0.879\n",
      "loss at iter 1797:0.4125\n",
      "train accuracy: 0.889\n",
      "loss at iter 1798:0.4252\n",
      "train accuracy: 0.876\n",
      "loss at iter 1799:0.4183\n",
      "train accuracy: 0.896\n",
      "loss at iter 1800:0.4004\n",
      "train accuracy: 0.896\n",
      "loss at iter 1801:0.4456\n",
      "train accuracy: 0.88\n",
      "loss at iter 1802:0.3985\n",
      "train accuracy: 0.892\n",
      "loss at iter 1803:0.4100\n",
      "train accuracy: 0.897\n",
      "loss at iter 1804:0.4128\n",
      "train accuracy: 0.899\n",
      "loss at iter 1805:0.4120\n",
      "train accuracy: 0.893\n",
      "loss at iter 1806:0.4453\n",
      "train accuracy: 0.873\n",
      "loss at iter 1807:0.4019\n",
      "train accuracy: 0.903\n",
      "loss at iter 1808:0.4180\n",
      "train accuracy: 0.883\n",
      "loss at iter 1809:0.4457\n",
      "train accuracy: 0.881\n",
      "loss at iter 1810:0.4499\n",
      "train accuracy: 0.881\n",
      "loss at iter 1811:0.4186\n",
      "train accuracy: 0.903\n",
      "loss at iter 1812:0.3984\n",
      "train accuracy: 0.891\n",
      "loss at iter 1813:0.4020\n",
      "train accuracy: 0.896\n",
      "loss at iter 1814:0.4160\n",
      "train accuracy: 0.886\n",
      "loss at iter 1815:0.4444\n",
      "train accuracy: 0.876\n",
      "loss at iter 1816:0.4155\n",
      "train accuracy: 0.886\n",
      "loss at iter 1817:0.4678\n",
      "train accuracy: 0.873\n",
      "loss at iter 1818:0.4025\n",
      "train accuracy: 0.895\n",
      "loss at iter 1819:0.4061\n",
      "train accuracy: 0.9\n",
      "loss at iter 1820:0.4540\n",
      "train accuracy: 0.875\n",
      "loss at iter 1821:0.4030\n",
      "train accuracy: 0.892\n",
      "loss at iter 1822:0.4027\n",
      "train accuracy: 0.88\n",
      "loss at iter 1823:0.4332\n",
      "train accuracy: 0.884\n",
      "loss at iter 1824:0.4350\n",
      "train accuracy: 0.888\n",
      "loss at iter 1825:0.4354\n",
      "train accuracy: 0.887\n",
      "loss at iter 1826:0.4512\n",
      "train accuracy: 0.876\n",
      "loss at iter 1827:0.4607\n",
      "train accuracy: 0.871\n",
      "loss at iter 1828:0.4360\n",
      "train accuracy: 0.887\n",
      "loss at iter 1829:0.4255\n",
      "train accuracy: 0.887\n",
      "loss at iter 1830:0.4074\n",
      "train accuracy: 0.883\n",
      "loss at iter 1831:0.4091\n",
      "train accuracy: 0.892\n",
      "loss at iter 1832:0.4343\n",
      "train accuracy: 0.881\n",
      "loss at iter 1833:0.4344\n",
      "train accuracy: 0.888\n",
      "loss at iter 1834:0.4437\n",
      "train accuracy: 0.893\n",
      "loss at iter 1835:0.4652\n",
      "train accuracy: 0.877\n",
      "loss at iter 1836:0.3882\n",
      "train accuracy: 0.896\n",
      "loss at iter 1837:0.4247\n",
      "train accuracy: 0.891\n",
      "loss at iter 1838:0.4171\n",
      "train accuracy: 0.889\n",
      "loss at iter 1839:0.4259\n",
      "train accuracy: 0.895\n",
      "loss at iter 1840:0.4265\n",
      "train accuracy: 0.887\n",
      "loss at iter 1841:0.4254\n",
      "train accuracy: 0.889\n",
      "loss at iter 1842:0.3721\n",
      "train accuracy: 0.909\n",
      "loss at iter 1843:0.3934\n",
      "train accuracy: 0.887\n",
      "loss at iter 1844:0.4419\n",
      "train accuracy: 0.89\n",
      "loss at iter 1845:0.4272\n",
      "train accuracy: 0.884\n",
      "loss at iter 1846:0.4232\n",
      "train accuracy: 0.887\n",
      "loss at iter 1847:0.4285\n",
      "train accuracy: 0.882\n",
      "loss at iter 1848:0.4093\n",
      "train accuracy: 0.886\n",
      "loss at iter 1849:0.3955\n",
      "train accuracy: 0.903\n",
      "loss at iter 1850:0.4518\n",
      "train accuracy: 0.88\n",
      "loss at iter 1851:0.3866\n",
      "train accuracy: 0.893\n",
      "loss at iter 1852:0.4320\n",
      "train accuracy: 0.895\n",
      "loss at iter 1853:0.4063\n",
      "train accuracy: 0.886\n",
      "loss at iter 1854:0.3982\n",
      "train accuracy: 0.891\n",
      "loss at iter 1855:0.3754\n",
      "train accuracy: 0.911\n",
      "loss at iter 1856:0.4228\n",
      "train accuracy: 0.907\n",
      "loss at iter 1857:0.4117\n",
      "train accuracy: 0.892\n",
      "loss at iter 1858:0.4081\n",
      "train accuracy: 0.893\n",
      "loss at iter 1859:0.4237\n",
      "train accuracy: 0.891\n",
      "loss at iter 1860:0.4123\n",
      "train accuracy: 0.887\n",
      "loss at iter 1861:0.4225\n",
      "train accuracy: 0.898\n",
      "loss at iter 1862:0.3983\n",
      "train accuracy: 0.896\n",
      "loss at iter 1863:0.4339\n",
      "train accuracy: 0.882\n",
      "loss at iter 1864:0.4104\n",
      "train accuracy: 0.892\n",
      "loss at iter 1865:0.4354\n",
      "train accuracy: 0.886\n",
      "loss at iter 1866:0.4271\n",
      "train accuracy: 0.881\n",
      "loss at iter 1867:0.4077\n",
      "train accuracy: 0.892\n",
      "loss at iter 1868:0.3829\n",
      "train accuracy: 0.881\n",
      "loss at iter 1869:0.3929\n",
      "train accuracy: 0.902\n",
      "loss at iter 1870:0.4216\n",
      "train accuracy: 0.897\n",
      "loss at iter 1871:0.4296\n",
      "train accuracy: 0.882\n",
      "loss at iter 1872:0.4481\n",
      "train accuracy: 0.878\n",
      "loss at iter 1873:0.3923\n",
      "train accuracy: 0.908\n",
      "loss at iter 1874:0.4414\n",
      "train accuracy: 0.875\n",
      "loss at iter 1875:0.4183\n",
      "train accuracy: 0.886\n",
      "loss at iter 1876:0.3938\n",
      "train accuracy: 0.898\n",
      "loss at iter 1877:0.4130\n",
      "train accuracy: 0.897\n",
      "loss at iter 1878:0.4491\n",
      "train accuracy: 0.882\n",
      "loss at iter 1879:0.4293\n",
      "train accuracy: 0.883\n",
      "loss at iter 1880:0.3778\n",
      "train accuracy: 0.898\n",
      "loss at iter 1881:0.4299\n",
      "train accuracy: 0.89\n",
      "loss at iter 1882:0.4750\n",
      "train accuracy: 0.87\n",
      "loss at iter 1883:0.3945\n",
      "train accuracy: 0.895\n",
      "loss at iter 1884:0.4145\n",
      "train accuracy: 0.881\n",
      "loss at iter 1885:0.3893\n",
      "train accuracy: 0.898\n",
      "loss at iter 1886:0.4313\n",
      "train accuracy: 0.888\n",
      "loss at iter 1887:0.3829\n",
      "train accuracy: 0.904\n",
      "loss at iter 1888:0.4390\n",
      "train accuracy: 0.872\n",
      "loss at iter 1889:0.3962\n",
      "train accuracy: 0.901\n",
      "loss at iter 1890:0.4571\n",
      "train accuracy: 0.868\n",
      "loss at iter 1891:0.3934\n",
      "train accuracy: 0.907\n",
      "loss at iter 1892:0.4223\n",
      "train accuracy: 0.887\n",
      "loss at iter 1893:0.4295\n",
      "train accuracy: 0.884\n",
      "loss at iter 1894:0.4084\n",
      "train accuracy: 0.891\n",
      "loss at iter 1895:0.4100\n",
      "train accuracy: 0.893\n",
      "loss at iter 1896:0.3924\n",
      "train accuracy: 0.896\n",
      "loss at iter 1897:0.3929\n",
      "train accuracy: 0.901\n",
      "loss at iter 1898:0.4444\n",
      "train accuracy: 0.878\n",
      "loss at iter 1899:0.3968\n",
      "train accuracy: 0.888\n",
      "loss at iter 1900:0.3847\n",
      "train accuracy: 0.886\n",
      "loss at iter 1901:0.4395\n",
      "train accuracy: 0.894\n",
      "loss at iter 1902:0.4050\n",
      "train accuracy: 0.896\n",
      "loss at iter 1903:0.3865\n",
      "train accuracy: 0.894\n",
      "loss at iter 1904:0.4407\n",
      "train accuracy: 0.887\n",
      "loss at iter 1905:0.4726\n",
      "train accuracy: 0.866\n",
      "loss at iter 1906:0.3909\n",
      "train accuracy: 0.893\n",
      "loss at iter 1907:0.4379\n",
      "train accuracy: 0.884\n",
      "loss at iter 1908:0.3884\n",
      "train accuracy: 0.899\n",
      "loss at iter 1909:0.4169\n",
      "train accuracy: 0.88\n",
      "loss at iter 1910:0.3846\n",
      "train accuracy: 0.908\n",
      "loss at iter 1911:0.3565\n",
      "train accuracy: 0.908\n",
      "loss at iter 1912:0.4223\n",
      "train accuracy: 0.889\n",
      "loss at iter 1913:0.3987\n",
      "train accuracy: 0.898\n",
      "loss at iter 1914:0.3647\n",
      "train accuracy: 0.909\n",
      "loss at iter 1915:0.4250\n",
      "train accuracy: 0.875\n",
      "loss at iter 1916:0.4494\n",
      "train accuracy: 0.885\n",
      "loss at iter 1917:0.3675\n",
      "train accuracy: 0.896\n",
      "loss at iter 1918:0.3997\n",
      "train accuracy: 0.892\n",
      "loss at iter 1919:0.4383\n",
      "train accuracy: 0.885\n",
      "loss at iter 1920:0.4271\n",
      "train accuracy: 0.89\n",
      "loss at iter 1921:0.4533\n",
      "train accuracy: 0.878\n",
      "loss at iter 1922:0.4708\n",
      "train accuracy: 0.867\n",
      "loss at iter 1923:0.4333\n",
      "train accuracy: 0.891\n",
      "loss at iter 1924:0.4383\n",
      "train accuracy: 0.886\n",
      "loss at iter 1925:0.3846\n",
      "train accuracy: 0.894\n",
      "loss at iter 1926:0.4233\n",
      "train accuracy: 0.884\n",
      "loss at iter 1927:0.3699\n",
      "train accuracy: 0.9\n",
      "loss at iter 1928:0.4470\n",
      "train accuracy: 0.873\n",
      "loss at iter 1929:0.3745\n",
      "train accuracy: 0.896\n",
      "loss at iter 1930:0.4146\n",
      "train accuracy: 0.897\n",
      "loss at iter 1931:0.3923\n",
      "train accuracy: 0.908\n",
      "loss at iter 1932:0.3550\n",
      "train accuracy: 0.921\n",
      "loss at iter 1933:0.3958\n",
      "train accuracy: 0.889\n",
      "loss at iter 1934:0.3958\n",
      "train accuracy: 0.887\n",
      "loss at iter 1935:0.3719\n",
      "train accuracy: 0.902\n",
      "loss at iter 1936:0.4132\n",
      "train accuracy: 0.894\n",
      "loss at iter 1937:0.3974\n",
      "train accuracy: 0.903\n",
      "loss at iter 1938:0.4257\n",
      "train accuracy: 0.893\n",
      "loss at iter 1939:0.4360\n",
      "train accuracy: 0.875\n",
      "loss at iter 1940:0.4030\n",
      "train accuracy: 0.885\n",
      "loss at iter 1941:0.4083\n",
      "train accuracy: 0.884\n",
      "loss at iter 1942:0.3911\n",
      "train accuracy: 0.898\n",
      "loss at iter 1943:0.4349\n",
      "train accuracy: 0.891\n",
      "loss at iter 1944:0.4150\n",
      "train accuracy: 0.88\n",
      "loss at iter 1945:0.4074\n",
      "train accuracy: 0.896\n",
      "loss at iter 1946:0.4017\n",
      "train accuracy: 0.894\n",
      "loss at iter 1947:0.3633\n",
      "train accuracy: 0.909\n",
      "loss at iter 1948:0.3831\n",
      "train accuracy: 0.906\n",
      "loss at iter 1949:0.3926\n",
      "train accuracy: 0.896\n",
      "loss at iter 1950:0.3801\n",
      "train accuracy: 0.897\n",
      "loss at iter 1951:0.4313\n",
      "train accuracy: 0.879\n",
      "loss at iter 1952:0.3962\n",
      "train accuracy: 0.895\n",
      "loss at iter 1953:0.4090\n",
      "train accuracy: 0.894\n",
      "loss at iter 1954:0.4461\n",
      "train accuracy: 0.886\n",
      "loss at iter 1955:0.4516\n",
      "train accuracy: 0.888\n",
      "loss at iter 1956:0.4327\n",
      "train accuracy: 0.874\n",
      "loss at iter 1957:0.3514\n",
      "train accuracy: 0.902\n",
      "loss at iter 1958:0.4074\n",
      "train accuracy: 0.886\n",
      "loss at iter 1959:0.4201\n",
      "train accuracy: 0.886\n",
      "loss at iter 1960:0.4412\n",
      "train accuracy: 0.868\n",
      "loss at iter 1961:0.4301\n",
      "train accuracy: 0.887\n",
      "loss at iter 1962:0.3860\n",
      "train accuracy: 0.909\n",
      "loss at iter 1963:0.3877\n",
      "train accuracy: 0.9\n",
      "loss at iter 1964:0.3891\n",
      "train accuracy: 0.892\n",
      "loss at iter 1965:0.3769\n",
      "train accuracy: 0.902\n",
      "loss at iter 1966:0.4051\n",
      "train accuracy: 0.895\n",
      "loss at iter 1967:0.4179\n",
      "train accuracy: 0.893\n",
      "loss at iter 1968:0.4144\n",
      "train accuracy: 0.891\n",
      "loss at iter 1969:0.3977\n",
      "train accuracy: 0.896\n",
      "loss at iter 1970:0.3778\n",
      "train accuracy: 0.901\n",
      "loss at iter 1971:0.3744\n",
      "train accuracy: 0.916\n",
      "loss at iter 1972:0.4229\n",
      "train accuracy: 0.888\n",
      "loss at iter 1973:0.4026\n",
      "train accuracy: 0.893\n",
      "loss at iter 1974:0.3929\n",
      "train accuracy: 0.896\n",
      "loss at iter 1975:0.3823\n",
      "train accuracy: 0.895\n",
      "loss at iter 1976:0.3897\n",
      "train accuracy: 0.895\n",
      "loss at iter 1977:0.3785\n",
      "train accuracy: 0.901\n",
      "loss at iter 1978:0.4178\n",
      "train accuracy: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 1979:0.4076\n",
      "train accuracy: 0.885\n",
      "loss at iter 1980:0.4238\n",
      "train accuracy: 0.887\n",
      "loss at iter 1981:0.4122\n",
      "train accuracy: 0.889\n",
      "loss at iter 1982:0.4245\n",
      "train accuracy: 0.881\n",
      "loss at iter 1983:0.3733\n",
      "train accuracy: 0.902\n",
      "loss at iter 1984:0.4158\n",
      "train accuracy: 0.894\n",
      "loss at iter 1985:0.4796\n",
      "train accuracy: 0.877\n",
      "loss at iter 1986:0.3660\n",
      "train accuracy: 0.908\n",
      "loss at iter 1987:0.3715\n",
      "train accuracy: 0.902\n",
      "loss at iter 1988:0.3842\n",
      "train accuracy: 0.9\n",
      "loss at iter 1989:0.3865\n",
      "train accuracy: 0.891\n",
      "loss at iter 1990:0.3777\n",
      "train accuracy: 0.91\n",
      "loss at iter 1991:0.4063\n",
      "train accuracy: 0.892\n",
      "loss at iter 1992:0.3986\n",
      "train accuracy: 0.885\n",
      "loss at iter 1993:0.3921\n",
      "train accuracy: 0.886\n",
      "loss at iter 1994:0.3677\n",
      "train accuracy: 0.899\n",
      "loss at iter 1995:0.3963\n",
      "train accuracy: 0.897\n",
      "loss at iter 1996:0.4142\n",
      "train accuracy: 0.894\n",
      "loss at iter 1997:0.3702\n",
      "train accuracy: 0.913\n",
      "loss at iter 1998:0.3662\n",
      "train accuracy: 0.902\n",
      "loss at iter 1999:0.4108\n",
      "train accuracy: 0.894\n",
      "loss at iter 2000:0.4151\n",
      "train accuracy: 0.885\n",
      "loss at iter 2001:0.4101\n",
      "train accuracy: 0.885\n",
      "loss at iter 2002:0.4013\n",
      "train accuracy: 0.895\n",
      "loss at iter 2003:0.3867\n",
      "train accuracy: 0.905\n",
      "loss at iter 2004:0.4287\n",
      "train accuracy: 0.889\n",
      "loss at iter 2005:0.4318\n",
      "train accuracy: 0.896\n",
      "loss at iter 2006:0.4268\n",
      "train accuracy: 0.876\n",
      "loss at iter 2007:0.3690\n",
      "train accuracy: 0.901\n",
      "loss at iter 2008:0.3620\n",
      "train accuracy: 0.915\n",
      "loss at iter 2009:0.3855\n",
      "train accuracy: 0.893\n",
      "loss at iter 2010:0.3709\n",
      "train accuracy: 0.903\n",
      "loss at iter 2011:0.3762\n",
      "train accuracy: 0.902\n",
      "loss at iter 2012:0.3920\n",
      "train accuracy: 0.894\n",
      "loss at iter 2013:0.3819\n",
      "train accuracy: 0.901\n",
      "loss at iter 2014:0.3958\n",
      "train accuracy: 0.902\n",
      "loss at iter 2015:0.4208\n",
      "train accuracy: 0.877\n",
      "loss at iter 2016:0.4110\n",
      "train accuracy: 0.899\n",
      "loss at iter 2017:0.4006\n",
      "train accuracy: 0.886\n",
      "loss at iter 2018:0.3939\n",
      "train accuracy: 0.893\n",
      "loss at iter 2019:0.4062\n",
      "train accuracy: 0.885\n",
      "loss at iter 2020:0.3789\n",
      "train accuracy: 0.897\n",
      "loss at iter 2021:0.3665\n",
      "train accuracy: 0.9\n",
      "loss at iter 2022:0.4125\n",
      "train accuracy: 0.894\n",
      "loss at iter 2023:0.3910\n",
      "train accuracy: 0.89\n",
      "loss at iter 2024:0.3839\n",
      "train accuracy: 0.889\n",
      "loss at iter 2025:0.4219\n",
      "train accuracy: 0.88\n",
      "loss at iter 2026:0.3971\n",
      "train accuracy: 0.898\n",
      "loss at iter 2027:0.3945\n",
      "train accuracy: 0.898\n",
      "loss at iter 2028:0.4001\n",
      "train accuracy: 0.886\n",
      "loss at iter 2029:0.4009\n",
      "train accuracy: 0.892\n",
      "loss at iter 2030:0.4078\n",
      "train accuracy: 0.89\n",
      "loss at iter 2031:0.4062\n",
      "train accuracy: 0.874\n",
      "loss at iter 2032:0.3708\n",
      "train accuracy: 0.897\n",
      "loss at iter 2033:0.4143\n",
      "train accuracy: 0.886\n",
      "loss at iter 2034:0.3707\n",
      "train accuracy: 0.897\n",
      "loss at iter 2035:0.3679\n",
      "train accuracy: 0.906\n",
      "loss at iter 2036:0.4059\n",
      "train accuracy: 0.894\n",
      "loss at iter 2037:0.4072\n",
      "train accuracy: 0.896\n",
      "loss at iter 2038:0.4167\n",
      "train accuracy: 0.901\n",
      "loss at iter 2039:0.3442\n",
      "train accuracy: 0.917\n",
      "loss at iter 2040:0.4167\n",
      "train accuracy: 0.901\n",
      "loss at iter 2041:0.3999\n",
      "train accuracy: 0.889\n",
      "loss at iter 2042:0.3757\n",
      "train accuracy: 0.892\n",
      "loss at iter 2043:0.3895\n",
      "train accuracy: 0.895\n",
      "loss at iter 2044:0.4340\n",
      "train accuracy: 0.881\n",
      "loss at iter 2045:0.3543\n",
      "train accuracy: 0.905\n",
      "loss at iter 2046:0.3964\n",
      "train accuracy: 0.884\n",
      "loss at iter 2047:0.4091\n",
      "train accuracy: 0.891\n",
      "loss at iter 2048:0.3621\n",
      "train accuracy: 0.909\n",
      "loss at iter 2049:0.3787\n",
      "train accuracy: 0.906\n",
      "loss at iter 2050:0.4350\n",
      "train accuracy: 0.881\n",
      "loss at iter 2051:0.3823\n",
      "train accuracy: 0.899\n",
      "loss at iter 2052:0.4257\n",
      "train accuracy: 0.881\n",
      "loss at iter 2053:0.3991\n",
      "train accuracy: 0.901\n",
      "loss at iter 2054:0.3881\n",
      "train accuracy: 0.885\n",
      "loss at iter 2055:0.4160\n",
      "train accuracy: 0.893\n",
      "loss at iter 2056:0.4192\n",
      "train accuracy: 0.893\n",
      "loss at iter 2057:0.3465\n",
      "train accuracy: 0.907\n",
      "loss at iter 2058:0.3864\n",
      "train accuracy: 0.893\n",
      "loss at iter 2059:0.4113\n",
      "train accuracy: 0.891\n",
      "loss at iter 2060:0.3689\n",
      "train accuracy: 0.904\n",
      "loss at iter 2061:0.3936\n",
      "train accuracy: 0.898\n",
      "loss at iter 2062:0.3899\n",
      "train accuracy: 0.887\n",
      "loss at iter 2063:0.4282\n",
      "train accuracy: 0.877\n",
      "loss at iter 2064:0.3461\n",
      "train accuracy: 0.906\n",
      "loss at iter 2065:0.3722\n",
      "train accuracy: 0.893\n",
      "loss at iter 2066:0.4022\n",
      "train accuracy: 0.897\n",
      "loss at iter 2067:0.3720\n",
      "train accuracy: 0.903\n",
      "loss at iter 2068:0.3783\n",
      "train accuracy: 0.888\n",
      "loss at iter 2069:0.3938\n",
      "train accuracy: 0.892\n",
      "loss at iter 2070:0.3804\n",
      "train accuracy: 0.9\n",
      "loss at iter 2071:0.3862\n",
      "train accuracy: 0.904\n",
      "loss at iter 2072:0.3677\n",
      "train accuracy: 0.904\n",
      "loss at iter 2073:0.3765\n",
      "train accuracy: 0.894\n",
      "loss at iter 2074:0.3875\n",
      "train accuracy: 0.898\n",
      "loss at iter 2075:0.3664\n",
      "train accuracy: 0.898\n",
      "loss at iter 2076:0.4239\n",
      "train accuracy: 0.882\n",
      "loss at iter 2077:0.3723\n",
      "train accuracy: 0.901\n",
      "loss at iter 2078:0.3920\n",
      "train accuracy: 0.903\n",
      "loss at iter 2079:0.3963\n",
      "train accuracy: 0.892\n",
      "loss at iter 2080:0.3866\n",
      "train accuracy: 0.904\n",
      "loss at iter 2081:0.3257\n",
      "train accuracy: 0.92\n",
      "loss at iter 2082:0.3704\n",
      "train accuracy: 0.905\n",
      "loss at iter 2083:0.4074\n",
      "train accuracy: 0.897\n",
      "loss at iter 2084:0.3657\n",
      "train accuracy: 0.9\n",
      "loss at iter 2085:0.3700\n",
      "train accuracy: 0.901\n",
      "loss at iter 2086:0.4413\n",
      "train accuracy: 0.885\n",
      "loss at iter 2087:0.4074\n",
      "train accuracy: 0.885\n",
      "loss at iter 2088:0.4113\n",
      "train accuracy: 0.889\n",
      "loss at iter 2089:0.3787\n",
      "train accuracy: 0.914\n",
      "loss at iter 2090:0.3896\n",
      "train accuracy: 0.894\n",
      "loss at iter 2091:0.3388\n",
      "train accuracy: 0.91\n",
      "loss at iter 2092:0.4360\n",
      "train accuracy: 0.883\n",
      "loss at iter 2093:0.3500\n",
      "train accuracy: 0.914\n",
      "loss at iter 2094:0.4327\n",
      "train accuracy: 0.878\n",
      "loss at iter 2095:0.3993\n",
      "train accuracy: 0.897\n",
      "loss at iter 2096:0.3504\n",
      "train accuracy: 0.906\n",
      "loss at iter 2097:0.4188\n",
      "train accuracy: 0.878\n",
      "loss at iter 2098:0.3751\n",
      "train accuracy: 0.892\n",
      "loss at iter 2099:0.3888\n",
      "train accuracy: 0.893\n",
      "loss at iter 2100:0.3852\n",
      "train accuracy: 0.903\n",
      "loss at iter 2101:0.3589\n",
      "train accuracy: 0.896\n",
      "loss at iter 2102:0.3930\n",
      "train accuracy: 0.888\n",
      "loss at iter 2103:0.3807\n",
      "train accuracy: 0.902\n",
      "loss at iter 2104:0.4137\n",
      "train accuracy: 0.891\n",
      "loss at iter 2105:0.4160\n",
      "train accuracy: 0.898\n",
      "loss at iter 2106:0.4210\n",
      "train accuracy: 0.881\n",
      "loss at iter 2107:0.3393\n",
      "train accuracy: 0.905\n",
      "loss at iter 2108:0.3885\n",
      "train accuracy: 0.897\n",
      "loss at iter 2109:0.3912\n",
      "train accuracy: 0.896\n",
      "loss at iter 2110:0.4027\n",
      "train accuracy: 0.899\n",
      "loss at iter 2111:0.3700\n",
      "train accuracy: 0.901\n",
      "loss at iter 2112:0.3626\n",
      "train accuracy: 0.898\n",
      "loss at iter 2113:0.3599\n",
      "train accuracy: 0.905\n",
      "loss at iter 2114:0.3964\n",
      "train accuracy: 0.89\n",
      "loss at iter 2115:0.4237\n",
      "train accuracy: 0.885\n",
      "loss at iter 2116:0.3966\n",
      "train accuracy: 0.894\n",
      "loss at iter 2117:0.3666\n",
      "train accuracy: 0.909\n",
      "loss at iter 2118:0.3971\n",
      "train accuracy: 0.89\n",
      "loss at iter 2119:0.3833\n",
      "train accuracy: 0.893\n",
      "loss at iter 2120:0.3632\n",
      "train accuracy: 0.898\n",
      "loss at iter 2121:0.3774\n",
      "train accuracy: 0.899\n",
      "loss at iter 2122:0.3549\n",
      "train accuracy: 0.907\n",
      "loss at iter 2123:0.3713\n",
      "train accuracy: 0.906\n",
      "loss at iter 2124:0.3692\n",
      "train accuracy: 0.913\n",
      "loss at iter 2125:0.3979\n",
      "train accuracy: 0.893\n",
      "loss at iter 2126:0.3849\n",
      "train accuracy: 0.901\n",
      "loss at iter 2127:0.4109\n",
      "train accuracy: 0.881\n",
      "loss at iter 2128:0.3776\n",
      "train accuracy: 0.903\n",
      "loss at iter 2129:0.4570\n",
      "train accuracy: 0.873\n",
      "loss at iter 2130:0.3848\n",
      "train accuracy: 0.887\n",
      "loss at iter 2131:0.3314\n",
      "train accuracy: 0.91\n",
      "loss at iter 2132:0.3581\n",
      "train accuracy: 0.905\n",
      "loss at iter 2133:0.3829\n",
      "train accuracy: 0.898\n",
      "loss at iter 2134:0.3528\n",
      "train accuracy: 0.896\n",
      "loss at iter 2135:0.3926\n",
      "train accuracy: 0.906\n",
      "loss at iter 2136:0.3973\n",
      "train accuracy: 0.895\n",
      "loss at iter 2137:0.3997\n",
      "train accuracy: 0.891\n",
      "loss at iter 2138:0.3747\n",
      "train accuracy: 0.896\n",
      "loss at iter 2139:0.3780\n",
      "train accuracy: 0.903\n",
      "loss at iter 2140:0.3789\n",
      "train accuracy: 0.905\n",
      "loss at iter 2141:0.3939\n",
      "train accuracy: 0.888\n",
      "loss at iter 2142:0.3474\n",
      "train accuracy: 0.909\n",
      "loss at iter 2143:0.4043\n",
      "train accuracy: 0.887\n",
      "loss at iter 2144:0.3673\n",
      "train accuracy: 0.908\n",
      "loss at iter 2145:0.3724\n",
      "train accuracy: 0.903\n",
      "loss at iter 2146:0.3724\n",
      "train accuracy: 0.895\n",
      "loss at iter 2147:0.4170\n",
      "train accuracy: 0.878\n",
      "loss at iter 2148:0.3667\n",
      "train accuracy: 0.9\n",
      "loss at iter 2149:0.3880\n",
      "train accuracy: 0.897\n",
      "loss at iter 2150:0.3944\n",
      "train accuracy: 0.882\n",
      "loss at iter 2151:0.4012\n",
      "train accuracy: 0.893\n",
      "loss at iter 2152:0.3850\n",
      "train accuracy: 0.897\n",
      "loss at iter 2153:0.3547\n",
      "train accuracy: 0.9\n",
      "loss at iter 2154:0.4302\n",
      "train accuracy: 0.87\n",
      "loss at iter 2155:0.3681\n",
      "train accuracy: 0.907\n",
      "loss at iter 2156:0.3884\n",
      "train accuracy: 0.895\n",
      "loss at iter 2157:0.3868\n",
      "train accuracy: 0.905\n",
      "loss at iter 2158:0.3597\n",
      "train accuracy: 0.899\n",
      "loss at iter 2159:0.3602\n",
      "train accuracy: 0.905\n",
      "loss at iter 2160:0.3629\n",
      "train accuracy: 0.907\n",
      "loss at iter 2161:0.3620\n",
      "train accuracy: 0.909\n",
      "loss at iter 2162:0.3716\n",
      "train accuracy: 0.909\n",
      "loss at iter 2163:0.3426\n",
      "train accuracy: 0.917\n",
      "loss at iter 2164:0.4050\n",
      "train accuracy: 0.888\n",
      "loss at iter 2165:0.4316\n",
      "train accuracy: 0.883\n",
      "loss at iter 2166:0.3451\n",
      "train accuracy: 0.899\n",
      "loss at iter 2167:0.3892\n",
      "train accuracy: 0.88\n",
      "loss at iter 2168:0.3595\n",
      "train accuracy: 0.908\n",
      "loss at iter 2169:0.3848\n",
      "train accuracy: 0.894\n",
      "loss at iter 2170:0.3708\n",
      "train accuracy: 0.896\n",
      "loss at iter 2171:0.4042\n",
      "train accuracy: 0.89\n",
      "loss at iter 2172:0.3831\n",
      "train accuracy: 0.888\n",
      "loss at iter 2173:0.3928\n",
      "train accuracy: 0.894\n",
      "loss at iter 2174:0.3526\n",
      "train accuracy: 0.912\n",
      "loss at iter 2175:0.3940\n",
      "train accuracy: 0.894\n",
      "loss at iter 2176:0.3997\n",
      "train accuracy: 0.89\n",
      "loss at iter 2177:0.3729\n",
      "train accuracy: 0.89\n",
      "loss at iter 2178:0.4279\n",
      "train accuracy: 0.884\n",
      "loss at iter 2179:0.4148\n",
      "train accuracy: 0.886\n",
      "loss at iter 2180:0.3940\n",
      "train accuracy: 0.893\n",
      "loss at iter 2181:0.3506\n",
      "train accuracy: 0.899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 2182:0.3705\n",
      "train accuracy: 0.911\n",
      "loss at iter 2183:0.4151\n",
      "train accuracy: 0.883\n",
      "loss at iter 2184:0.3750\n",
      "train accuracy: 0.901\n",
      "loss at iter 2185:0.3607\n",
      "train accuracy: 0.907\n",
      "loss at iter 2186:0.3568\n",
      "train accuracy: 0.908\n",
      "loss at iter 2187:0.3585\n",
      "train accuracy: 0.905\n",
      "loss at iter 2188:0.3626\n",
      "train accuracy: 0.891\n",
      "loss at iter 2189:0.3747\n",
      "train accuracy: 0.905\n",
      "loss at iter 2190:0.3642\n",
      "train accuracy: 0.907\n",
      "loss at iter 2191:0.3560\n",
      "train accuracy: 0.911\n",
      "loss at iter 2192:0.3592\n",
      "train accuracy: 0.903\n",
      "loss at iter 2193:0.3666\n",
      "train accuracy: 0.901\n",
      "loss at iter 2194:0.3596\n",
      "train accuracy: 0.909\n",
      "loss at iter 2195:0.3501\n",
      "train accuracy: 0.913\n",
      "loss at iter 2196:0.3915\n",
      "train accuracy: 0.887\n",
      "loss at iter 2197:0.3823\n",
      "train accuracy: 0.896\n",
      "loss at iter 2198:0.3887\n",
      "train accuracy: 0.897\n",
      "loss at iter 2199:0.3804\n",
      "train accuracy: 0.897\n",
      "loss at iter 2200:0.3893\n",
      "train accuracy: 0.894\n",
      "loss at iter 2201:0.3252\n",
      "train accuracy: 0.911\n",
      "loss at iter 2202:0.3796\n",
      "train accuracy: 0.896\n",
      "loss at iter 2203:0.3579\n",
      "train accuracy: 0.914\n",
      "loss at iter 2204:0.4107\n",
      "train accuracy: 0.894\n",
      "loss at iter 2205:0.3602\n",
      "train accuracy: 0.908\n",
      "loss at iter 2206:0.3872\n",
      "train accuracy: 0.897\n",
      "loss at iter 2207:0.3836\n",
      "train accuracy: 0.906\n",
      "loss at iter 2208:0.3796\n",
      "train accuracy: 0.902\n",
      "loss at iter 2209:0.3613\n",
      "train accuracy: 0.912\n",
      "loss at iter 2210:0.3695\n",
      "train accuracy: 0.897\n",
      "loss at iter 2211:0.3754\n",
      "train accuracy: 0.893\n",
      "loss at iter 2212:0.3713\n",
      "train accuracy: 0.897\n",
      "loss at iter 2213:0.3707\n",
      "train accuracy: 0.899\n",
      "loss at iter 2214:0.3560\n",
      "train accuracy: 0.896\n",
      "loss at iter 2215:0.3725\n",
      "train accuracy: 0.902\n",
      "loss at iter 2216:0.3855\n",
      "train accuracy: 0.903\n",
      "loss at iter 2217:0.3314\n",
      "train accuracy: 0.903\n",
      "loss at iter 2218:0.3896\n",
      "train accuracy: 0.876\n",
      "loss at iter 2219:0.3599\n",
      "train accuracy: 0.901\n",
      "loss at iter 2220:0.3812\n",
      "train accuracy: 0.893\n",
      "loss at iter 2221:0.4039\n",
      "train accuracy: 0.891\n",
      "loss at iter 2222:0.3818\n",
      "train accuracy: 0.901\n",
      "loss at iter 2223:0.3609\n",
      "train accuracy: 0.913\n",
      "loss at iter 2224:0.3957\n",
      "train accuracy: 0.894\n",
      "loss at iter 2225:0.3659\n",
      "train accuracy: 0.891\n",
      "loss at iter 2226:0.3586\n",
      "train accuracy: 0.897\n",
      "loss at iter 2227:0.3913\n",
      "train accuracy: 0.903\n",
      "loss at iter 2228:0.3789\n",
      "train accuracy: 0.902\n",
      "loss at iter 2229:0.3845\n",
      "train accuracy: 0.902\n",
      "loss at iter 2230:0.3994\n",
      "train accuracy: 0.89\n",
      "loss at iter 2231:0.4036\n",
      "train accuracy: 0.887\n",
      "loss at iter 2232:0.3892\n",
      "train accuracy: 0.891\n",
      "loss at iter 2233:0.3974\n",
      "train accuracy: 0.898\n",
      "loss at iter 2234:0.4027\n",
      "train accuracy: 0.889\n",
      "loss at iter 2235:0.3549\n",
      "train accuracy: 0.909\n",
      "loss at iter 2236:0.3386\n",
      "train accuracy: 0.895\n",
      "loss at iter 2237:0.3754\n",
      "train accuracy: 0.909\n",
      "loss at iter 2238:0.3735\n",
      "train accuracy: 0.894\n",
      "loss at iter 2239:0.3615\n",
      "train accuracy: 0.9\n",
      "loss at iter 2240:0.4179\n",
      "train accuracy: 0.882\n",
      "loss at iter 2241:0.3220\n",
      "train accuracy: 0.911\n",
      "loss at iter 2242:0.3402\n",
      "train accuracy: 0.914\n",
      "loss at iter 2243:0.3549\n",
      "train accuracy: 0.911\n",
      "loss at iter 2244:0.3570\n",
      "train accuracy: 0.896\n",
      "loss at iter 2245:0.3805\n",
      "train accuracy: 0.895\n",
      "loss at iter 2246:0.3804\n",
      "train accuracy: 0.901\n",
      "loss at iter 2247:0.3783\n",
      "train accuracy: 0.891\n",
      "loss at iter 2248:0.3740\n",
      "train accuracy: 0.896\n",
      "loss at iter 2249:0.3482\n",
      "train accuracy: 0.905\n",
      "loss at iter 2250:0.3761\n",
      "train accuracy: 0.896\n",
      "loss at iter 2251:0.3988\n",
      "train accuracy: 0.887\n",
      "loss at iter 2252:0.3708\n",
      "train accuracy: 0.899\n",
      "loss at iter 2253:0.3907\n",
      "train accuracy: 0.89\n",
      "loss at iter 2254:0.3463\n",
      "train accuracy: 0.909\n",
      "loss at iter 2255:0.3535\n",
      "train accuracy: 0.9\n",
      "loss at iter 2256:0.3497\n",
      "train accuracy: 0.91\n",
      "loss at iter 2257:0.3462\n",
      "train accuracy: 0.903\n",
      "loss at iter 2258:0.3596\n",
      "train accuracy: 0.893\n",
      "loss at iter 2259:0.3784\n",
      "train accuracy: 0.907\n",
      "loss at iter 2260:0.3665\n",
      "train accuracy: 0.902\n",
      "loss at iter 2261:0.3717\n",
      "train accuracy: 0.897\n",
      "loss at iter 2262:0.3603\n",
      "train accuracy: 0.908\n",
      "loss at iter 2263:0.3410\n",
      "train accuracy: 0.915\n",
      "loss at iter 2264:0.3243\n",
      "train accuracy: 0.915\n",
      "loss at iter 2265:0.3677\n",
      "train accuracy: 0.902\n",
      "loss at iter 2266:0.3510\n",
      "train accuracy: 0.909\n",
      "loss at iter 2267:0.4504\n",
      "train accuracy: 0.878\n",
      "loss at iter 2268:0.3969\n",
      "train accuracy: 0.874\n",
      "loss at iter 2269:0.3887\n",
      "train accuracy: 0.892\n",
      "loss at iter 2270:0.3706\n",
      "train accuracy: 0.902\n",
      "loss at iter 2271:0.3605\n",
      "train accuracy: 0.9\n",
      "loss at iter 2272:0.3400\n",
      "train accuracy: 0.914\n",
      "loss at iter 2273:0.3708\n",
      "train accuracy: 0.901\n",
      "loss at iter 2274:0.3694\n",
      "train accuracy: 0.898\n",
      "loss at iter 2275:0.3635\n",
      "train accuracy: 0.902\n",
      "loss at iter 2276:0.3850\n",
      "train accuracy: 0.894\n",
      "loss at iter 2277:0.3664\n",
      "train accuracy: 0.891\n",
      "loss at iter 2278:0.3737\n",
      "train accuracy: 0.891\n",
      "loss at iter 2279:0.3492\n",
      "train accuracy: 0.907\n",
      "loss at iter 2280:0.3541\n",
      "train accuracy: 0.916\n",
      "loss at iter 2281:0.3458\n",
      "train accuracy: 0.893\n",
      "loss at iter 2282:0.3917\n",
      "train accuracy: 0.901\n",
      "loss at iter 2283:0.3876\n",
      "train accuracy: 0.892\n",
      "loss at iter 2284:0.3644\n",
      "train accuracy: 0.901\n",
      "loss at iter 2285:0.3511\n",
      "train accuracy: 0.902\n",
      "loss at iter 2286:0.3603\n",
      "train accuracy: 0.909\n",
      "loss at iter 2287:0.3570\n",
      "train accuracy: 0.898\n",
      "loss at iter 2288:0.4000\n",
      "train accuracy: 0.897\n",
      "loss at iter 2289:0.4141\n",
      "train accuracy: 0.888\n",
      "loss at iter 2290:0.3811\n",
      "train accuracy: 0.901\n",
      "loss at iter 2291:0.3482\n",
      "train accuracy: 0.902\n",
      "loss at iter 2292:0.3803\n",
      "train accuracy: 0.893\n",
      "loss at iter 2293:0.3567\n",
      "train accuracy: 0.905\n",
      "loss at iter 2294:0.3432\n",
      "train accuracy: 0.9\n",
      "loss at iter 2295:0.3771\n",
      "train accuracy: 0.908\n",
      "loss at iter 2296:0.3657\n",
      "train accuracy: 0.904\n",
      "loss at iter 2297:0.3563\n",
      "train accuracy: 0.895\n",
      "loss at iter 2298:0.3825\n",
      "train accuracy: 0.901\n",
      "loss at iter 2299:0.3831\n",
      "train accuracy: 0.892\n",
      "loss at iter 2300:0.3612\n",
      "train accuracy: 0.905\n",
      "loss at iter 2301:0.3549\n",
      "train accuracy: 0.915\n",
      "loss at iter 2302:0.3823\n",
      "train accuracy: 0.891\n",
      "loss at iter 2303:0.3623\n",
      "train accuracy: 0.901\n",
      "loss at iter 2304:0.3603\n",
      "train accuracy: 0.907\n",
      "loss at iter 2305:0.3759\n",
      "train accuracy: 0.903\n",
      "loss at iter 2306:0.3775\n",
      "train accuracy: 0.893\n",
      "loss at iter 2307:0.3346\n",
      "train accuracy: 0.912\n",
      "loss at iter 2308:0.3422\n",
      "train accuracy: 0.906\n",
      "loss at iter 2309:0.3873\n",
      "train accuracy: 0.89\n",
      "loss at iter 2310:0.3537\n",
      "train accuracy: 0.913\n",
      "loss at iter 2311:0.3954\n",
      "train accuracy: 0.89\n",
      "loss at iter 2312:0.3910\n",
      "train accuracy: 0.889\n",
      "loss at iter 2313:0.3517\n",
      "train accuracy: 0.902\n",
      "loss at iter 2314:0.3643\n",
      "train accuracy: 0.903\n",
      "loss at iter 2315:0.3464\n",
      "train accuracy: 0.907\n",
      "loss at iter 2316:0.3629\n",
      "train accuracy: 0.897\n",
      "loss at iter 2317:0.3503\n",
      "train accuracy: 0.901\n",
      "loss at iter 2318:0.3553\n",
      "train accuracy: 0.891\n",
      "loss at iter 2319:0.3300\n",
      "train accuracy: 0.903\n",
      "loss at iter 2320:0.3555\n",
      "train accuracy: 0.903\n",
      "loss at iter 2321:0.3757\n",
      "train accuracy: 0.89\n",
      "loss at iter 2322:0.3570\n",
      "train accuracy: 0.897\n",
      "loss at iter 2323:0.3741\n",
      "train accuracy: 0.895\n",
      "loss at iter 2324:0.3805\n",
      "train accuracy: 0.898\n",
      "loss at iter 2325:0.3664\n",
      "train accuracy: 0.902\n",
      "loss at iter 2326:0.3752\n",
      "train accuracy: 0.904\n",
      "loss at iter 2327:0.3755\n",
      "train accuracy: 0.906\n",
      "loss at iter 2328:0.3546\n",
      "train accuracy: 0.904\n",
      "loss at iter 2329:0.3783\n",
      "train accuracy: 0.896\n",
      "loss at iter 2330:0.3341\n",
      "train accuracy: 0.903\n",
      "loss at iter 2331:0.3631\n",
      "train accuracy: 0.898\n",
      "loss at iter 2332:0.3506\n",
      "train accuracy: 0.908\n",
      "loss at iter 2333:0.3807\n",
      "train accuracy: 0.899\n",
      "loss at iter 2334:0.3792\n",
      "train accuracy: 0.891\n",
      "loss at iter 2335:0.4173\n",
      "train accuracy: 0.875\n",
      "loss at iter 2336:0.3839\n",
      "train accuracy: 0.896\n",
      "loss at iter 2337:0.3364\n",
      "train accuracy: 0.91\n",
      "loss at iter 2338:0.3819\n",
      "train accuracy: 0.891\n",
      "loss at iter 2339:0.3946\n",
      "train accuracy: 0.895\n",
      "loss at iter 2340:0.3786\n",
      "train accuracy: 0.905\n",
      "loss at iter 2341:0.3709\n",
      "train accuracy: 0.905\n",
      "loss at iter 2342:0.3247\n",
      "train accuracy: 0.916\n",
      "loss at iter 2343:0.3540\n",
      "train accuracy: 0.911\n",
      "loss at iter 2344:0.3647\n",
      "train accuracy: 0.905\n",
      "loss at iter 2345:0.3459\n",
      "train accuracy: 0.904\n",
      "loss at iter 2346:0.3577\n",
      "train accuracy: 0.905\n",
      "loss at iter 2347:0.3441\n",
      "train accuracy: 0.912\n",
      "loss at iter 2348:0.3372\n",
      "train accuracy: 0.909\n",
      "loss at iter 2349:0.3896\n",
      "train accuracy: 0.901\n",
      "loss at iter 2350:0.3866\n",
      "train accuracy: 0.885\n",
      "loss at iter 2351:0.3950\n",
      "train accuracy: 0.898\n",
      "loss at iter 2352:0.3451\n",
      "train accuracy: 0.916\n",
      "loss at iter 2353:0.3630\n",
      "train accuracy: 0.892\n",
      "loss at iter 2354:0.3742\n",
      "train accuracy: 0.897\n",
      "loss at iter 2355:0.3941\n",
      "train accuracy: 0.886\n",
      "loss at iter 2356:0.3300\n",
      "train accuracy: 0.899\n",
      "loss at iter 2357:0.3728\n",
      "train accuracy: 0.907\n",
      "loss at iter 2358:0.3674\n",
      "train accuracy: 0.902\n",
      "loss at iter 2359:0.4119\n",
      "train accuracy: 0.89\n",
      "loss at iter 2360:0.3887\n",
      "train accuracy: 0.89\n",
      "loss at iter 2361:0.3768\n",
      "train accuracy: 0.891\n",
      "loss at iter 2362:0.3897\n",
      "train accuracy: 0.892\n",
      "loss at iter 2363:0.3722\n",
      "train accuracy: 0.89\n",
      "loss at iter 2364:0.3750\n",
      "train accuracy: 0.893\n",
      "loss at iter 2365:0.3846\n",
      "train accuracy: 0.894\n",
      "loss at iter 2366:0.3136\n",
      "train accuracy: 0.921\n",
      "loss at iter 2367:0.3394\n",
      "train accuracy: 0.911\n",
      "loss at iter 2368:0.3294\n",
      "train accuracy: 0.914\n",
      "loss at iter 2369:0.3883\n",
      "train accuracy: 0.898\n",
      "loss at iter 2370:0.3014\n",
      "train accuracy: 0.921\n",
      "loss at iter 2371:0.3741\n",
      "train accuracy: 0.895\n",
      "loss at iter 2372:0.3427\n",
      "train accuracy: 0.909\n",
      "loss at iter 2373:0.4084\n",
      "train accuracy: 0.876\n",
      "loss at iter 2374:0.3780\n",
      "train accuracy: 0.891\n",
      "loss at iter 2375:0.3634\n",
      "train accuracy: 0.906\n",
      "loss at iter 2376:0.3772\n",
      "train accuracy: 0.899\n",
      "loss at iter 2377:0.3559\n",
      "train accuracy: 0.901\n",
      "loss at iter 2378:0.3466\n",
      "train accuracy: 0.902\n",
      "loss at iter 2379:0.3606\n",
      "train accuracy: 0.905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 2380:0.3401\n",
      "train accuracy: 0.901\n",
      "loss at iter 2381:0.3265\n",
      "train accuracy: 0.911\n",
      "loss at iter 2382:0.3546\n",
      "train accuracy: 0.905\n",
      "loss at iter 2383:0.3398\n",
      "train accuracy: 0.918\n",
      "loss at iter 2384:0.3618\n",
      "train accuracy: 0.905\n",
      "loss at iter 2385:0.3421\n",
      "train accuracy: 0.896\n",
      "loss at iter 2386:0.3687\n",
      "train accuracy: 0.899\n",
      "loss at iter 2387:0.3417\n",
      "train accuracy: 0.914\n",
      "loss at iter 2388:0.3077\n",
      "train accuracy: 0.922\n",
      "loss at iter 2389:0.3603\n",
      "train accuracy: 0.909\n",
      "loss at iter 2390:0.3656\n",
      "train accuracy: 0.894\n",
      "loss at iter 2391:0.3171\n",
      "train accuracy: 0.915\n",
      "loss at iter 2392:0.3801\n",
      "train accuracy: 0.905\n",
      "loss at iter 2393:0.3459\n",
      "train accuracy: 0.913\n",
      "loss at iter 2394:0.3411\n",
      "train accuracy: 0.898\n",
      "loss at iter 2395:0.3725\n",
      "train accuracy: 0.909\n",
      "loss at iter 2396:0.3534\n",
      "train accuracy: 0.908\n",
      "loss at iter 2397:0.3745\n",
      "train accuracy: 0.896\n",
      "loss at iter 2398:0.3400\n",
      "train accuracy: 0.914\n",
      "loss at iter 2399:0.3772\n",
      "train accuracy: 0.895\n",
      "loss at iter 2400:0.3694\n",
      "train accuracy: 0.896\n",
      "loss at iter 2401:0.3689\n",
      "train accuracy: 0.892\n",
      "loss at iter 2402:0.3893\n",
      "train accuracy: 0.888\n",
      "loss at iter 2403:0.3378\n",
      "train accuracy: 0.903\n",
      "loss at iter 2404:0.3410\n",
      "train accuracy: 0.912\n",
      "loss at iter 2405:0.3300\n",
      "train accuracy: 0.918\n",
      "loss at iter 2406:0.3901\n",
      "train accuracy: 0.886\n",
      "loss at iter 2407:0.3311\n",
      "train accuracy: 0.911\n",
      "loss at iter 2408:0.3645\n",
      "train accuracy: 0.9\n",
      "loss at iter 2409:0.3589\n",
      "train accuracy: 0.904\n",
      "loss at iter 2410:0.3700\n",
      "train accuracy: 0.905\n",
      "loss at iter 2411:0.3565\n",
      "train accuracy: 0.909\n",
      "loss at iter 2412:0.3563\n",
      "train accuracy: 0.896\n",
      "loss at iter 2413:0.3542\n",
      "train accuracy: 0.907\n",
      "loss at iter 2414:0.3988\n",
      "train accuracy: 0.884\n",
      "loss at iter 2415:0.3763\n",
      "train accuracy: 0.902\n",
      "loss at iter 2416:0.3574\n",
      "train accuracy: 0.903\n",
      "loss at iter 2417:0.3750\n",
      "train accuracy: 0.889\n",
      "loss at iter 2418:0.3853\n",
      "train accuracy: 0.882\n",
      "loss at iter 2419:0.3184\n",
      "train accuracy: 0.922\n",
      "loss at iter 2420:0.3708\n",
      "train accuracy: 0.887\n",
      "loss at iter 2421:0.3590\n",
      "train accuracy: 0.895\n",
      "loss at iter 2422:0.3143\n",
      "train accuracy: 0.91\n",
      "loss at iter 2423:0.3477\n",
      "train accuracy: 0.896\n",
      "loss at iter 2424:0.3536\n",
      "train accuracy: 0.905\n",
      "loss at iter 2425:0.3582\n",
      "train accuracy: 0.9\n",
      "loss at iter 2426:0.3653\n",
      "train accuracy: 0.887\n",
      "loss at iter 2427:0.3517\n",
      "train accuracy: 0.908\n",
      "loss at iter 2428:0.3137\n",
      "train accuracy: 0.916\n",
      "loss at iter 2429:0.3643\n",
      "train accuracy: 0.895\n",
      "loss at iter 2430:0.3817\n",
      "train accuracy: 0.9\n",
      "loss at iter 2431:0.3336\n",
      "train accuracy: 0.904\n",
      "loss at iter 2432:0.3254\n",
      "train accuracy: 0.905\n",
      "loss at iter 2433:0.3307\n",
      "train accuracy: 0.913\n",
      "loss at iter 2434:0.4008\n",
      "train accuracy: 0.897\n",
      "loss at iter 2435:0.3487\n",
      "train accuracy: 0.893\n",
      "loss at iter 2436:0.4002\n",
      "train accuracy: 0.896\n",
      "loss at iter 2437:0.3289\n",
      "train accuracy: 0.918\n",
      "loss at iter 2438:0.3553\n",
      "train accuracy: 0.906\n",
      "loss at iter 2439:0.3655\n",
      "train accuracy: 0.897\n",
      "loss at iter 2440:0.3598\n",
      "train accuracy: 0.897\n",
      "loss at iter 2441:0.3158\n",
      "train accuracy: 0.914\n",
      "loss at iter 2442:0.3390\n",
      "train accuracy: 0.911\n",
      "loss at iter 2443:0.3597\n",
      "train accuracy: 0.91\n",
      "loss at iter 2444:0.3838\n",
      "train accuracy: 0.905\n",
      "loss at iter 2445:0.3860\n",
      "train accuracy: 0.896\n",
      "loss at iter 2446:0.3387\n",
      "train accuracy: 0.908\n",
      "loss at iter 2447:0.3371\n",
      "train accuracy: 0.911\n",
      "loss at iter 2448:0.3461\n",
      "train accuracy: 0.906\n",
      "loss at iter 2449:0.3580\n",
      "train accuracy: 0.913\n",
      "loss at iter 2450:0.3200\n",
      "train accuracy: 0.904\n",
      "loss at iter 2451:0.3483\n",
      "train accuracy: 0.896\n",
      "loss at iter 2452:0.4200\n",
      "train accuracy: 0.878\n",
      "loss at iter 2453:0.3406\n",
      "train accuracy: 0.911\n",
      "loss at iter 2454:0.3368\n",
      "train accuracy: 0.911\n",
      "loss at iter 2455:0.3607\n",
      "train accuracy: 0.9\n",
      "loss at iter 2456:0.3108\n",
      "train accuracy: 0.915\n",
      "loss at iter 2457:0.3631\n",
      "train accuracy: 0.909\n",
      "loss at iter 2458:0.3459\n",
      "train accuracy: 0.901\n",
      "loss at iter 2459:0.4035\n",
      "train accuracy: 0.883\n",
      "loss at iter 2460:0.3421\n",
      "train accuracy: 0.917\n",
      "loss at iter 2461:0.3772\n",
      "train accuracy: 0.902\n",
      "loss at iter 2462:0.3196\n",
      "train accuracy: 0.919\n",
      "loss at iter 2463:0.3600\n",
      "train accuracy: 0.904\n",
      "loss at iter 2464:0.3674\n",
      "train accuracy: 0.899\n",
      "loss at iter 2465:0.3387\n",
      "train accuracy: 0.911\n",
      "loss at iter 2466:0.3605\n",
      "train accuracy: 0.904\n",
      "loss at iter 2467:0.3434\n",
      "train accuracy: 0.902\n",
      "loss at iter 2468:0.3837\n",
      "train accuracy: 0.89\n",
      "loss at iter 2469:0.3627\n",
      "train accuracy: 0.91\n",
      "loss at iter 2470:0.3337\n",
      "train accuracy: 0.908\n",
      "loss at iter 2471:0.4044\n",
      "train accuracy: 0.889\n",
      "loss at iter 2472:0.3185\n",
      "train accuracy: 0.912\n",
      "loss at iter 2473:0.3784\n",
      "train accuracy: 0.894\n",
      "loss at iter 2474:0.2938\n",
      "train accuracy: 0.927\n",
      "loss at iter 2475:0.3810\n",
      "train accuracy: 0.902\n",
      "loss at iter 2476:0.3650\n",
      "train accuracy: 0.894\n",
      "loss at iter 2477:0.3199\n",
      "train accuracy: 0.913\n",
      "loss at iter 2478:0.3351\n",
      "train accuracy: 0.911\n",
      "loss at iter 2479:0.3003\n",
      "train accuracy: 0.922\n",
      "loss at iter 2480:0.3397\n",
      "train accuracy: 0.911\n",
      "loss at iter 2481:0.3768\n",
      "train accuracy: 0.899\n",
      "loss at iter 2482:0.3764\n",
      "train accuracy: 0.895\n",
      "loss at iter 2483:0.3392\n",
      "train accuracy: 0.92\n",
      "loss at iter 2484:0.3209\n",
      "train accuracy: 0.914\n",
      "loss at iter 2485:0.3661\n",
      "train accuracy: 0.888\n",
      "loss at iter 2486:0.3949\n",
      "train accuracy: 0.9\n",
      "loss at iter 2487:0.3733\n",
      "train accuracy: 0.887\n",
      "loss at iter 2488:0.3543\n",
      "train accuracy: 0.888\n",
      "loss at iter 2489:0.3521\n",
      "train accuracy: 0.896\n",
      "loss at iter 2490:0.3704\n",
      "train accuracy: 0.895\n",
      "loss at iter 2491:0.3670\n",
      "train accuracy: 0.894\n",
      "loss at iter 2492:0.3658\n",
      "train accuracy: 0.902\n",
      "loss at iter 2493:0.3327\n",
      "train accuracy: 0.92\n",
      "loss at iter 2494:0.3581\n",
      "train accuracy: 0.893\n",
      "loss at iter 2495:0.3354\n",
      "train accuracy: 0.913\n",
      "loss at iter 2496:0.3187\n",
      "train accuracy: 0.92\n",
      "loss at iter 2497:0.3546\n",
      "train accuracy: 0.911\n",
      "loss at iter 2498:0.3727\n",
      "train accuracy: 0.891\n",
      "loss at iter 2499:0.3323\n",
      "train accuracy: 0.904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-003dd8b73541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss_i, accuracy_i, optimizer_i = s.run([\n",
      "\u001b[0;32m<ipython-input-11-2ff72687da78>\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mres_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mres_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mres_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mres_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2ff72687da78>\u001b[0m in \u001b[0;36mreshuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "batcher = MiniBatchGenerator(X_train, y_train, 1000)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "for i in range(10000):\n",
    "    X_batch, y_batch = batcher.next()\n",
    "    \n",
    "    loss_i, accuracy_i, optimizer_i = s.run([\n",
    "        loss, accuracy, optimizer\n",
    "    ],  {input_x: X_batch, \n",
    "                     label_y: y_batch})\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train accuracy:\", accuracy_i)\n",
    "\n",
    "accuracy_i = s.run([\n",
    "    accuracy\n",
    "],  {input_x: X_test, \n",
    "     label_y: y_test})\n",
    "\n",
    "print(\"test_accuracy:\", accuracy_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 28 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
